---
title: Drop Unsupported Params | liteLLM
url: https://docs.litellm.ai/docs/completion/drop_params
source: sitemap
fetched_at: 2026-01-21T19:44:22.942890689-03:00
rendered_js: false
word_count: 315
summary: This document explains how to manage unsupported OpenAI parameters in LiteLLM by dropping them automatically, manually specifying fields for removal, or force-allowing specific parameters for cross-provider compatibility.
tags:
    - litellm
    - parameter-management
    - openai-compatibility
    - llm-proxy
    - python-sdk
    - error-handling
category: configuration
---

Drop unsupported OpenAI params by your LLM Provider.

## Default Behavior[â€‹](#default-behavior "Direct link to Default Behavior")

**By default, LiteLLM raises an exception** if you send a parameter to a model that doesn't support it.

For example, if you send `temperature=0.2` to a model that doesn't support the `temperature` parameter, LiteLLM will raise an exception.

**When `drop_params=True` is set**, LiteLLM will drop the unsupported parameter instead of raising an exception. This allows your code to work seamlessly across different providers without having to customize parameters for each one.

## Quick Start[â€‹](#quick-start "Direct link to Quick Start")

```
import litellm 
import os 

# set keys 
os.environ["COHERE_API_KEY"]="co-.."

litellm.drop_params =True# ðŸ‘ˆ KEY CHANGE

response = litellm.completion(
                model="command-r",
                messages=[{"role":"user","content":"Hey, how's it going?"}],
                response_format={"key":"value"},
)
```

LiteLLM maps all supported openai params by provider + model (e.g. function calling is supported by anthropic on bedrock but not titan).

See `litellm.get_supported_openai_params("command-r")` [**Code**](https://github.com/BerriAI/litellm/blob/main/litellm/utils.py#L3584)

If a provider/model doesn't support a particular param, you can drop it.

## OpenAI Proxy Usage[â€‹](#openai-proxy-usage "Direct link to OpenAI Proxy Usage")

```
litellm_settings:
drop_params:true
```

## Pass drop\_params in `completion(..)`[â€‹](#pass-drop_params-in-completion "Direct link to pass-drop_params-in-completion")

Just drop\_params when calling specific models

- SDK
- PROXY

```
import litellm 
import os 

# set keys 
os.environ["COHERE_API_KEY"]="co-.."

response = litellm.completion(
                model="command-r",
                messages=[{"role":"user","content":"Hey, how's it going?"}],
                response_format={"key":"value"},
                drop_params=True
)
```

## Specify params to drop[â€‹](#specify-params-to-drop "Direct link to Specify params to drop")

To drop specific params when calling a provider (E.g. 'logit\_bias' for vllm)

Use `additional_drop_params`

- SDK
- PROXY

```
import litellm 
import os 

# set keys 
os.environ["COHERE_API_KEY"]="co-.."

response = litellm.completion(
                model="command-r",
                messages=[{"role":"user","content":"Hey, how's it going?"}],
                response_format={"key":"value"},
                additional_drop_params=["response_format"]
)
```

**additional\_drop\_params**: List or null - Is a list of openai params you want to drop when making a call to the model.

### Nested Field Removal[â€‹](#nested-field-removal "Direct link to Nested Field Removal")

Drop nested fields within complex objects using JSONPath-like notation:

- SDK
- PROXY

```
import litellm

response = litellm.completion(
    model="bedrock/us.anthropic.claude-sonnet-4-5-20250929-v1:0",
    messages=[{"role":"user","content":"Hello"}],
    tools=[{
"name":"search",
"description":"Search files",
"input_schema":{"type":"object","properties":{"query":{"type":"string"}}},
"input_examples":[{"query":"test"}]# Will be removed
}],
    additional_drop_params=["tools[*].input_examples"]# Remove from all tools
)
```

**Supported syntax:**

- `field` - Top-level field
- `parent.child` - Nested object field
- `array[*]` - All array elements
- `array[0]` - Specific array index
- `tools[*].input_examples` - Field in all array elements
- `tools[0].metadata.field` - Specific index + nested field

**Example use cases:**

- Remove `input_examples` from tool definitions (Claude Code + AWS Bedrock)
- Drop provider-specific fields from nested structures
- Clean up nested parameters before sending to LLM

## Specify allowed openai params in a request[â€‹](#specify-allowed-openai-params-in-a-request "Direct link to Specify allowed openai params in a request")

Tell litellm to allow specific openai params in a request. Use this if you get a `litellm.UnsupportedParamsError` and want to allow a param. LiteLLM will pass the param as is to the model.

- LiteLLM Python SDK
- LiteLLM Proxy

In this example we pass `allowed_openai_params=["tools"]` to allow the `tools` param.

Pass allowed\_openai\_params to LiteLLM Python SDK

```
await litellm.acompletion(
    model="azure/o_series/<my-deployment-name>",
    api_key="xxxxx",
    api_base=api_base,
    messages=[{"role":"user","content":"Hello! return a json object"}],
    tools=[{"type":"function","function":{"name":"get_current_time","description":"Get the current time in a given location.","parameters":{"type":"object","properties":{"location":{"type":"string","description":"The city name, e.g. San Francisco"}},"required":["location"]}}}]
    allowed_openai_params=["tools"],
)
```