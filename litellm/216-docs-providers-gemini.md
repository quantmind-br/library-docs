---
title: Gemini - Google AI Studio | liteLLM
url: https://docs.litellm.ai/docs/providers/gemini
source: sitemap
fetched_at: 2026-01-21T19:49:06.957763268-03:00
rendered_js: false
word_count: 1733
summary: This document provides technical instructions for integrating Google AI Studio's Gemini models using LiteLLM, detailing authentication, parameter mapping for reasoning, and cost optimization.
tags:
    - google-ai-studio
    - gemini-api
    - litellm
    - reasoning-content
    - text-to-speech
    - model-configuration
category: guide
---

PropertyDetailsDescriptionGoogle AI Studio is a fully-managed AI development platform for building and using generative AI.Provider Route on LiteLLM`gemini/`Provider Doc[Google AI Studio â†—](https://aistudio.google.com/)API Endpoint for Provider[https://generativelanguage.googleapis.com](https://generativelanguage.googleapis.com)Supported OpenAI Endpoints`/chat/completions`, [`/embeddings`](https://docs.litellm.ai/docs/embedding/supported_embedding#gemini-ai-embedding-models), `/completions`, [`/videos`](https://docs.litellm.ai/docs/providers/gemini/videos), [`/images/edits`](https://docs.litellm.ai/docs/image_edits)Pass-through Endpoint[Supported](https://docs.litellm.ai/docs/pass_through/google_ai_studio)

Gemini API vs Vertex AI

Model FormatProviderAuth Required`gemini/gemini-2.0-flash`Gemini API`GEMINI_API_KEY` (simple API key)`vertex_ai/gemini-2.0-flash`Vertex AIGCP credentials + project`gemini-2.0-flash` (no prefix)Vertex AIGCP credentials + project

**If you just want to use an API key** (like OpenAI), use the `gemini/` prefix.

Models without a prefix default to Vertex AI which requires full GCP authentication.

## API Keys[â€‹](#api-keys "Direct link to API Keys")

```
import os
os.environ["GEMINI_API_KEY"]="your-api-key"
```

## Sample Usage[â€‹](#sample-usage "Direct link to Sample Usage")

```
from litellm import completion
import os

os.environ['GEMINI_API_KEY']=""
response = completion(
    model="gemini/gemini-pro",
    messages=[{"role":"user","content":"write code for saying hi from LiteLLM"}]
)
```

## Supported OpenAI Params[â€‹](#supported-openai-params "Direct link to Supported OpenAI Params")

- temperature
- top\_p
- max\_tokens
- max\_completion\_tokens
- stream
- tools
- tool\_choice
- functions
- response\_format
- n
- stop
- logprobs
- frequency\_penalty
- modalities
- reasoning\_content
- audio (for TTS models only)

**Anthropic Params**

- thinking (used to set max budget tokens across anthropic/gemini models)

[**See Updated List**](https://github.com/BerriAI/litellm/blob/main/litellm/llms/gemini/chat/transformation.py#L70)

## Usage - Thinking / `reasoning_content`[â€‹](#usage---thinking--reasoning_content "Direct link to usage---thinking--reasoning_content")

LiteLLM translates OpenAI's `reasoning_effort` to Gemini's `thinking` parameter. [Code](https://github.com/BerriAI/litellm/blob/620664921902d7a9bfb29897a7b27c1a7ef4ddfb/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py#L362)

**Cost Optimization:** Use `reasoning_effort="none"` (OpenAI standard) for significant cost savings - up to 96% cheaper. [Google's docs](https://ai.google.dev/gemini-api/docs/openai)

info

Note: Reasoning cannot be turned off on Gemini 2.5 Pro models.

Gemini 3 Models

For **Gemini 3+ models** (e.g., `gemini-3-pro-preview`), LiteLLM automatically maps `reasoning_effort` to the new `thinking_level` parameter instead of `thinking_budget`. The `thinking_level` parameter uses `"low"` or `"high"` values for better control over reasoning depth.

Image Models

**Gemini image models** (e.g., `gemini-3-pro-image-preview`, `gemini-2.0-flash-exp-image-generation`) do **not** support the `thinking_level` parameter. LiteLLM automatically excludes image models from receiving thinking configuration to prevent API errors.

**Mapping for Gemini 2.5 and earlier models**

reasoning\_effortthinkingNotes"none""budget\_tokens": 0, "includeThoughts": falseðŸ’° **Recommended for cost optimization** - OpenAI-compatible, always 0"disable""budget\_tokens": DEFAULT (0), "includeThoughts": falseLiteLLM-specific, configurable via env var"low""budget\_tokens": 1024"medium""budget\_tokens": 2048"high""budget\_tokens": 4096

**Mapping for Gemini 3+ models**

reasoning\_effortthinking\_levelNotes"minimal""low"Minimizes latency and cost"low""low"Best for simple instruction following or chat"medium""high"Maps to high (medium not yet available)"high""high"Maximizes reasoning depth"disable""low"Cannot fully disable thinking in Gemini 3"none""low"Cannot fully disable thinking in Gemini 3

- SDK
- PROXY

```
from litellm import completion

# Cost-optimized: Use reasoning_effort="none" for best pricing
resp = completion(
    model="gemini/gemini-2.0-flash-thinking-exp-01-21",
    messages=[{"role":"user","content":"What is the capital of France?"}],
    reasoning_effort="none",# Up to 96% cheaper!
)

# Or use other levels: "low", "medium", "high"
resp = completion(
    model="gemini/gemini-2.5-flash-preview-04-17",
    messages=[{"role":"user","content":"What is the capital of France?"}],
    reasoning_effort="low",
)

```

### Gemini 3+ Models - `thinking_level` Parameter[â€‹](#gemini-3-models---thinking_level-parameter "Direct link to gemini-3-models---thinking_level-parameter")

For Gemini 3+ models (e.g., `gemini-3-pro-preview`), you can use the new `thinking_level` parameter directly:

- SDK
- PROXY

```
from litellm import completion

# Use thinking_level for Gemini 3 models
resp = completion(
    model="gemini/gemini-3-pro-preview",
    messages=[{"role":"user","content":"Solve this complex math problem step by step."}],
    reasoning_effort="high",# Options: "low" or "high"
)

# Low thinking level for faster, simpler tasks
resp = completion(
    model="gemini/gemini-3-pro-preview",
    messages=[{"role":"user","content":"What is the weather today?"}],
    reasoning_effort="low",# Minimizes latency and cost
)
```

warning

**Temperature Recommendation for Gemini 3 Models**

For Gemini 3 models, LiteLLM defaults `temperature` to `1.0` and strongly recommends keeping it at this default. Setting `temperature < 1.0` can cause:

- Infinite loops
- Degraded reasoning performance
- Failure on complex tasks

LiteLLM will automatically set `temperature=1.0` if not specified for Gemini 3+ models.

**Expected Response**

```
ModelResponse(
id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',
    created=1740470510,
    model='claude-3-7-sonnet-20250219',
object='chat.completion',
    system_fingerprint=None,
    choices=[
        Choices(
            finish_reason='stop',
            index=0,
            message=Message(
                content="The capital of France is Paris.",
                role='assistant',
                tool_calls=None,
                function_call=None,
                reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'
),
)
],
    usage=Usage(
        completion_tokens=68,
        prompt_tokens=42,
        total_tokens=110,
        completion_tokens_details=None,
        prompt_tokens_details=PromptTokensDetailsWrapper(
            audio_tokens=None,
            cached_tokens=0,
            text_tokens=None,
            image_tokens=None
),
        cache_creation_input_tokens=0,
        cache_read_input_tokens=0
)
)
```

### Pass `thinking` to Gemini models[â€‹](#pass-thinking-to-gemini-models "Direct link to pass-thinking-to-gemini-models")

You can also pass the `thinking` parameter to Gemini models.

This is translated to Gemini's [`thinkingConfig` parameter](https://ai.google.dev/gemini-api/docs/thinking#set-budget).

- SDK
- PROXY

```
response = litellm.completion(
  model="gemini/gemini-2.5-flash-preview-04-17",
  messages=[{"role":"user","content":"What is the capital of France?"}],
  thinking={"type":"enabled","budget_tokens":1024},
)
```

## Text-to-Speech (TTS) Audio Output[â€‹](#text-to-speech-tts-audio-output "Direct link to Text-to-Speech (TTS) Audio Output")

info

LiteLLM supports Gemini TTS models that can generate audio responses using the OpenAI-compatible `audio` parameter format.

### Supported Models[â€‹](#supported-models "Direct link to Supported Models")

LiteLLM supports Gemini TTS models with audio capabilities (e.g. `gemini-2.5-flash-preview-tts` and `gemini-2.5-pro-preview-tts`). For the complete list of available TTS models and voices, see the [official Gemini TTS documentation](https://ai.google.dev/gemini-api/docs/speech-generation).

### Limitations[â€‹](#limitations "Direct link to Limitations")

warning

**Important Limitations**:

- Gemini TTS models only support the `pcm16` audio format
- **Streaming support has not been added** to TTS models yet
- The `modalities` parameter must be set to `['audio']` for TTS requests

### Quick Start[â€‹](#quick-start "Direct link to Quick Start")

- SDK
- PROXY

```
from litellm import completion
import os

os.environ['GEMINI_API_KEY']="your-api-key"

response = completion(
    model="gemini/gemini-2.5-flash-preview-tts",
    messages=[{"role":"user","content":"Say hello in a friendly voice"}],
    modalities=["audio"],# Required for TTS models
    audio={
"voice":"Kore",
"format":"pcm16"# Required: must be "pcm16"
}
)

print(response)
```

### Advanced Usage[â€‹](#advanced-usage "Direct link to Advanced Usage")

You can combine TTS with other Gemini features:

```
response = completion(
    model="gemini/gemini-2.5-pro-preview-tts",
    messages=[
{"role":"system","content":"You are a helpful assistant that speaks clearly."},
{"role":"user","content":"Explain quantum computing in simple terms"}
],
    modalities=["audio"],
    audio={
"voice":"Charon",
"format":"pcm16"
},
    temperature=0.7,
    max_tokens=150
)
```

For more information about Gemini's TTS capabilities and available voices, see the [official Gemini TTS documentation](https://ai.google.dev/gemini-api/docs/speech-generation).

## Passing Gemini Specific Params[â€‹](#passing-gemini-specific-params "Direct link to Passing Gemini Specific Params")

### Response schema[â€‹](#response-schema "Direct link to Response schema")

LiteLLM supports sending `response_schema` as a param for Gemini-1.5-Pro on Google AI Studio.

**Response Schema**

- SDK
- PROXY

```
from litellm import completion 
import json 
import os 

os.environ['GEMINI_API_KEY']=""

messages =[
{
"role":"user",
"content":"List 5 popular cookie recipes."
}
]

response_schema ={
"type":"array",
"items":{
"type":"object",
"properties":{
"recipe_name":{
"type":"string",
},
},
"required":["recipe_name"],
},
}


completion(
    model="gemini/gemini-1.5-pro",
    messages=messages,
    response_format={"type":"json_object","response_schema": response_schema}# ðŸ‘ˆ KEY CHANGE
)

print(json.loads(completion.choices[0].message.content))
```

**Validate Schema**

To validate the response\_schema, set `enforce_validation: true`.

- SDK
- PROXY

```
from litellm import completion, JSONSchemaValidationError
try:
	completion(
    model="gemini/gemini-1.5-pro",
    messages=messages,
    response_format={
"type":"json_object",
"response_schema": response_schema,
"enforce_validation": true # ðŸ‘ˆ KEY CHANGE
}
)
except JSONSchemaValidationError as e:
print("Raw Response: {}".format(e.raw_response))
raise e
```

LiteLLM will validate the response against the schema, and raise a `JSONSchemaValidationError` if the response does not match the schema.

JSONSchemaValidationError inherits from `openai.APIError`

Access the raw response with `e.raw_response`

### GenerationConfig Params[â€‹](#generationconfig-params "Direct link to GenerationConfig Params")

To pass additional GenerationConfig params - e.g. `topK`, just pass it in the request body of the call, and LiteLLM will pass it straight through as a key-value pair in the request body.

[**See Gemini GenerationConfigParams**](https://ai.google.dev/api/generate-content#v1beta.GenerationConfig)

- SDK
- PROXY

```
from litellm import completion 
import json 
import os 

os.environ['GEMINI_API_KEY']=""

messages =[
{
"role":"user",
"content":"List 5 popular cookie recipes."
}
]

completion(
    model="gemini/gemini-1.5-pro",
    messages=messages,
    topK=1# ðŸ‘ˆ KEY CHANGE
)

print(json.loads(completion.choices[0].message.content))
```

**Validate Schema**

To validate the response\_schema, set `enforce_validation: true`.

- SDK
- PROXY

```
from litellm import completion, JSONSchemaValidationError
try:
	completion(
    model="gemini/gemini-1.5-pro",
    messages=messages,
    response_format={
"type":"json_object",
"response_schema": response_schema,
"enforce_validation": true # ðŸ‘ˆ KEY CHANGE
}
)
except JSONSchemaValidationError as e:
print("Raw Response: {}".format(e.raw_response))
raise e
```

## Specifying Safety Settings[â€‹](#specifying-safety-settings "Direct link to Specifying Safety Settings")

In certain use-cases you may need to make calls to the models and pass [safety settings](https://ai.google.dev/docs/safety_setting_gemini) different from the defaults. To do so, simple pass the `safety_settings` argument to `completion` or `acompletion`. For example:

```
response = completion(
    model="gemini/gemini-pro",
    messages=[{"role":"user","content":"write code for saying hi from LiteLLM"}],
    safety_settings=[
{
"category":"HARM_CATEGORY_HARASSMENT",
"threshold":"BLOCK_NONE",
},
{
"category":"HARM_CATEGORY_HATE_SPEECH",
"threshold":"BLOCK_NONE",
},
{
"category":"HARM_CATEGORY_SEXUALLY_EXPLICIT",
"threshold":"BLOCK_NONE",
},
{
"category":"HARM_CATEGORY_DANGEROUS_CONTENT",
"threshold":"BLOCK_NONE",
},
]
)
```

```
from litellm import completion
import os
# set env
os.environ["GEMINI_API_KEY"]=".."

tools =[
{
"type":"function",
"function":{
"name":"get_current_weather",
"description":"Get the current weather in a given location",
"parameters":{
"type":"object",
"properties":{
"location":{
"type":"string",
"description":"The city and state, e.g. San Francisco, CA",
},
"unit":{"type":"string","enum":["celsius","fahrenheit"]},
},
"required":["location"],
},
},
}
]
messages =[{"role":"user","content":"What's the weather like in Boston today?"}]

response = completion(
    model="gemini/gemini-1.5-flash",
    messages=messages,
    tools=tools,
)
# Add any assertions, here to check response args
print(response)
assertisinstance(response.choices[0].message.tool_calls[0].function.name,str)
assertisinstance(
    response.choices[0].message.tool_calls[0].function.arguments,str
)


```

### Google Search Tool[â€‹](#google-search-tool "Direct link to Google Search Tool")

- SDK
- PROXY

```
from litellm import completion
import os

os.environ["GEMINI_API_KEY"]=".."

tools =[{"googleSearch":{}}]# ðŸ‘ˆ ADD GOOGLE SEARCH

response = completion(
    model="gemini/gemini-2.0-flash",
    messages=[{"role":"user","content":"What is the weather in San Francisco?"}],
    tools=tools,
)

print(response)
```

### URL Context[â€‹](#url-context "Direct link to URL Context")

- SDK
- PROXY

```
from litellm import completion
import os

os.environ["GEMINI_API_KEY"]=".."

# ðŸ‘‡ ADD URL CONTEXT
tools =[{"urlContext":{}}]

response = completion(
    model="gemini/gemini-2.0-flash",
    messages=[{"role":"user","content":"Summarize this document: https://ai.google.dev/gemini-api/docs/models"}],
    tools=tools,
)

print(response)

# Access URL context metadata
url_context_metadata = response.model_extra['vertex_ai_url_context_metadata']
urlMetadata = url_context_metadata[0]['urlMetadata'][0]
print(f"Retrieved URL: {urlMetadata['retrievedUrl']}")
print(f"Retrieval Status: {urlMetadata['urlRetrievalStatus']}")
```

### Google Search Retrieval[â€‹](#google-search-retrieval "Direct link to Google Search Retrieval")

- SDK
- PROXY

```
from litellm import completion
import os

os.environ["GEMINI_API_KEY"]=".."

tools =[{"googleSearch":{}}]# ðŸ‘ˆ ADD GOOGLE SEARCH

response = completion(
    model="gemini/gemini-2.0-flash",
    messages=[{"role":"user","content":"What is the weather in San Francisco?"}],
    tools=tools,
)

print(response)
```

### Code Execution Tool[â€‹](#code-execution-tool "Direct link to Code Execution Tool")

- SDK
- PROXY

```
from litellm import completion
import os

os.environ["GEMINI_API_KEY"]=".."

tools =[{"codeExecution":{}}]# ðŸ‘ˆ ADD GOOGLE SEARCH

response = completion(
    model="gemini/gemini-2.0-flash",
    messages=[{"role":"user","content":"What is the weather in San Francisco?"}],
    tools=tools,
)

print(response)
```

### Computer Use Tool[â€‹](#computer-use-tool "Direct link to Computer Use Tool")

- LiteLLM Python SDK
- LiteLLM Proxy Server

```
from litellm import completion
import os

os.environ["GEMINI_API_KEY"]="your-api-key"

# Computer Use tool with browser environment
tools =[
{
"type":"computer_use",
"environment":"browser",# optional: "browser" or "unspecified"
"excluded_predefined_functions":["drag_and_drop"]# optional
}
]

messages =[
{
"role":"user",
"content":[
{
"type":"text",
"text":"Navigate to google.com and search for 'LiteLLM'"
},
{
"type":"image_url",
"image_url":{
"url":"data:image/png;base64,..."# screenshot of current browser state
}
}
]
}
]

response = completion(
    model="gemini/gemini-2.5-computer-use-preview-10-2025",
    messages=messages,
    tools=tools,
)

print(response)

# Handling tool responses with screenshots
# When the model makes a tool call, send the response back with a screenshot:
if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]

# Add assistant message with tool call
    messages.append(response.choices[0].message.model_dump())

# Add tool response with screenshot
    messages.append({
"role":"tool",
"tool_call_id": tool_call.id,
"content":[
{
"type":"text",
"text":'{"url": "https://example.com", "status": "completed"}'
},
{
"type":"input_image",
"image_url":"data:image/png;base64,..."# New screenshot after action (Can send an image url as well, litellm handles the conversion)
}
]
})

# Continue conversation with updated screenshot
    response = completion(
        model="gemini/gemini-2.5-computer-use-preview-10-2025",
        messages=messages,
        tools=tools,
)
```

### Environment Mapping[â€‹](#environment-mapping "Direct link to Environment Mapping")

LiteLLM InputGemini API Value`"browser"``ENVIRONMENT_BROWSER``"unspecified"``ENVIRONMENT_UNSPECIFIED``ENVIRONMENT_BROWSER``ENVIRONMENT_BROWSER` (passed through)`ENVIRONMENT_UNSPECIFIED``ENVIRONMENT_UNSPECIFIED` (passed through)

## Thought Signatures[â€‹](#thought-signatures "Direct link to Thought Signatures")

Thought signatures are encrypted representations of the model's internal reasoning process for a given turn in a conversation. By passing thought signatures back to the model in subsequent requests, you provide it with the context of its previous thoughts, allowing it to build upon its reasoning and maintain a coherent line of inquiry.

Thought signatures are particularly important for multi-turn function calling scenarios where the model needs to maintain context across multiple tool invocations.

### How Thought Signatures Work[â€‹](#how-thought-signatures-work "Direct link to How Thought Signatures Work")

- **Function calls with signatures**: When Gemini returns a function call, it includes a `thought_signature` in the response
- **Preservation**: LiteLLM automatically extracts and stores thought signatures in `provider_specific_fields` of tool calls
- **Return in conversation history**: When you include the assistant's message with tool calls in subsequent requests, LiteLLM automatically preserves and returns the thought signatures to Gemini
- **Parallel function calls**: Only the first function call in a parallel set has a thought signature
- **Sequential function calls**: Each function call in a multi-step sequence has its own signature

### Enabling Thought Signatures[â€‹](#enabling-thought-signatures "Direct link to Enabling Thought Signatures")

To enable thought signatures, you need to enable thinking/reasoning:

- SDK
- PROXY

```
from litellm import completion

response = completion(
    model="gemini/gemini-2.5-flash",
    messages=[{"role":"user","content":"What's the weather in Tokyo?"}],
    tools=[...],
    reasoning_effort="low",# Enable thinking to get thought signatures
)
```

### Multi-Turn Function Calling with Thought Signatures[â€‹](#multi-turn-function-calling-with-thought-signatures "Direct link to Multi-Turn Function Calling with Thought Signatures")

When building conversation history for multi-turn function calling, you must include the thought signatures from previous responses. LiteLLM handles this automatically when you append the full assistant message to your conversation history.

- OpenAI Client
- cURL

```
from openai import OpenAI
import json

client = OpenAI(api_key="sk-1234", base_url="http://localhost:4000")

defget_current_temperature(location:str)->dict:
"""Gets the current weather temperature for a given location."""
return{"temperature":30,"unit":"celsius"}

defset_thermostat_temperature(temperature:int)->dict:
"""Sets the thermostat to a desired temperature."""
return{"status":"success"}

get_weather_declaration ={
"name":"get_current_temperature",
"description":"Gets the current weather temperature for a given location.",
"parameters":{
"type":"object",
"properties":{"location":{"type":"string"}},
"required":["location"],
},
}

set_thermostat_declaration ={
"name":"set_thermostat_temperature",
"description":"Sets the thermostat to a desired temperature.",
"parameters":{
"type":"object",
"properties":{"temperature":{"type":"integer"}},
"required":["temperature"],
},
}

# Initial request
messages =[
{"role":"user","content":"If it's too hot or too cold in London, set the thermostat to a comfortable level."}
]

response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=messages,
    tools=[get_weather_declaration, set_thermostat_declaration],
    reasoning_effort="low"
)

# Append the assistant's message (includes thought signatures automatically)
messages.append(response.choices[0].message)

# Execute tool calls and append results
for tool_call in response.choices[0].message.tool_calls:
if tool_call.function.name =="get_current_temperature":
        result = get_current_temperature(**json.loads(tool_call.function.arguments))
        messages.append({
"role":"tool",
"content": json.dumps(result),
"tool_call_id": tool_call.id
})

# Second request - thought signatures are automatically preserved
response2 = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=messages,
    tools=[get_weather_declaration, set_thermostat_declaration],
    reasoning_effort="low"
)

print(response2.choices[0].message.content)
```

### Important Notes[â€‹](#important-notes "Direct link to Important Notes")

1. **Automatic Handling**: LiteLLM automatically extracts thought signatures from Gemini responses and preserves them when you include assistant messages in conversation history. You don't need to manually extract or manage them.
2. **Parallel Function Calls**: When the model makes parallel function calls, only the first function call will have a thought signature. Subsequent parallel calls won't have signatures.
3. **Sequential Function Calls**: In multi-step function calling scenarios, each step's first function call will have its own thought signature that must be preserved.
4. **Required for Context**: Thought signatures are essential for maintaining reasoning context across multi-turn conversations with function calling. Without them, the model may lose context of its previous reasoning.
5. **Format**: Thought signatures are stored in `provider_specific_fields.thought_signature` of tool calls in the response, and are automatically included when you append the assistant message to your conversation history.
6. **Chat Completions Clients**: With chat completions clients where you cannot control whether or not the previous assistant message is included as-is (ex langchain's ChatOpenAI), LiteLLM also preserves the thought signature by appending it to the tool call id (`call_123__thought__<thought-signature>`) and extracting it back out before sending the outbound request to Gemini.

## JSON Mode[â€‹](#json-mode "Direct link to JSON Mode")

- SDK
- PROXY

```
from litellm import completion 
import json 
import os 

os.environ['GEMINI_API_KEY']=""

messages =[
{
"role":"user",
"content":"List 5 popular cookie recipes."
}
]


completion(
    model="gemini/gemini-1.5-pro",
    messages=messages,
    response_format={"type":"json_object"}# ðŸ‘ˆ KEY CHANGE
)

print(json.loads(completion.choices[0].message.content))
```

## Gemini-Pro-Vision

LiteLLM Supports the following image types passed in `url`

- Images with direct links - [https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg](https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg)
- Image in local storage - ./localimage.jpeg

For Gemini 3+ models, LiteLLM supports per-part media resolution control using OpenAI's `detail` parameter. This allows you to specify different resolution levels for individual images and videos in your request, whether using `image_url` or `file` content types.

**Supported `detail` values:**

- `"low"` - Maps to `media_resolution: "low"` (280 tokens for images, 70 tokens per frame for videos)
- `"medium"` - Maps to `media_resolution: "medium"`
- `"high"` - Maps to `media_resolution: "high"` (1120 tokens for images)
- `"ultra_high"` - Maps to `media_resolution: "ultra_high"`
- `"auto"` or `None` - Model decides optimal resolution (no `media_resolution` set)

**Usage Examples:**

- Images
- Videos with Files

```
from litellm import completion

messages =[
{
"role":"user",
"content":[
{
"type":"image_url",
"image_url":{
"url":"https://example.com/chart.png",
"detail":"high"# High resolution for detailed chart analysis
}
},
{
"type":"text",
"text":"Analyze this chart"
},
{
"type":"image_url",
"image_url":{
"url":"https://example.com/icon.png",
"detail":"low"# Low resolution for simple icon
}
}
]
}
]

response = completion(
    model="gemini/gemini-3-pro-preview",
    messages=messages,
)
```

info

**Per-Part Resolution:** Each image or video in your request can have its own `detail` setting, allowing mixed-resolution requests (e.g., a high-res chart alongside a low-res icon). This feature works with both `image_url` and `file` content types, and is only available for Gemini 3+ models.

For Gemini 3+ models, LiteLLM supports fine-grained video processing control through the `video_metadata` field. This allows you to specify frame extraction rates and time ranges for video analysis.

**Supported `video_metadata` parameters:**

ParameterTypeDescriptionExample`fps`NumberFrame extraction rate (frames per second)`5``start_offset`StringStart time for video clip processing`"10s"``end_offset`StringEnd time for video clip processing`"60s"`

note

**Field Name Conversion:** LiteLLM automatically converts snake\_case field names to camelCase for the Gemini API:

- `start_offset` â†’ `startOffset`
- `end_offset` â†’ `endOffset`
- `fps` remains unchanged

warning

- **Gemini 3+ Only:** This feature is only available for Gemini 3.0 and newer models
- **Video Files Recommended:** While `video_metadata` is designed for video files, error handling for other media types is delegated to the Vertex AI API
- **File Formats Supported:** Works with `gs://`, `https://`, and base64-encoded video files

**Usage Examples:**

- Basic Video Metadata
- Combined with Detail
- PROXY

```
from litellm import completion

response = completion(
    model="gemini/gemini-3-pro-preview",
    messages=[
{
"role":"user",
"content":[
{"type":"text","text":"Analyze this video clip"},
{
"type":"file",
"file":{
"file_id":"gs://my-bucket/video.mp4",
"format":"video/mp4",
"video_metadata":{
"fps":5,# Extract 5 frames per second
"start_offset":"10s",# Start from 10 seconds
"end_offset":"60s"# End at 60 seconds
}
}
}
]
}
]
)

print(response.choices[0].message.content)
```

## Sample Usage[â€‹](#sample-usage-1 "Direct link to Sample Usage")

```
import os
import litellm
from dotenv import load_dotenv

# Load the environment variables from .env file
load_dotenv()
os.environ["GEMINI_API_KEY"]= os.getenv('GEMINI_API_KEY')

prompt ='Describe the image in a few sentences.'
# Note: You can pass here the URL or Path of image directly.
image_url ='https://storage.googleapis.com/github-repo/img/gemini/intro/landmark3.jpg'

# Create the messages payload according to the documentation
messages =[
{
"role":"user",
"content":[
{
"type":"text",
"text": prompt
},
{
"type":"image_url",
"image_url":{"url": image_url}
}
]
}
]

# Make the API call to Gemini model
response = litellm.completion(
    model="gemini/gemini-pro-vision",
    messages=messages,
)

# Extract the response content
content = response.get('choices',[{}])[0].get('message',{}).get('content')

# Print the result
print(content)
```

## Usage - PDF / Videos / etc. Files[â€‹](#usage---pdf--videos--etc-files "Direct link to Usage - PDF / Videos / etc. Files")

### Inline Data (e.g. audio stream)[â€‹](#inline-data-eg-audio-stream "Direct link to Inline Data (e.g. audio stream)")

LiteLLM follows the OpenAI format and accepts sending inline data as an encoded base64 string.

The format to follow is

```
data:<mime_type>;base64,<encoded_data>
```

\** LITELLM CALL \**

```
import litellm
from pathlib import Path
import base64
import os

os.environ["GEMINI_API_KEY"]=""

litellm.set_verbose =True# ðŸ‘ˆ See Raw call 

audio_bytes = Path("speech_vertex.mp3").read_bytes()
encoded_data = base64.b64encode(audio_bytes).decode("utf-8")
print("Audio Bytes = {}".format(audio_bytes))
model ="gemini/gemini-1.5-flash"
response = litellm.completion(
    model=model,
    messages=[
{
"role":"user",
"content":[
{"type":"text","text":"Please summarize the audio."},
{
"type":"file",
"file":{
"file_data":"data:audio/mp3;base64,{}".format(encoded_data),# ðŸ‘ˆ SET MIME_TYPE + DATA
}
},
],
}
],
)
```

\** Equivalent GOOGLE API CALL \**

```
# Initialize a Gemini model appropriate for your use case.
model = genai.GenerativeModel('models/gemini-1.5-flash')

# Create the prompt.
prompt ="Please summarize the audio."

# Load the samplesmall.mp3 file into a Python Blob object containing the audio
# file's bytes and then pass the prompt and the audio to Gemini.
response = model.generate_content([
    prompt,
{
"mime_type":"audio/mp3",
"data": pathlib.Path('samplesmall.mp3').read_bytes()
}
])

# Output Gemini's response to the prompt and the inline audio.
print(response.text)
```

### https:// file[â€‹](#https-file "Direct link to https:// file")

```
import litellm
import os

os.environ["GEMINI_API_KEY"]=""

litellm.set_verbose =True# ðŸ‘ˆ See Raw call 

model ="gemini/gemini-1.5-flash"
response = litellm.completion(
    model=model,
    messages=[
{
"role":"user",
"content":[
{"type":"text","text":"Please summarize the file."},
{
"type":"file",
"file":{
"file_id":"https://storage...",# ðŸ‘ˆ SET THE IMG URL
"format":"application/pdf"# OPTIONAL
}
},
],
}
],
)
```

### gs:// file[â€‹](#gs-file "Direct link to gs:// file")

```
import litellm
import os

os.environ["GEMINI_API_KEY"]=""

litellm.set_verbose =True# ðŸ‘ˆ See Raw call 

model ="gemini/gemini-1.5-flash"
response = litellm.completion(
    model=model,
    messages=[
{
"role":"user",
"content":[
{"type":"text","text":"Please summarize the file."},
{
"type":"file",
"file":{
"file_id":"gs://storage...",# ðŸ‘ˆ SET THE IMG URL
"format":"application/pdf"# OPTIONAL
}
},
],
}
],
)
```

## Chat Models[â€‹](#chat-models "Direct link to Chat Models")

tip

**We support ALL Gemini models, just set `model=gemini/<any-model-on-gemini>` as a prefix when sending litellm requests**

Model NameFunction CallRequired OS Variablesgemini-pro`completion(model='gemini/gemini-pro', messages)``os.environ['GEMINI_API_KEY']`gemini-1.5-pro-latest`completion(model='gemini/gemini-1.5-pro-latest', messages)``os.environ['GEMINI_API_KEY']`gemini-2.0-flash`completion(model='gemini/gemini-2.0-flash', messages)``os.environ['GEMINI_API_KEY']`gemini-2.0-flash-exp`completion(model='gemini/gemini-2.0-flash-exp', messages)``os.environ['GEMINI_API_KEY']`gemini-2.0-flash-lite-preview-02-05`completion(model='gemini/gemini-2.0-flash-lite-preview-02-05', messages)``os.environ['GEMINI_API_KEY']`gemini-2.5-flash-preview-09-2025`completion(model='gemini/gemini-2.5-flash-preview-09-2025', messages)``os.environ['GEMINI_API_KEY']`gemini-2.5-flash-lite-preview-09-2025`completion(model='gemini/gemini-2.5-flash-lite-preview-09-2025', messages)``os.environ['GEMINI_API_KEY']`gemini-flash-latest`completion(model='gemini/gemini-flash-latest', messages)``os.environ['GEMINI_API_KEY']`gemini-flash-lite-latest`completion(model='gemini/gemini-flash-lite-latest', messages)``os.environ['GEMINI_API_KEY']`

## Context Caching[â€‹](#context-caching "Direct link to Context Caching")

Use Google AI Studio context caching is supported by

```
{
    {
        "role": "system",
        "content": ...,
        "cache_control": {"type": "ephemeral"} # ðŸ‘ˆ KEY CHANGE
    },
    ...
}
```

in your message content block.

### Custom TTL Support[â€‹](#custom-ttl-support "Direct link to Custom TTL Support")

You can now specify a custom Time-To-Live (TTL) for your cached content using the `ttl` parameter:

```
{
    {
        "role": "system",
        "content": ...,
        "cache_control": {
            "type": "ephemeral",
            "ttl": "3600s"  # ðŸ‘ˆ Cache for 1 hour
        }
    },
    ...
}
```

**TTL Format Requirements:**

- Must be a string ending with 's' for seconds
- Must contain a positive number (can be decimal)
- Examples: `"3600s"` (1 hour), `"7200s"` (2 hours), `"1800s"` (30 minutes), `"1.5s"` (1.5 seconds)

**TTL Behavior:**

- If multiple cached messages have different TTLs, the first valid TTL encountered will be used
- Invalid TTL formats are ignored and the cache will use Google's default expiration time
- If no TTL is specified, Google's default cache expiration (approximately 1 hour) applies

### Architecture Diagram[â€‹](#architecture-diagram "Direct link to Architecture Diagram")

**Notes:**

- [Relevant code](https://github.com/BerriAI/litellm/blob/main/litellm/llms/vertex_ai/context_caching/vertex_ai_context_caching.py#L255)
- Gemini Context Caching only allows 1 block of continuous messages to be cached.
- If multiple non-continuous blocks contain `cache_control` - the first continuous block will be used. (sent to `/cachedContent` in the [Gemini format](https://ai.google.dev/api/caching#cache_create-SHELL))
- The raw request to Gemini's `/generateContent` endpoint looks like this:

```
curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001:generateContent?key=$GOOGLE_API_KEY" \
-H 'Content-Type: application/json' \
-d '{
      "contents": [
        {
          "parts":[{
            "text": "Please summarize this transcript"
          }],
          "role": "user"
        },
      ],
      "cachedContent": "'$CACHE_NAME'"
    }'

```

### Example Usage[â€‹](#example-usage "Direct link to Example Usage")

- SDK
- SDK with Custom TTL
- PROXY

```
from litellm import completion 

for _ inrange(2):
    resp = completion(
        model="gemini/gemini-1.5-pro",
        messages=[
# System Message
{
"role":"system",
"content":[
{
"type":"text",
"text":"Here is the full text of a complex legal agreement"*4000,
"cache_control":{"type":"ephemeral"},# ðŸ‘ˆ KEY CHANGE
}
],
},
# marked for caching with the cache_control parameter, so that this checkpoint can read from the previous cache.
{
"role":"user",
"content":[
{
"type":"text",
"text":"What are the key terms and conditions in this agreement?",
"cache_control":{"type":"ephemeral"},
}
],
}]
)

print(resp.usage)# ðŸ‘ˆ 2nd usage block will be less, since cached tokens used
```

## Image Generation[â€‹](#image-generation "Direct link to Image Generation")

- SDK
- PROXY

```
from litellm import completion 

response = completion(
    model="gemini/gemini-2.0-flash-exp-image-generation",
    messages=[{"role":"user","content":"Generate an image of a cat"}],
    modalities=["image","text"],
)
assert response.choices[0].message.content isnotNone# "data:image/png;base64,e4rr.."
```

### Image Generation Pricing[â€‹](#image-generation-pricing "Direct link to Image Generation Pricing")

Gemini image generation models (like `gemini-3-pro-image-preview`) return `image_tokens` in the response usage. These tokens are priced differently from text tokens:

Token TypePrice per 1M tokensPrice per tokenText output$12$0.000012Image output$120$0.00012

The number of image tokens depends on the output resolution:

ResolutionTokens per imageCost per image1K-2K (1024x1024 to 2048x2048)1,120$0.1344K (4096x4096)2,000$0.24

LiteLLM automatically calculates costs using `output_cost_per_image_token` from the model pricing configuration.

**Example response usage:**

```
{
"completion_tokens_details":{
"reasoning_tokens":225,
"text_tokens":0,
"image_tokens":1120
}
}
```

For more details, see [Google's Gemini pricing documentation](https://ai.google.dev/gemini-api/docs/pricing).