---
title: Databricks | liteLLM
url: https://docs.litellm.ai/docs/providers/databricks
source: sitemap
fetched_at: 2026-01-21T19:48:50.748595358-03:00
rendered_js: false
word_count: 486
summary: This document explains how to integrate and use models hosted on Databricks with LiteLLM, covering various authentication methods, configuration settings, and advanced features like reasoning content.
tags:
    - databricks
    - litellm
    - authentication
    - oauth-m2m
    - python-sdk
    - reasoning-content
    - model-integration
category: guide
---

LiteLLM supports all models on Databricks

tip

**We support ALL Databricks models, just set `model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**

## Authentication[​](#authentication "Direct link to Authentication")

LiteLLM supports multiple authentication methods for Databricks, listed in order of preference:

### OAuth M2M (Recommended for Production)[​](#oauth-m2m-recommended-for-production "Direct link to OAuth M2M (Recommended for Production)")

OAuth Machine-to-Machine authentication using Service Principal credentials is the **recommended method for production** deployments per Databricks Partner requirements.

```
import os
from litellm import completion

# Set OAuth credentials (Service Principal)
os.environ["DATABRICKS_CLIENT_ID"]="your-service-principal-application-id"
os.environ["DATABRICKS_CLIENT_SECRET"]="your-service-principal-secret"
os.environ["DATABRICKS_API_BASE"]="https://adb-xxx.azuredatabricks.net/serving-endpoints"

response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages=[{"role":"user","content":"Hello!"}],
)
```

### Personal Access Token (PAT)[​](#personal-access-token-pat "Direct link to Personal Access Token (PAT)")

PAT authentication is supported for development and testing scenarios.

```
import os
from litellm import completion

os.environ["DATABRICKS_API_KEY"]="dapi..."# Your Personal Access Token
os.environ["DATABRICKS_API_BASE"]="https://adb-xxx.azuredatabricks.net/serving-endpoints"

response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages=[{"role":"user","content":"Hello!"}],
)
```

### Databricks SDK Authentication (Automatic)[​](#databricks-sdk-authentication-automatic "Direct link to Databricks SDK Authentication (Automatic)")

If no credentials are provided, LiteLLM will use the Databricks SDK for automatic authentication. This supports OAuth, Azure AD, and other unified auth methods configured in your environment.

```
from litellm import completion

# No environment variables needed - uses Databricks SDK unified auth
# Requires: pip install databricks-sdk
response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages=[{"role":"user","content":"Hello!"}],
)
```

## Custom User-Agent for Partner Attribution[​](#custom-user-agent-for-partner-attribution "Direct link to Custom User-Agent for Partner Attribution")

If you're building a product on top of LiteLLM that integrates with Databricks, you can pass your own partner identifier for proper attribution in Databricks telemetry.

The partner name will be prefixed to the LiteLLM user agent:

```
# Via parameter
response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages=[{"role":"user","content":"Hello!"}],
    user_agent="mycompany/1.0.0",
)
# Resulting User-Agent: mycompany_litellm/1.79.1

# Via environment variable
os.environ["DATABRICKS_USER_AGENT"]="mycompany/1.0.0"
# Resulting User-Agent: mycompany_litellm/1.79.1
```

InputResulting User-Agent(none)`litellm/1.79.1``mycompany/1.0.0``mycompany_litellm/1.79.1``partner_product/2.5.0``partner_product_litellm/1.79.1``acme``acme_litellm/1.79.1`

**Note:** The version from your custom user agent is ignored; LiteLLM's version is always used.

## Security[​](#security "Direct link to Security")

LiteLLM automatically redacts sensitive information (tokens, secrets, API keys) from all debug logs to prevent credential leakage. This includes:

- Authorization headers
- API keys and tokens
- Client secrets
- Personal access tokens (PATs)

## Usage[​](#usage "Direct link to Usage")

- SDK
- PROXY

### ENV VAR[​](#env-var "Direct link to ENV VAR")

```
import os 
os.environ["DATABRICKS_API_KEY"]=""
os.environ["DATABRICKS_API_BASE"]=""
```

### Example Call[​](#example-call "Direct link to Example Call")

```
from litellm import completion
import os
## set ENV variables
os.environ["DATABRICKS_API_KEY"]="databricks key"
os.environ["DATABRICKS_API_BASE"]="databricks base url"# e.g.: https://adb-3064715882934586.6.azuredatabricks.net/serving-endpoints

# Databricks dbrx-instruct call
response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages =[{"content":"Hello, how are you?","role":"user"}]
)
```

## Passing additional params - max\_tokens, temperature[​](#passing-additional-params---max_tokens-temperature "Direct link to Passing additional params - max_tokens, temperature")

See all litellm.completion supported params [here](https://docs.litellm.ai/docs/completion/input#translated-openai-params)

```
# !pip install litellm
from litellm import completion
import os
## set ENV variables
os.environ["DATABRICKS_API_KEY"]="databricks key"
os.environ["DATABRICKS_API_BASE"]="databricks api base"

# databricks dbrx call
response = completion(
    model="databricks/databricks-dbrx-instruct",
    messages =[{"content":"Hello, how are you?","role":"user"}],
    max_tokens=20,
    temperature=0.5
)
```

**proxy**

```
model_list:
-model_name: llama-3
litellm_params:
model: databricks/databricks-meta-llama-3-70b-instruct
api_key: os.environ/DATABRICKS_API_KEY
max_tokens:20
temperature:0.5
```

## Usage - Thinking / `reasoning_content`[​](#usage---thinking--reasoning_content "Direct link to usage---thinking--reasoning_content")

LiteLLM translates OpenAI's `reasoning_effort` to Anthropic's `thinking` parameter. [Code](https://github.com/BerriAI/litellm/blob/23051d89dd3611a81617d84277059cd88b2df511/litellm/llms/anthropic/chat/transformation.py#L298)

reasoning\_effortthinking"low""budget\_tokens": 1024"medium""budget\_tokens": 2048"high""budget\_tokens": 4096

Known Limitations:

- Support for passing thinking blocks back to Claude [Issue](https://github.com/BerriAI/litellm/issues/9790)

<!--THE END-->

- SDK
- PROXY

```
from litellm import completion
import os

# set ENV variables (can also be passed in to .completion() - e.g. `api_base`, `api_key`)
os.environ["DATABRICKS_API_KEY"]="databricks key"
os.environ["DATABRICKS_API_BASE"]="databricks base url"

resp = completion(
    model="databricks/databricks-claude-3-7-sonnet",
    messages=[{"role":"user","content":"What is the capital of France?"}],
    reasoning_effort="low",
)

```

**Expected Response**

```
ModelResponse(
id='chatcmpl-c542d76d-f675-4e87-8e5f-05855f5d0f5e',
    created=1740470510,
    model='claude-3-7-sonnet-20250219',
object='chat.completion',
    system_fingerprint=None,
    choices=[
        Choices(
            finish_reason='stop',
            index=0,
            message=Message(
                content="The capital of France is Paris.",
                role='assistant',
                tool_calls=None,
                function_call=None,
                provider_specific_fields={
'citations':None,
'thinking_blocks':[
{
'type':'thinking',
'thinking':'The capital of France is Paris. This is a very straightforward factual question.',
'signature':'EuYBCkQYAiJAy6...'
}
]
}
),
            thinking_blocks=[
{
'type':'thinking',
'thinking':'The capital of France is Paris. This is a very straightforward factual question.',
'signature':'EuYBCkQYAiJAy6AGB...'
}
],
            reasoning_content='The capital of France is Paris. This is a very straightforward factual question.'
)
],
    usage=Usage(
        completion_tokens=68,
        prompt_tokens=42,
        total_tokens=110,
        completion_tokens_details=None,
        prompt_tokens_details=PromptTokensDetailsWrapper(
            audio_tokens=None,
            cached_tokens=0,
            text_tokens=None,
            image_tokens=None
),
        cache_creation_input_tokens=0,
        cache_read_input_tokens=0
)
)
```

### Citations[​](#citations "Direct link to Citations")

Anthropic models served through Databricks can return citation metadata. LiteLLM exposes these via `response.choices[0].message.provider_specific_fields["citations"]`.

### Pass `thinking` to Anthropic models[​](#pass-thinking-to-anthropic-models "Direct link to pass-thinking-to-anthropic-models")

You can also pass the `thinking` parameter to Anthropic models.

You can also pass the `thinking` parameter to Anthropic models.

- SDK
- PROXY

```
from litellm import completion
import os

# set ENV variables (can also be passed in to .completion() - e.g. `api_base`, `api_key`)
os.environ["DATABRICKS_API_KEY"]="databricks key"
os.environ["DATABRICKS_API_BASE"]="databricks base url"

response = litellm.completion(
  model="databricks/databricks-claude-3-7-sonnet",
  messages=[{"role":"user","content":"What is the capital of France?"}],
  thinking={"type":"enabled","budget_tokens":1024},
)
```

## Supported Databricks Chat Completion Models[​](#supported-databricks-chat-completion-models "Direct link to Supported Databricks Chat Completion Models")

tip

**We support ALL Databricks models, just set `model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**

Model NameCommanddatabricks/databricks-claude-3-7-sonnet`completion(model='databricks/databricks/databricks-claude-3-7-sonnet', messages=messages)`databricks-meta-llama-3-1-70b-instruct`completion(model='databricks/databricks-meta-llama-3-1-70b-instruct', messages=messages)`databricks-meta-llama-3-1-405b-instruct`completion(model='databricks/databricks-meta-llama-3-1-405b-instruct', messages=messages)`databricks-dbrx-instruct`completion(model='databricks/databricks-dbrx-instruct', messages=messages)`databricks-meta-llama-3-70b-instruct`completion(model='databricks/databricks-meta-llama-3-70b-instruct', messages=messages)`databricks-llama-2-70b-chat`completion(model='databricks/databricks-llama-2-70b-chat', messages=messages)`databricks-mixtral-8x7b-instruct`completion(model='databricks/databricks-mixtral-8x7b-instruct', messages=messages)`databricks-mpt-30b-instruct`completion(model='databricks/databricks-mpt-30b-instruct', messages=messages)`databricks-mpt-7b-instruct`completion(model='databricks/databricks-mpt-7b-instruct', messages=messages)`

## Embedding Models[​](#embedding-models "Direct link to Embedding Models")

### Passing Databricks specific params - 'instruction'[​](#passing-databricks-specific-params---instruction "Direct link to Passing Databricks specific params - 'instruction'")

For embedding models, databricks lets you pass in an additional param 'instruction'. [Full Spec](https://github.com/BerriAI/litellm/blob/43353c28b341df0d9992b45c6ce464222ebd7984/litellm/llms/databricks.py#L164)

```
# !pip install litellm
from litellm import embedding
import os
## set ENV variables
os.environ["DATABRICKS_API_KEY"]="databricks key"
os.environ["DATABRICKS_API_BASE"]="databricks url"

# Databricks bge-large-en call
response = litellm.embedding(
      model="databricks/databricks-bge-large-en",
input=["good morning from litellm"],
      instruction="Represent this sentence for searching relevant passages:",
)
```

**proxy**

```
model_list:
-model_name: bge-large
litellm_params:
model: databricks/databricks-bge-large-en
api_key: os.environ/DATABRICKS_API_KEY
api_base: os.environ/DATABRICKS_API_BASE
instruction:"Represent this sentence for searching relevant passages:"
```

## Supported Databricks Embedding Models[​](#supported-databricks-embedding-models "Direct link to Supported Databricks Embedding Models")

tip

**We support ALL Databricks models, just set `model=databricks/<any-model-on-databricks>` as a prefix when sending litellm requests**

Model NameCommanddatabricks-bge-large-en`embedding(model='databricks/databricks-bge-large-en', messages=messages)`databricks-gte-large-en`embedding(model='databricks/databricks-gte-large-en', messages=messages)`