---
title: Agent evals
url: https://platform.openai.com/docs/guides/agent-evals.md
source: llms
fetched_at: 2026-01-24T16:13:32.715567355-03:00
rendered_js: false
word_count: 165
summary: This document provides an overview of tools and methodologies for measuring and improving the performance and consistency of AI agents on the OpenAI Platform.
tags:
    - agent-evaluations
    - openai-platform
    - trace-grading
    - model-testing
    - performance-metrics
    - iterative-improvement
category: guide
---

Agent evals
===========

Measure agent quality with reproducible evaluations.

The OpenAI Platform offers a suite of evaluation tools to help you ensure your agents perform consistently and accurately.

For identifying errors at the workflow-level, we recommend our [trace grading](/docs/guides/trace-grading) functionality.

For an easy way to build and iterate on your evals, we recommend exploring [Datasets](/docs/guides/evaluation-getting-started).

If you need advanced features such as evaluation against external models, want to interact with your eval runs via API, or want to run evaluations on a larger scale, consider using [Evals](/docs/guides/evals) instead.

Next steps
----------

For more inspiration, visit the [OpenAI Cookbook](https://cookbook.openai.com), which contains example code and links to third-party resources, or learn more about our tools for evals:

[

Getting started with evals: Datasets

Operate a flywheel of continuous improvement using evaluations.

](/docs/guides/evaluation-getting-started)[

Working with evals

Evaluate against external models, interact with evals via API, and more.

](/docs/guides/evals)[

Prompt optimizer

Use your dataset to automatically improve your prompts.

](/docs/guides/prompt-optimizer)[

Cookbook: Building resilient prompts with evals

Operate a flywheel of continuous improvement using evaluations.

](https://cookbook.openai.com/examples/evaluation/Building_resilient_prompts_using_an_evaluation_flywheel.md)