---
title: "\U0001F501 MLflow - OSS LLM Observability and Evaluation | liteLLM"
url: https://docs.litellm.ai/docs/observability/mlflow
source: sitemap
fetched_at: 2026-01-21T19:46:20.840402993-03:00
rendered_js: false
word_count: 337
summary: This document explains how to integrate MLflow with LiteLLM to enable auto-tracing, observability, and evaluation for Large Language Model applications.
tags:
    - mlflow
    - litellm
    - observability
    - tracing
    - opentelemetry
    - mlops
    - llm-evaluation
    - proxy-configuration
category: guide
---

## What is MLflow?[â€‹](#what-is-mlflow "Direct link to What is MLflow?")

**MLflow** is an end-to-end open source MLOps platform for [experiment tracking](https://www.mlflow.org/docs/latest/tracking.html), [model management](https://www.mlflow.org/docs/latest/models.html), [evaluation](https://www.mlflow.org/docs/latest/llms/llm-evaluate/index.html), [observability (tracing)](https://www.mlflow.org/docs/latest/llms/tracing/index.html), and [deployment](https://www.mlflow.org/docs/latest/deployment/index.html). MLflow empowers teams to collaboratively develop and refine LLM applications efficiently.

MLflowâ€™s integration with LiteLLM supports advanced observability compatible with OpenTelemetry.

## Getting Started[â€‹](#getting-started "Direct link to Getting Started")

Install MLflow:

```
pip install "litellm[mlflow]"
```

To enable MLflow auto tracing for LiteLLM:

```
import mlflow

mlflow.litellm.autolog()

# Alternative, you can set the callback manually in LiteLLM
# litellm.callbacks = ["mlflow"]
```

Since MLflow is open-source and free, **no sign-up or API key is needed to log traces!**

```
import litellm
import os

# Set your LLM provider's API key
os.environ["OPENAI_API_KEY"]=""

# Call LiteLLM as usual
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[
{"role":"user","content":"Hi ðŸ‘‹ - i'm openai"}
]
)
```

Open the MLflow UI and go to the `Traces` tab to view logged traces:

MLflow integration with LiteLLM support tracking tool calls in addition to the messages.

```
import mlflow

# Enable MLflow auto-tracing for LiteLLM
mlflow.litellm.autolog()

# Define the tool function.
defget_weather(location:str)->str:
if location =="Tokyo":
return"sunny"
elif location =="Paris":
return"rainy"
return"unknown"

# Define function spec
get_weather_tool ={
"type":"function",
"function":{
"name":"get_weather",
"description":"Get the current weather in a given location",
"parameters":{
"properties":{
"location":{
"description":"The city and state, e.g., San Francisco, CA",
"type":"string",
},
},
"required":["location"],
"type":"object",
},
},
}

# Call LiteLLM as usual
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[
{"role":"user","content":"What's the weather like in Paris today?"}
],
    tools=[get_weather_tool]
)
```

## Evaluation[â€‹](#evaluation "Direct link to Evaluation")

MLflow LiteLLM integration allow you to run qualitative assessment against LLM to evaluate or/and monitor your GenAI application.

Visit [Evaluate LLMs Tutorial](https://docs.litellm.ai/docs/tutorials/eval_suites) for the complete guidance on how to run evaluation suite with LiteLLM and MLflow.

## Exporting Traces to OpenTelemetry collectors[â€‹](#exporting-traces-to-opentelemetry-collectors "Direct link to Exporting Traces to OpenTelemetry collectors")

MLflow traces are compatible with OpenTelemetry. You can export traces to any OpenTelemetry collector (e.g., Jaeger, Zipkin, Datadog, New Relic) by setting the endpoint URL in the environment variables.

```
# Set the endpoint of the OpenTelemetry Collector
os.environ["OTEL_EXPORTER_OTLP_TRACES_ENDPOINT"] = "http://localhost:4317/v1/traces"
# Optionally, set the service name to group traces
os.environ["OTEL_SERVICE_NAME"] = "<your-service-name>"
```

See [MLflow documentation](https://mlflow.org/docs/latest/llms/tracing/index.html#using-opentelemetry-collector-for-exporting-traces) for more details.

## Combine LiteLLM Trace with Your Application Trace[â€‹](#combine-litellm-trace-with-your-application-trace "Direct link to Combine LiteLLM Trace with Your Application Trace")

LiteLLM is often part of larger LLM applications, such as agentic models. MLflow Tracing allows you to instrument custom Python code, which can then be combined with LiteLLM traces.

```
import litellm
import mlflow
from mlflow.entities import SpanType

# Enable MLflow auto-tracing for LiteLLM
mlflow.litellm.autolog()


classCustomAgent:
# Use @mlflow.trace to instrument Python functions.
@mlflow.trace(span_type=SpanType.AGENT)
defrun(self, query:str):
# do something

while i < self.max_turns:
            response = litellm.completion(
                model="gpt-4o-mini",
                messages=messages,
)

            action = self.get_action(response)
...

@mlflow.trace
defget_action(llm_response):
...
```

This approach generates a unified trace, combining your custom Python code with LiteLLM calls.

## LiteLLM Proxy Server[â€‹](#litellm-proxy-server "Direct link to LiteLLM Proxy Server")

### Dependencies[â€‹](#dependencies "Direct link to Dependencies")

For using `mlflow` on LiteLLM Proxy Server, you need to install the `mlflow` package on your docker container.

```
pip install "mlflow>=3.1.4"
```

### Configuration[â€‹](#configuration "Direct link to Configuration")

Configure MLflow in your LiteLLM proxy configuration file:

```
model_list:
-model_name: openai/*
litellm_params:
model: openai/*

litellm_settings:
success_callback:["mlflow"]
failure_callback:["mlflow"]
```

### Environment Variables[â€‹](#environment-variables "Direct link to Environment Variables")

For MLflow with Databricks service, set these required environment variables:

```
DATABRICKS_TOKEN="dapixxxxx"
DATABRICKS_HOST="https://dbc-xxxx.cloud.databricks.com"
MLFLOW_TRACKING_URI="databricks"
MLFLOW_REGISTRY_URI="databricks-uc"
MLFLOW_EXPERIMENT_ID="xxxx"
```

### Adding Tags for Better Tracing[â€‹](#adding-tags-for-better-tracing "Direct link to Adding Tags for Better Tracing")

You can add custom tags to your requests for improved trace organization and filtering in MLflow. Tags help you categorize and search your traces by job ID, task name, or any custom metadata.

- curl
- OpenAI Python SDK

```
curl --location 'http://0.0.0.0:4000/chat/completions' \
    --header 'Content-Type: application/json' \
    --header 'Authorization: Bearer sk-1234' \
    --data '{
    "model": "gemini-2.5-flash",
    "messages": [
        {
        "role": "user",
        "content": "what llm are you"
        }
    ],
    "litellm_metadata": {
        "tags": ["jobID:214590dsff09fds", "taskName:run_page_classification"]
    }
}'
```

## Support[â€‹](#support "Direct link to Support")

- For advanced usage and integrations of tracing, visit the [MLflow Tracing documentation](https://mlflow.org/docs/latest/llms/tracing/index.html).
- For any question or issue with this integration, please [submit an issue](https://github.com/mlflow/mlflow/issues/new/choose) on our [Github](https://github.com/mlflow/mlflow) repository!