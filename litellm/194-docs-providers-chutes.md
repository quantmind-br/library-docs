---
title: Chutes | liteLLM
url: https://docs.litellm.ai/docs/providers/chutes
source: sitemap
fetched_at: 2026-01-21T19:48:39.505696436-03:00
rendered_js: false
word_count: 380
summary: This document provides instructions for integrating the Chutes AI deployment platform with LiteLLM, covering API configuration, supported frameworks like vLLM and SGLang, and hardware optimization for scaling LLM applications.
tags:
    - chutes
    - litellm
    - ai-deployment
    - vllm
    - sglang
    - openai-compatible
    - llm-scaling
category: guide
---

## Overview[​](#overview "Direct link to Overview")

PropertyDetailsDescriptionChutes is a cloud-native AI deployment platform that allows you to deploy, run, and scale LLM applications with OpenAI-compatible APIs using pre-built templates for popular frameworks like vLLM and SGLang.Provider Route on LiteLLM`chutes/`Link to Provider Doc[Chutes Website ↗](https://chutes.ai)Base URL`https://llm.chutes.ai/v1/`Supported Operations[`/chat/completions`](#sample-usage), Embeddings

## What is Chutes?[​](#what-is-chutes "Direct link to What is Chutes?")

Chutes is a powerful AI deployment and serving platform that provides:

- **Pre-built Templates**: Ready-to-use configurations for vLLM, SGLang, diffusion models, and embeddings
- **OpenAI-Compatible APIs**: Use standard OpenAI SDKs and clients
- **Multi-GPU Scaling**: Support for large models across multiple GPUs
- **Streaming Responses**: Real-time model outputs
- **Custom Configurations**: Override any parameter for your specific needs
- **Performance Optimization**: Pre-configured optimization settings

## Required Variables[​](#required-variables "Direct link to Required Variables")

Environment Variables

```
os.environ["CHUTES_API_KEY"]=""# your Chutes API key
```

Get your Chutes API key from [chutes.ai](https://chutes.ai).

## Usage - LiteLLM Python SDK[​](#usage---litellm-python-sdk "Direct link to Usage - LiteLLM Python SDK")

### Non-streaming[​](#non-streaming "Direct link to Non-streaming")

Chutes Non-streaming Completion

```
import os
import litellm
from litellm import completion

os.environ["CHUTES_API_KEY"]=""# your Chutes API key

messages =[{"content":"What is the capital of France?","role":"user"}]

# Chutes call
response = completion(
    model="chutes/model-name",# Replace with actual model name
    messages=messages
)

print(response)
```

### Streaming[​](#streaming "Direct link to Streaming")

Chutes Streaming Completion

```
import os
import litellm
from litellm import completion

os.environ["CHUTES_API_KEY"]=""# your Chutes API key

messages =[{"content":"Write a short poem about AI","role":"user"}]

# Chutes call with streaming
response = completion(
    model="chutes/model-name",# Replace with actual model name
    messages=messages,
    stream=True
)

for chunk in response:
print(chunk)
```

## Usage - LiteLLM Proxy Server[​](#usage---litellm-proxy-server "Direct link to Usage - LiteLLM Proxy Server")

### 1. Save key in your environment[​](#1-save-key-in-your-environment "Direct link to 1. Save key in your environment")

### 2. Start the proxy[​](#2-start-the-proxy "Direct link to 2. Start the proxy")

```
model_list:
-model_name: chutes-model
litellm_params:
model: chutes/model-name  # Replace with actual model name
api_key: os.environ/CHUTES_API_KEY
```

## Supported OpenAI Parameters[​](#supported-openai-parameters "Direct link to Supported OpenAI Parameters")

Chutes supports all standard OpenAI-compatible parameters:

ParameterTypeDescription`messages`array**Required**. Array of message objects with 'role' and 'content'`model`string**Required**. Model ID or HuggingFace model identifier`stream`booleanOptional. Enable streaming responses`temperature`floatOptional. Sampling temperature`top_p`floatOptional. Nucleus sampling parameter`max_tokens`integerOptional. Maximum tokens to generate`frequency_penalty`floatOptional. Penalize frequent tokens`presence_penalty`floatOptional. Penalize tokens based on presence`stop`string/arrayOptional. Stop sequences`tools`arrayOptional. List of available tools/functions`tool_choice`string/objectOptional. Control tool/function calling`response_format`objectOptional. Response format specification

## Support Frameworks[​](#support-frameworks "Direct link to Support Frameworks")

Chutes provides optimized templates for popular AI frameworks:

### vLLM (High-Performance LLM Serving)[​](#vllm-high-performance-llm-serving "Direct link to vLLM (High-Performance LLM Serving)")

- OpenAI-compatible endpoints
- Multi-GPU scaling support
- Advanced optimization settings
- Best for production workloads

### SGLang (Advanced LLM Serving)[​](#sglang-advanced-llm-serving "Direct link to SGLang (Advanced LLM Serving)")

- Structured generation capabilities
- Advanced features and controls
- Custom configuration options
- Best for complex use cases

### Diffusion Models (Image Generation)[​](#diffusion-models-image-generation "Direct link to Diffusion Models (Image Generation)")

- Pre-configured image generation templates
- Optimized settings for best results
- Support for popular diffusion models

### Embedding Models[​](#embedding-models "Direct link to Embedding Models")

- Text embedding templates
- Vector search optimization
- Support for popular embedding models

## Authentication[​](#authentication "Direct link to Authentication")

Chutes supports multiple authentication methods:

- API Key via `X-API-Key` header
- Bearer token via `Authorization` header

Example for LiteLLM (uses environment variable):

```
os.environ["CHUTES_API_KEY"]="your-api-key"
```

## Performance Optimization[​](#performance-optimization "Direct link to Performance Optimization")

Chutes offers hardware selection and optimization:

- **Small Models (7B-13B)**: 1 GPU with 24GB VRAM
- **Medium Models (30B-70B)**: 4 GPUs with 80GB VRAM each
- **Large Models (100B+)**: 8 GPUs with 140GB+ VRAM each

Engine optimization parameters available for fine-tuning performance.

## Deployment Options[​](#deployment-options "Direct link to Deployment Options")

Chutes provides flexible deployment:

- **Quick Setup**: Use pre-built templates for instant deployment
- **Custom Images**: Deploy with custom Docker images
- **Scaling**: Configure max instances and auto-scaling thresholds
- **Hardware**: Choose specific GPU types and configurations

## Additional Resources[​](#additional-resources "Direct link to Additional Resources")

- [Chutes Documentation](https://chutes.ai/docs)
- [Chutes Getting Started](https://chutes.ai/docs/getting-started/running-a-chute)
- [Chutes API Reference](https://chutes.ai/docs/sdk-reference)