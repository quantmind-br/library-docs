{
  "generated_at": "2026-01-21T19:56:06.167564456-03:00",
  "source_url": "https://docs.litellm.ai/sitemap.xml",
  "strategy": "sitemap",
  "total_documents": 732,
  "documents": [
    {
      "file_path": "001-docs-a2a.md",
      "title": "Agent Gateway (A2A Protocol) - Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/a2a",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:49.165735939-03:00",
      "description": "Add A2A Agents on LiteLLM AI Gateway, Invoke agents in A2A Protocol, track request/response logs in LiteLLM Logs. Manage which Teams, Keys can access which Agents onboarded.",
      "summary": "This document explains how to integrate, manage, and invoke A2A-compatible agents from various providers within the LiteLLM AI Gateway. It details the setup process for different agent platforms, provides SDK usage examples, and outlines how to track logs and metrics.",
      "tags": [
        "litellm",
        "a2a-protocol",
        "ai-gateway",
        "agent-management",
        "vertex-ai",
        "azure-ai-foundry",
        "langgraph",
        "api-logging"
      ],
      "category": "guide",
      "original_file_path": "docs-a2a.md"
    },
    {
      "file_path": "002-docs-mcp.md",
      "title": "MCP Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:40.500663191-03:00",
      "description": "LiteLLM Proxy provides an MCP Gateway that allows you to use a fixed endpoint for all MCP tools and control MCP access by Key, Team.",
      "summary": "This document explains how to configure and use LiteLLM Proxy as an MCP Gateway to manage tools and servers via various transports and OpenAPI specifications.",
      "tags": [
        "litellm-proxy",
        "mcp-gateway",
        "model-context-protocol",
        "openapi-integration",
        "oauth-configuration",
        "server-management"
      ],
      "category": "guide",
      "original_file_path": "docs-mcp.md"
    },
    {
      "file_path": "003-docs-pass-through-intro.md",
      "title": "Why Pass-Through Endpoints? | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/intro",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:53.212379997-03:00",
      "description": "These endpoints are useful for 2 scenarios:",
      "summary": "This document explains how LiteLLM's passthrough endpoints function to forward provider-specific requests while providing unified authentication, cost tracking, and centralized logging.",
      "tags": [
        "litellm-proxy",
        "passthrough-endpoints",
        "api-forwarding",
        "cost-tracking",
        "unified-authentication",
        "request-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-intro.md"
    },
    {
      "file_path": "004-docs-proxy-configs.md",
      "title": "Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/configs",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:27.454642722-03:00",
      "description": "Set model list, apibase, apikey, temperature & proxy server settings (master-key) on the config.yaml.",
      "summary": "This document provides instructions for setting up and managing the LiteLLM Proxy using a YAML configuration file to define models, settings, and environment variables.",
      "tags": [
        "litellm",
        "config-yaml",
        "proxy-server",
        "llm-configuration",
        "load-balancing",
        "model-aliasing"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-configs.md"
    },
    {
      "file_path": "005-docs-proxy-user-keys.md",
      "title": "Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/user_keys",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:00.236871294-03:00",
      "description": "LiteLLM Proxy is OpenAI-Compatible, and supports:",
      "summary": "This document demonstrates how to use the OpenAI Python client to send a single chat completion request to multiple models simultaneously through a proxy server.",
      "tags": [
        "openai-sdk",
        "chat-completions",
        "multi-model",
        "python-client",
        "proxy-server",
        "llm-routing"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-user-keys.md"
    },
    {
      "file_path": "006-docs-search.md",
      "title": "Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:24.98674559-03:00",
      "description": "| Feature | Supported |",
      "summary": "This document explains how to use LiteLLM to perform search operations using its Python SDK and AI Gateway across multiple providers like Perplexity, Tavily, and Exa. It details configuration, request parameters, response formats, and implementation of search load balancing.",
      "tags": [
        "litellm",
        "search-api",
        "python-sdk",
        "api-gateway",
        "perplexity-ai",
        "load-balancing",
        "web-search"
      ],
      "category": "guide",
      "original_file_path": "docs-search.md"
    },
    {
      "file_path": "007-docs-secret-managers-overview.md",
      "title": "Secret Managers Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/overview",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:48.309527579-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document explains how to configure LiteLLM to integrate with external key management systems for reading and writing API secrets and virtual keys.",
      "tags": [
        "secret-management",
        "litellm-configuration",
        "kms-integration",
        "aws-secret-manager",
        "security",
        "virtual-keys"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-overview.md"
    },
    {
      "file_path": "008-docs-secret.md",
      "title": "Secret Managers Overview | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:36.232058948-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document explains how to configure LiteLLM to read and write API keys and virtual keys using various secret management systems like AWS Secret Manager and Azure Key Vault.",
      "tags": [
        "secret-management",
        "litellm",
        "aws-secret-manager",
        "key-vault",
        "security-configuration",
        "api-keys"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret.md"
    },
    {
      "file_path": "009-index.md",
      "title": "LiteLLM - Getting Started | liteLLM",
      "url": "https://docs.litellm.ai/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:56:00.824340265-03:00",
      "description": "https://github.com/BerriAI/litellm",
      "summary": "LiteLLM provides a unified interface and proxy server to call over 100 different LLMs using the OpenAI input/output format, featuring load balancing and cost tracking.",
      "tags": [
        "litellm",
        "llm-gateway",
        "openai-compatibility",
        "python-sdk",
        "observability",
        "cost-tracking",
        "multi-model-orchestration"
      ],
      "category": "guide",
      "original_file_path": "index.md"
    },
    {
      "file_path": "010-release-notes-v1-80-0.md",
      "title": "v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents",
      "url": "https://docs.litellm.ai/release_notes/v1-80-0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:57.117609175-03:00",
      "description": "Deploy this version",
      "summary": "This document details the v1.80.0 release of LiteLLM, highlighting new features like Agent Hub, RunwayML integration, and the OpenAI-compatible Vector Store Files API. It also includes performance benchmarks for embeddings and a list of newly supported AI models.",
      "tags": [
        "litellm",
        "release-notes",
        "vector-store-api",
        "runwayml",
        "api-gateway",
        "performance-optimization",
        "model-support"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-0.md"
    },
    {
      "file_path": "011-release-notes-v1-80-8.md",
      "title": "v1.80.8-stable - Introducing A2A Agent Gateway",
      "url": "https://docs.litellm.ai/release_notes/v1-80-8",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:05.945093281-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the features and updates in LiteLLM version 1.80.8-stable, including the new Agent Gateway, Guardrails API v2, and expanded support for various LLM providers and models.",
      "tags": [
        "litellm-release",
        "agent-gateway",
        "llm-proxy",
        "api-updates",
        "model-pricing",
        "ai-infrastructure"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-8.md"
    },
    {
      "file_path": "012-docs-tutorials-claude-responses-api.md",
      "title": "Claude Code Quickstart | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_responses_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:10.023499558-03:00",
      "description": "This tutorial shows how to call Claude models through LiteLLM proxy from Claude Code.",
      "summary": "This guide explains how to integrate Claude Code with LiteLLM proxy to access and manage Claude models across multiple providers including Anthropic, AWS Bedrock, and Google Vertex AI.",
      "tags": [
        "litellm",
        "claude-code",
        "proxy-server",
        "model-routing",
        "anthropic-claude",
        "aws-bedrock",
        "vertex-ai"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-claude-responses-api.md"
    },
    {
      "file_path": "013-docs-default-code-snippet.md",
      "title": "Get Started | liteLLM",
      "url": "https://docs.litellm.ai/docs/default_code_snippet",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:05.072037806-03:00",
      "description": "This section assumes you've already added your API keys in",
      "summary": "This document demonstrates how to use the litellm library to make standardized completion calls across different LLM providers like OpenAI and Cohere using a unified Python interface.",
      "tags": [
        "litellm",
        "python-sdk",
        "llm-integration",
        "openai",
        "cohere",
        "completion-api"
      ],
      "category": "tutorial",
      "original_file_path": "docs-default-code-snippet.md"
    },
    {
      "file_path": "014-docs-proxy-docker-quick-start.md",
      "title": "Getting Started Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/docker_quick_start",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:46.32377303-03:00",
      "description": "End-to-End tutorial for LiteLLM Proxy to:",
      "summary": "This tutorial provides a step-by-step guide to setting up and configuring the LiteLLM Proxy for Azure OpenAI, including model integration, API testing, and virtual key management with database integration.",
      "tags": [
        "litellm-proxy",
        "azure-openai",
        "docker-deployment",
        "api-gateway",
        "access-control",
        "virtual-keys"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-docker-quick-start.md"
    },
    {
      "file_path": "015-docs-proxy-guardrails-quick-start.md",
      "title": "Guardrails - Quick Start | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/quick_start",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:34.719726849-03:00",
      "description": "Setup Prompt Injection Detection, PII Masking on LiteLLM Proxy (AI Gateway)",
      "summary": "This document provides instructions for configuring and implementing AI guardrails, such as PII masking and prompt injection detection, within the LiteLLM Proxy environment.",
      "tags": [
        "litellm",
        "ai-gateway",
        "guardrails",
        "pii-masking",
        "prompt-injection",
        "security",
        "data-privacy"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-quick-start.md"
    },
    {
      "file_path": "016-docs-proxy-quick-start.md",
      "title": "CLI - Quick Start | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/quick_start",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:24.441068088-03:00",
      "description": "Setup LiteLLM Proxy quickly via CLI.",
      "summary": "This document provides a comprehensive guide for setting up and using the LiteLLM Proxy to manage multiple LLM providers through a unified OpenAI-compatible interface. It explains installation via CLI, configuration using YAML files, load balancing, and debugging procedures.",
      "tags": [
        "litellm-proxy",
        "llm-gateway",
        "cli-setup",
        "model-routing",
        "load-balancing",
        "openai-interface",
        "api-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-quick-start.md"
    },
    {
      "file_path": "017-docs-proxy-ui-logs.md",
      "title": "Getting Started with UI Logs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ui_logs",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:55.286525524-03:00",
      "description": "View Spend, Token Usage, Key, Team Name for Each Request to LiteLLM",
      "summary": "This document explains how to configure and manage logging in LiteLLM, including tracking spend and token usage while providing instructions for data retention and privacy settings.",
      "tags": [
        "litellm",
        "logging",
        "spend-tracking",
        "token-usage",
        "data-retention",
        "proxy-configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-ui-logs.md"
    },
    {
      "file_path": "018-docs-proxy-ui.md",
      "title": "Quick Start | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:53.456643358-03:00",
      "description": "Create keys, track spend, add models without worrying about the config / CRUD endpoints.",
      "summary": "This document explains how to set up, access, and configure the LiteLLM Admin UI for managing models, API keys, and proxy server settings.",
      "tags": [
        "litellm",
        "admin-ui",
        "proxy-server",
        "model-management",
        "api-key-management",
        "user-access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-ui.md"
    },
    {
      "file_path": "019-docs.md",
      "title": "LiteLLM - Getting Started | liteLLM",
      "url": "https://docs.litellm.ai/docs/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:48.507269062-03:00",
      "description": "https://github.com/BerriAI/litellm",
      "summary": "LiteLLM provides a unified interface to call over 100 LLMs using the OpenAI input/output format via a Python SDK or a proxy server. It simplifies cross-provider integration by offering features like consistent response formats, retry logic, cost tracking, and observability.",
      "tags": [
        "llm-integration",
        "openai-format",
        "python-sdk",
        "api-gateway",
        "cost-tracking",
        "observability",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs.md"
    },
    {
      "file_path": "020-docs-proxy-db-deadlocks.md",
      "title": "High Availability Setup (Resolve DB Deadlocks) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/db_deadlocks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:41.810134738-03:00",
      "description": "This configuration is required for production deployments handling 1000+ requests per second. Without Redis configured, you may experience PostgreSQL connection exhaustion (FATAL: sorry, too many clients already).",
      "summary": "This document explains how to configure a Redis transaction buffer in LiteLLM to prevent PostgreSQL database deadlocks and connection exhaustion during high-traffic production deployments.",
      "tags": [
        "litellm",
        "redis",
        "postgresql",
        "high-availability",
        "database-deadlocks",
        "performance-tuning",
        "transaction-buffer"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-db-deadlocks.md"
    },
    {
      "file_path": "021-docs-tutorials-installation.md",
      "title": "Set up environment | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/installation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:29.782167386-03:00",
      "description": "Let's get the necessary keys to set up our demo environment.",
      "summary": "This document provides instructions and direct links for obtaining API keys from various LLM providers to set up a demo environment.",
      "tags": [
        "api-keys",
        "llm-providers",
        "environment-setup",
        "authentication",
        "openai",
        "cohere",
        "ai21"
      ],
      "category": "configuration",
      "original_file_path": "docs-tutorials-installation.md"
    },
    {
      "file_path": "022-blog-anthropic-advanced-features.md",
      "title": "Day 0 Support: Claude 4.5 Opus (+Advanced Features)",
      "url": "https://docs.litellm.ai/blog/anthropic_advanced_features",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:44.691596569-03:00",
      "description": "This guide covers Anthropic's latest model (Claude Opus 4.5) and its advanced features now available in LiteLLM: Tool Search, Programmatic Tool Calling, Tool Input Examples, and the Effort Parameter.",
      "summary": "This document explains how to implement advanced Anthropic Claude 4.5 features, such as Tool Search and Programmatic Tool Calling, using the LiteLLM library across multiple cloud providers.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "claude-4-5",
        "python-sdk",
        "cloud-integration"
      ],
      "category": "guide",
      "original_file_path": "blog-anthropic-advanced-features.md"
    },
    {
      "file_path": "023-blog-gemini-3-flash.md",
      "title": "DAY 0 Support: Gemini 3 Flash on LiteLLM",
      "url": "https://docs.litellm.ai/blog/gemini_3_flash",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:50.899730081-03:00",
      "description": "LiteLLM now supports gemini-3-flash-preview and all the new API changes along with it.",
      "summary": "This document details the integration of Google Gemini 3 Flash Preview into LiteLLM, explaining how to implement thinking levels, thought signatures, and OpenAI-compatible reasoning parameters.",
      "tags": [
        "litellm",
        "gemini-3-flash",
        "google-gemini",
        "reasoning-effort",
        "thinking-level",
        "llm-proxy",
        "model-integration"
      ],
      "category": "guide",
      "original_file_path": "blog-gemini-3-flash.md"
    },
    {
      "file_path": "024-blog-gemini-3.md",
      "title": "DAY 0 Support: Gemini 3 on LiteLLM",
      "url": "https://docs.litellm.ai/blog/gemini_3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:44.687252888-03:00",
      "description": "This guide covers common questions and best practices for using gemini-3-pro-preview with LiteLLM Proxy and SDK.",
      "summary": "This document explains how to use Gemini 3 Pro Preview with LiteLLM, focusing on thought signatures, multi-turn function calling, and maintaining conversation history across different model versions.",
      "tags": [
        "litellm",
        "gemini-3-pro",
        "thought-signatures",
        "function-calling",
        "llm-proxy",
        "google-gemini"
      ],
      "category": "guide",
      "original_file_path": "blog-gemini-3.md"
    },
    {
      "file_path": "025-blog-tags-advanced-features.md",
      "title": "One post tagged with \"advanced features\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/advanced-features",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:51.731059675-03:00",
      "summary": "This document demonstrates how to implement Anthropic's tool search and deferred tool loading functionality using LiteLLM to manage large sets of tools efficiently.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "deferred-loading",
        "python",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "blog-tags-advanced-features.md"
    },
    {
      "file_path": "026-blog-tags-anthropic.md",
      "title": "One post tagged with \"anthropic\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/anthropic",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:52.888423225-03:00",
      "summary": "This document demonstrates how to implement deferred tool loading and tool searching using LiteLLM with Anthropic Claude models.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "function-calling",
        "tool-search",
        "deferred-loading",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "blog-tags-anthropic.md"
    },
    {
      "file_path": "027-blog-tags-claude.md",
      "title": "One post tagged with \"claude\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/claude",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:53.211430064-03:00",
      "summary": "This document demonstrates how to implement deferred tool loading and tool search functionality using LiteLLM and Anthropic models to efficiently handle large sets of tools.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "deferred-loading",
        "tool-search",
        "python"
      ],
      "category": "guide",
      "original_file_path": "blog-tags-claude.md"
    },
    {
      "file_path": "028-blog-tags-day-0-support.md",
      "title": "2 posts tagged with \"day 0 support\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/day-0-support",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:56.567601684-03:00",
      "summary": "This document provides a guide for using Gemini 3 Pro Preview with LiteLLM, focusing on endpoint support, thought signature management, and multi-turn function calling.",
      "tags": [
        "litellm",
        "google-gemini",
        "gemini-3",
        "thought-signatures",
        "function-calling",
        "llm-proxy",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "blog-tags-day-0-support.md"
    },
    {
      "file_path": "029-blog-tags-effort.md",
      "title": "One post tagged with \"effort\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/effort",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:56.514942048-03:00",
      "summary": "This document demonstrates how to implement deferred tool loading and tool search functionality using LiteLLM and Anthropic's Claude models to handle large sets of functions efficiently.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "deferred-loading",
        "python",
        "function-calling"
      ],
      "category": "tutorial",
      "original_file_path": "blog-tags-effort.md"
    },
    {
      "file_path": "030-blog-tags-gemini.md",
      "title": "2 posts tagged with \"gemini\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/gemini",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:58.730233561-03:00",
      "summary": "This guide provides instructions and best practices for using the Gemini 3 Pro Preview model with LiteLLM, specifically covering endpoint compatibility, thought signature preservation, and multi-turn function calling.",
      "tags": [
        "gemini-3",
        "litellm",
        "thought-signatures",
        "function-calling",
        "python-sdk",
        "streaming",
        "multi-turn-conversations"
      ],
      "category": "guide",
      "original_file_path": "blog-tags-gemini.md"
    },
    {
      "file_path": "031-blog-tags-llms.md",
      "title": "2 posts tagged with \"llms\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/llms",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:59.73610858-03:00",
      "summary": "This guide explains how to integrate and use the Gemini 3 Pro Preview model with LiteLLM, focusing on handling thought signatures, multi-turn function calling, and cross-model conversation compatibility.",
      "tags": [
        "gemini-3",
        "litellm",
        "thought-signatures",
        "function-calling",
        "streaming",
        "api-integration",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "blog-tags-llms.md"
    },
    {
      "file_path": "032-blog-tags-programmatic-tool-calling.md",
      "title": "One post tagged with \"programmatic tool calling\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/programmatic-tool-calling",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:59.789928193-03:00",
      "summary": "This document demonstrates how to implement tool search and deferred tool loading using LiteLLM with Claude models to handle large toolsets efficiently.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "deferred-loading",
        "tool-search",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "blog-tags-programmatic-tool-calling.md"
    },
    {
      "file_path": "033-blog-tags-tool-search.md",
      "title": "One post tagged with \"tool search\" | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags/tool-search",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:02.699945527-03:00",
      "summary": "This document demonstrates how to implement tool searching and deferred tool loading using LiteLLM with Anthropic's Claude models. It shows how to define functions that are loaded on-demand during the model's completion process.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "tool-calling",
        "deferred-loading",
        "python-sdk",
        "function-calling"
      ],
      "category": "guide",
      "original_file_path": "blog-tags-tool-search.md"
    },
    {
      "file_path": "034-blog.md",
      "title": "Blog | liteLLM",
      "url": "https://docs.litellm.ai/blog",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:44.710623463-03:00",
      "description": "Blog",
      "summary": "This document provides instructions and best practices for using Gemini 3 Pro Preview with LiteLLM, specifically addressing thought signatures, multi-turn function calling, and model switching compatibility.",
      "tags": [
        "litellm",
        "gemini-3",
        "google-gemini",
        "function-calling",
        "thought-signatures",
        "python-sdk",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "blog.md"
    },
    {
      "file_path": "035-contributing.md",
      "title": "Contributing to Documentation | liteLLM",
      "url": "https://docs.litellm.ai/contributing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:07.784943782-03:00",
      "description": "Clone litellm",
      "summary": "This document provides instructions for setting up, running, and contributing to the LiteLLM documentation using MkDocs in a local environment.",
      "tags": [
        "litellm",
        "documentation",
        "mkdocs",
        "local-setup",
        "contribution-guide"
      ],
      "category": "tutorial",
      "original_file_path": "contributing.md"
    },
    {
      "file_path": "036-docs-a2a-agent-permissions.md",
      "title": "Agent Permission Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/a2a_agent_permissions",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:49.200196267-03:00",
      "description": "Control which A2A agents can be accessed by specific keys or teams in LiteLLM.",
      "summary": "This document explains how to manage access control for A2A agents in LiteLLM by restricting agent permissions for specific virtual keys and teams.",
      "tags": [
        "litellm",
        "agent-permissions",
        "access-control",
        "security",
        "multi-tenancy",
        "api-keys",
        "team-management"
      ],
      "category": "guide",
      "original_file_path": "docs-a2a-agent-permissions.md"
    },
    {
      "file_path": "037-docs-a2a-cost-tracking.md",
      "title": "A2A Agent Cost Tracking | liteLLM",
      "url": "https://docs.litellm.ai/docs/a2a_cost_tracking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:51.480860264-03:00",
      "description": "LiteLLM supports adding custom cost tracking for A2A agents. You can configure:",
      "summary": "This guide explains how to configure and verify custom cost tracking for A2A agents in LiteLLM using flat fees or token-based pricing.",
      "tags": [
        "litellm",
        "cost-tracking",
        "a2a-agents",
        "agent-management",
        "usage-monitoring"
      ],
      "category": "tutorial",
      "original_file_path": "docs-a2a-cost-tracking.md"
    },
    {
      "file_path": "038-docs-adding-provider-adding-guardrail-support.md",
      "title": "Adding Guardrail Support to Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/adding_provider/adding_guardrail_support",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:52.153877522-03:00",
      "description": "This guide explains how to add guardrail translation support to new LiteLLM endpoints (e.g., Chat Completions, Responses API, etc.).",
      "summary": "This document explains how to implement guardrail translation support for LiteLLM endpoints by creating custom handlers for input and output processing. It provides a step-by-step implementation guide for inheriting from BaseTranslation and registering the handler within the LiteLLM framework.",
      "tags": [
        "litellm",
        "guardrails",
        "api-integration",
        "content-moderation",
        "middleware",
        "python-development"
      ],
      "category": "guide",
      "original_file_path": "docs-adding-provider-adding-guardrail-support.md"
    },
    {
      "file_path": "039-docs-adding-provider-directory-structure.md",
      "title": "Directory Structure | liteLLM",
      "url": "https://docs.litellm.ai/docs/adding_provider/directory_structure",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:55.165318551-03:00",
      "description": "When adding a new provider, you need to create a directory for the provider that follows the following structure:",
      "summary": "This document outlines the required directory structure and file organization for integrating a new model provider into the LiteLLM library.",
      "tags": [
        "litellm",
        "provider-integration",
        "directory-structure",
        "llm-backend",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-adding-provider-directory-structure.md"
    },
    {
      "file_path": "040-docs-adding-provider-new-rerank-provider.md",
      "title": "Add Rerank Provider | liteLLM",
      "url": "https://docs.litellm.ai/docs/adding_provider/new_rerank_provider",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:56.473085725-03:00",
      "description": "LiteLLM follows the Cohere Rerank API format for all rerank providers. Here's how to add a new rerank provider:",
      "summary": "This document provides instructions on how to integrate new rerank providers into LiteLLM by implementing a custom configuration class and handling HTTP requests based on the Cohere Rerank API format.",
      "tags": [
        "litellm",
        "rerank-api",
        "provider-integration",
        "python-development",
        "custom-providers"
      ],
      "category": "guide",
      "original_file_path": "docs-adding-provider-new-rerank-provider.md"
    },
    {
      "file_path": "041-docs-adding-provider-simple-guardrail-tutorial.md",
      "title": "Adding a New Guardrail Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/adding_provider/simple_guardrail_tutorial",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:56.966630797-03:00",
      "description": "You're going to create a class that checks text before it goes to the LLM or after it comes back. If it violates your rules, you block it.",
      "summary": "This document provides step-by-step instructions for building and registering a custom guardrail class to validate LLM inputs and outputs within LiteLLM.",
      "tags": [
        "litellm",
        "guardrails",
        "custom-hooks",
        "llm-security",
        "middleware-development"
      ],
      "category": "tutorial",
      "original_file_path": "docs-adding-provider-simple-guardrail-tutorial.md"
    },
    {
      "file_path": "042-docs-ai-tools.md",
      "title": "AI Tools | liteLLM",
      "url": "https://docs.litellm.ai/docs/ai_tools",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:58.867747899-03:00",
      "description": "Integrate LiteLLM with AI tools like OpenWebUI, Claude Code, and more",
      "summary": "This document explains how to monitor usage metrics and associated costs for AI-powered coding assistants and CLI tools using the LiteLLM platform.",
      "tags": [
        "cost-tracking",
        "usage-monitoring",
        "litellm",
        "coding-assistants",
        "api-proxy"
      ],
      "category": "tutorial",
      "original_file_path": "docs-ai-tools.md"
    },
    {
      "file_path": "043-docs-audio-transcription.md",
      "title": "/audio/transcriptions | liteLLM",
      "url": "https://docs.litellm.ai/docs/audio_transcription",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:06.509526448-03:00",
      "description": "Overview",
      "summary": "This guide explains how to implement audio transcription using LiteLLM, covering supported providers, proxy configuration, and advanced features like model fallbacks.",
      "tags": [
        "litellm",
        "audio-transcription",
        "speech-to-text",
        "api-proxy",
        "model-fallbacks",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-audio-transcription.md"
    },
    {
      "file_path": "044-docs-batches.md",
      "title": "/batches | liteLLM",
      "url": "https://docs.litellm.ai/docs/batches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:06.735384508-03:00",
      "description": "Covers Batches, Files",
      "summary": "This document explains how to manage batch completions and file uploads using LiteLLM, including support for multi-account model-based routing and automated credential handling.",
      "tags": [
        "batch-processing",
        "file-management",
        "model-routing",
        "multi-account",
        "litellm-proxy",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-batches.md"
    },
    {
      "file_path": "045-docs-bedrock-converse.md",
      "title": "/converse | liteLLM",
      "url": "https://docs.litellm.ai/docs/bedrock_converse",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:09.450223429-03:00",
      "description": "Call Bedrock's /converse endpoint through LiteLLM Proxy.",
      "summary": "This document explains how to configure and use the LiteLLM Proxy to access Amazon Bedrock's converse and converse-stream endpoints, including instructions for load balancing and SDK integration.",
      "tags": [
        "litellm",
        "amazon-bedrock",
        "api-proxy",
        "load-balancing",
        "streaming-api",
        "boto3",
        "aws-credentials"
      ],
      "category": "guide",
      "original_file_path": "docs-bedrock-converse.md"
    },
    {
      "file_path": "046-docs-bedrock-invoke.md",
      "title": "/invoke | liteLLM",
      "url": "https://docs.litellm.ai/docs/bedrock_invoke",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:09.936674086-03:00",
      "description": "Call Bedrock's /invoke endpoint through LiteLLM Proxy.",
      "summary": "This document provides a guide for setting up and using the LiteLLM Proxy to interact with Amazon Bedrock endpoints, covering configuration, streaming, and regional load balancing. It includes implementation examples using both raw HTTP requests and the boto3 Python SDK.",
      "tags": [
        "litellm-proxy",
        "amazon-bedrock",
        "load-balancing",
        "streaming-api",
        "aws-boto3",
        "llm-gateway",
        "api-configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-bedrock-invoke.md"
    },
    {
      "file_path": "047-docs-budget-manager.md",
      "title": "Budget Manager | liteLLM",
      "url": "https://docs.litellm.ai/docs/budget_manager",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:11.749112738-03:00",
      "description": "Don't want to get crazy bills because either while you're calling LLM APIs or while your users are calling them? use this.",
      "summary": "This document explains how to implement budget management and cost tracking for LLM API calls using LiteLLM's global variables and the BudgetManager class. It covers setting usage limits, user-based rate limiting, and persisting spend data across different deployment environments.",
      "tags": [
        "litellm",
        "budget-management",
        "cost-tracking",
        "rate-limiting",
        "llm-usage",
        "api-billing"
      ],
      "category": "guide",
      "original_file_path": "docs-budget-manager.md"
    },
    {
      "file_path": "048-docs-caching-all-caches.md",
      "title": "Caching - In-Memory, Redis, s3, gcs, Redis Semantic Cache, Disk | liteLLM",
      "url": "https://docs.litellm.ai/docs/caching/all_caches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:13.143725819-03:00",
      "description": "See Code",
      "summary": "This document provides a comprehensive guide on implementing caching in LiteLLM using various backends like Redis, S3, and GCS. It explains how to initialize, configure, and customize cache behavior through environment variables, per-call controls, and custom logic.",
      "tags": [
        "litellm",
        "caching",
        "redis-cache",
        "semantic-cache",
        "performance-optimization",
        "python-sdk",
        "cloud-storage"
      ],
      "category": "guide",
      "original_file_path": "docs-caching-all-caches.md"
    },
    {
      "file_path": "049-docs-caching-caching-api.md",
      "title": "Hosted Cache - api.litellm.ai | liteLLM",
      "url": "https://docs.litellm.ai/docs/caching/caching_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:13.761861212-03:00",
      "description": "Use api.litellm.ai for caching completion() and embedding() responses",
      "summary": "This document demonstrates how to initialize and use LiteLLM's hosted caching for completions, embeddings, and streaming responses to optimize performance.",
      "tags": [
        "litellm",
        "caching",
        "hosted-cache",
        "llm-optimization",
        "embeddings",
        "streaming-responses"
      ],
      "category": "guide",
      "original_file_path": "docs-caching-caching-api.md"
    },
    {
      "file_path": "050-docs-caching-local-caching.md",
      "title": "LiteLLM - Local Caching | liteLLM",
      "url": "https://docs.litellm.ai/docs/caching/local_caching",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:16.878898057-03:00",
      "description": "Caching completion() and embedding() calls when switched on",
      "summary": "This document explains how to implement and configure caching for completion and embedding calls in LiteLLM using in-memory or Redis backends.",
      "tags": [
        "litellm",
        "caching",
        "redis",
        "completion-api",
        "embeddings",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-caching-local-caching.md"
    },
    {
      "file_path": "051-docs-completion-audio.md",
      "title": "Using Audio Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/audio",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:19.32475599-03:00",
      "description": "How to send / receive audio to a /chat/completions endpoint",
      "summary": "This document provides a code example for using litellm to process audio inputs and generate multimodal responses using models like gpt-4o-audio-preview.",
      "tags": [
        "litellm",
        "audio-processing",
        "python-sdk",
        "multimodal-ai",
        "gpt-4o-audio",
        "base64-encoding"
      ],
      "category": "tutorial",
      "original_file_path": "docs-completion-audio.md"
    },
    {
      "file_path": "052-docs-completion-batching.md",
      "title": "Batching Completion() | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/batching",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:20.164602825-03:00",
      "description": "LiteLLM allows you to:",
      "summary": "This document explains how to use LiteLLM's batch completion features to process multiple prompts simultaneously or query multiple models in parallel for speed and comparison.",
      "tags": [
        "litellm",
        "batch-completion",
        "parallel-processing",
        "multi-model",
        "latency-optimization",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-batching.md"
    },
    {
      "file_path": "053-docs-completion-computer-use.md",
      "title": "Computer Use | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/computer_use",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:21.295804488-03:00",
      "description": "Computer use allows models to interact with computer interfaces by taking screenshots and performing actions like clicking, typing, and scrolling. This enables AI models to autonomously operate desktop environments.",
      "summary": "This document explains how to implement computer use capabilities via LiteLLM to enable AI models to interact with desktop environments through screenshots, bash commands, and text editing.",
      "tags": [
        "litellm",
        "computer-use",
        "tool-calling",
        "automation",
        "anthropic-claude",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-computer-use.md"
    },
    {
      "file_path": "054-docs-completion-document-understanding.md",
      "title": "Using PDF Input | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/document_understanding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:21.976224059-03:00",
      "description": "How to send / receive pdf's (other document types) to a /chat/completions endpoint",
      "summary": "This document explains how to send PDF files and other document types to LLM providers using LiteLLM via URLs, base64 encoding, or file IDs. It includes instructions for checking model support and configuring specific document formats for multi-modal interactions.",
      "tags": [
        "litellm",
        "pdf-input",
        "multimodal-llm",
        "chat-completions",
        "python-sdk",
        "document-analysis"
      ],
      "category": "tutorial",
      "original_file_path": "docs-completion-document-understanding.md"
    },
    {
      "file_path": "055-docs-completion-function-call.md",
      "title": "Function Calling | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/function_call",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:24.462448391-03:00",
      "description": "Checking if a model supports function calling",
      "summary": "This document explains how to check for function calling support in LiteLLM and provides a step-by-step guide for implementing parallel function calls.",
      "tags": [
        "litellm",
        "function-calling",
        "parallel-function-calling",
        "python",
        "llm-integration",
        "tools-api"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-function-call.md"
    },
    {
      "file_path": "056-docs-completion-image-generation-chat.md",
      "title": "Image Generation in Chat Completions, Responses API | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/image_generation_chat",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:27.578924952-03:00",
      "description": "This guide covers how to generate images when using the chat/completions. Note - if you want this on Responses API please file a Feature Request here.",
      "summary": "Explains how to use LiteLLM to generate images through the chat completions endpoint using supported models from Google AI Studio and Vertex AI.",
      "tags": [
        "litellm",
        "image-generation",
        "gemini",
        "vertex-ai",
        "chat-completions",
        "python-sdk",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-image-generation-chat.md"
    },
    {
      "file_path": "057-docs-completion-json-mode.md",
      "title": "Structured Outputs (JSON Mode) | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/json_mode",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:28.980247293-03:00",
      "description": "Quick Start",
      "summary": "This document provides a technical guide on implementing structured outputs and JSON schemas using LiteLLM across various model providers. It details how to check for model compatibility, utilize Pydantic models for response formatting, and enable client-side schema validation.",
      "tags": [
        "litellm",
        "structured-outputs",
        "json-schema",
        "pydantic",
        "model-compatibility",
        "llm-api",
        "response-format"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-json-mode.md"
    },
    {
      "file_path": "058-docs-completion-knowledgebase.md",
      "title": "Using Vector Stores (Knowledge Bases) | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/knowledgebase",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:29.784252439-03:00",
      "description": "<Image",
      "summary": "This document explains how to integrate and manage various vector stores with LiteLLM to provide models with access to organizational data for retrieval-augmented generation.",
      "tags": [
        "litellm",
        "vector-stores",
        "rag-integration",
        "bedrock-knowledge-base",
        "openai-vector-stores",
        "vertex-ai-rag",
        "pg-vector"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-knowledgebase.md"
    },
    {
      "file_path": "059-docs-completion-mock-requests.md",
      "title": "Mock Completion() Responses - Save Testing Costs ðŸ’° | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/mock_requests",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:32.450201348-03:00",
      "description": "For testing purposes, you can use completion() with mock_response to mock calling the completion endpoint.",
      "summary": "This document explains how to use the mock_response parameter in LiteLLM to simulate API responses for testing purposes without calling actual LLM endpoints. It covers standard completion, streaming, and integration with testing frameworks like pytest.",
      "tags": [
        "litellm",
        "mock-response",
        "unit-testing",
        "python-sdk",
        "llm-api",
        "streaming",
        "pytest"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-mock-requests.md"
    },
    {
      "file_path": "060-docs-completion-multiple-deployments.md",
      "title": "Multiple Deployments | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/multiple_deployments",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:35.790454018-03:00",
      "description": "If you have multiple deployments of the same model, you can pass the list of deployments, and LiteLLM will return the first result.",
      "summary": "This document explains how to configure LiteLLM with multiple model deployments to retrieve the first successful response from a list of providers.",
      "tags": [
        "litellm",
        "model-deployments",
        "multi-provider",
        "python-sdk",
        "completion-api"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-multiple-deployments.md"
    },
    {
      "file_path": "061-docs-completion-predict-outputs.md",
      "title": "Predicted Outputs | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/predict_outputs",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:37.638957781-03:00",
      "description": "| Property | Details |",
      "summary": "This document demonstrates how to use the litellm library to refactor C# code using the prediction parameter for efficient model completions.",
      "tags": [
        "litellm",
        "code-refactoring",
        "python",
        "llm-prediction",
        "gpt-4o-mini"
      ],
      "category": "tutorial",
      "original_file_path": "docs-completion-predict-outputs.md"
    },
    {
      "file_path": "062-docs-completion-prompt-caching.md",
      "title": "Prompt Caching | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/prompt_caching",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:40.533560043-03:00",
      "description": "Supported Providers:",
      "summary": "This document explains how LiteLLM supports and implements prompt caching across various providers including OpenAI, Anthropic, and DeepSeek.",
      "tags": [
        "litellm",
        "prompt-caching",
        "openai",
        "anthropic",
        "deepseek",
        "cost-management"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-prompt-caching.md"
    },
    {
      "file_path": "063-docs-completion-prompt-formatting.md",
      "title": "Prompt Formatting | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/prompt_formatting",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:40.998264883-03:00",
      "description": "LiteLLM automatically translates the OpenAI ChatCompletions prompt format, to other models. You can control this by setting a custom prompt template for a model as well.",
      "summary": "This document explains how LiteLLM handles prompt template translation between models and provides instructions for registering custom prompt templates manually.",
      "tags": [
        "litellm",
        "prompt-templating",
        "huggingface",
        "llm-integration",
        "custom-configurations"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-prompt-formatting.md"
    },
    {
      "file_path": "064-docs-completion-provider-specific-params.md",
      "title": "Provider-specific Params | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/provider_specific_params",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:42.069741168-03:00",
      "description": "Providers might offer params not supported by OpenAI (e.g. top_k). LiteLLM treats any non-openai param, as a provider-specific param, and passes it to the provider in the request body, as a kwarg. See Reserved Params",
      "summary": "This document explains how to pass provider-specific parameters through LiteLLM using the SDK or Proxy, allowing for non-OpenAI standard arguments in API requests.",
      "tags": [
        "litellm",
        "provider-parameters",
        "api-configuration",
        "llm-proxy",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-provider-specific-params.md"
    },
    {
      "file_path": "065-docs-completion-reliable-completions.md",
      "title": "Reliability - Retries, Fallbacks | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/reliable_completions",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:44.375862408-03:00",
      "description": "LiteLLM helps prevent failed requests in 2 ways:",
      "summary": "This document explains how to use LiteLLM's reliability features, specifically request retries and model fallbacks, to ensure successful LLM API calls.",
      "tags": [
        "litellm",
        "error-handling",
        "fallbacks",
        "retries",
        "api-reliability",
        "model-switching",
        "rate-limiting"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-reliable-completions.md"
    },
    {
      "file_path": "066-docs-completion-shared-session.md",
      "title": "Shared Session Support | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/shared_session",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:44.566255837-03:00",
      "description": "Overview",
      "summary": "This document explains how to share aiohttp.ClientSession instances in LiteLLM to optimize performance and resource utilization by reusing HTTP connections across multiple API calls.",
      "tags": [
        "litellm",
        "aiohttp",
        "client-session",
        "performance-optimization",
        "async-io",
        "connection-pooling"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-shared-session.md"
    },
    {
      "file_path": "067-docs-completion-vision.md",
      "title": "Using Vision Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/vision",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:48.146464171-03:00",
      "description": "Quick Start",
      "summary": "This document explains how to use vision-capable models with LiteLLM, including instructions for passing images, checking model compatibility, and specifying image mime-types.",
      "tags": [
        "litellm",
        "vision-models",
        "multimodal",
        "python-sdk",
        "image-processing",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-vision.md"
    },
    {
      "file_path": "068-docs-completion-web-fetch.md",
      "title": "Web Fetch | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/web_fetch",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:50.882114404-03:00",
      "description": "The web fetch tool allows LLMs to retrieve full content from specified web pages and PDF documents. This enables AI models to access real-time information from the internet and incorporate web content into their responses.",
      "summary": "This document explains how to use the web fetch tool with LiteLLM to retrieve full content from specific URLs and PDFs for model processing. It provides implementation details for the LiteLLM Python SDK and Proxy, along with configuration parameters and model support lists.",
      "tags": [
        "litellm",
        "anthropic-claude",
        "web-fetch",
        "tool-calling",
        "content-retrieval",
        "data-extraction"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-web-fetch.md"
    },
    {
      "file_path": "069-docs-completion-web-search.md",
      "title": "Web Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/web_search",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:51.694997461-03:00",
      "description": "Use web search with litellm",
      "summary": "This guide explains how to enable and configure web search capabilities across multiple AI providers using LiteLLM's chat and response endpoints. It covers SDK implementation, proxy configuration, and methods for managing search context and model routing for real-time data retrieval.",
      "tags": [
        "litellm",
        "web-search",
        "ai-search",
        "python-sdk",
        "llm-integration",
        "model-routing",
        "search-context"
      ],
      "category": "guide",
      "original_file_path": "docs-completion-web-search.md"
    },
    {
      "file_path": "070-docs-containers.md",
      "title": "/containers | liteLLM",
      "url": "https://docs.litellm.ai/docs/containers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:55.37966941-03:00",
      "description": "Manage OpenAI code interpreter containers (sessions) for executing code in isolated environments.",
      "summary": "This document explains how to manage isolated OpenAI code interpreter sessions using LiteLLM's SDK, Proxy server, and compatible OpenAI client integrations.",
      "tags": [
        "litellm",
        "openai",
        "code-interpreter",
        "container-management",
        "python-sdk",
        "api-proxy",
        "isolated-execution"
      ],
      "category": "guide",
      "original_file_path": "docs-containers.md"
    },
    {
      "file_path": "071-docs-contribute-integration-custom-webhook-api.md",
      "title": "Contribute Custom Webhook API | liteLLM",
      "url": "https://docs.litellm.ai/docs/contribute_integration/custom_webhook_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:56.328466524-03:00",
      "description": "If your API just needs a Webhook event from LiteLLM, here's how to add a 'native' integration for it on LiteLLM:",
      "summary": "This document provides a step-by-step tutorial on adding native webhook integrations to LiteLLM by configuring generic API compatible callbacks and submitting a pull request.",
      "tags": [
        "litellm",
        "webhooks",
        "callbacks",
        "api-integration",
        "custom-logging",
        "open-source-contribution"
      ],
      "category": "tutorial",
      "original_file_path": "docs-contribute-integration-custom-webhook-api.md"
    },
    {
      "file_path": "072-docs-contributing-adding-openai-compatible-providers.md",
      "title": "Adding OpenAI-Compatible Providers | liteLLM",
      "url": "https://docs.litellm.ai/docs/contributing/adding_openai_compatible_providers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:58.876594156-03:00",
      "description": "For simple OpenAI-compatible providers (like Hyperbolic, Nscale, etc.), you can add support by editing a single JSON file.",
      "summary": "This document provides instructions on how to integrate OpenAI-compatible LLM providers into LiteLLM using a JSON configuration file. It outlines required fields, optional parameter mappings, and usage for adding custom model endpoints.",
      "tags": [
        "litellm",
        "openai-compatible",
        "provider-integration",
        "json-configuration",
        "llm-api",
        "custom-providers"
      ],
      "category": "guide",
      "original_file_path": "docs-contributing-adding-openai-compatible-providers.md"
    },
    {
      "file_path": "073-docs-contributing.md",
      "title": "Contributing - UI | liteLLM",
      "url": "https://docs.litellm.ai/docs/contributing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:58.171031618-03:00",
      "description": "Thanks for contributing to the LiteLLM UI! This guide will help you set up your local development environment.",
      "summary": "This document provides instructions for setting up a local development environment to contribute to the LiteLLM UI, covering repository cloning, proxy configuration, and UI development workflows.",
      "tags": [
        "litellm",
        "ui-development",
        "local-setup",
        "contribution-guide",
        "proxy-configuration",
        "frontend-development"
      ],
      "category": "guide",
      "original_file_path": "docs-contributing.md"
    },
    {
      "file_path": "074-docs-debugging-local-debugging.md",
      "title": "Local Debugging | liteLLM",
      "url": "https://docs.litellm.ai/docs/debugging/local_debugging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:02.058809606-03:00",
      "description": "There's 2 ways to do local debugging - litellm.turnondebug() and by passing in a custom function completion(...loggerfn=). Warning: Make sure to not use turnon_debug() in production. It logs API keys, which might end up in log files.",
      "summary": "This document explains how to perform local debugging and logging for LLM calls using LiteLLM's built-in debug mode and custom logging functions.",
      "tags": [
        "litellm",
        "debugging",
        "logging",
        "python-sdk",
        "api-monitoring",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-debugging-local-debugging.md"
    },
    {
      "file_path": "075-docs-enterprise.md",
      "title": "Enterprise | liteLLM",
      "url": "https://docs.litellm.ai/docs/enterprise",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:08.274559796-03:00",
      "description": "- âœ¨ SSO is free for up to 5 users. After that, an enterprise license is required. Get Started with Enterprise here",
      "summary": "This document outlines the LiteLLM Enterprise features, deployment models, and support services, covering self-hosted and hosted proxy options for large-scale AI implementations.",
      "tags": [
        "enterprise-license",
        "sso-authentication",
        "self-hosted-proxy",
        "hosted-proxy",
        "service-level-agreement",
        "data-security"
      ],
      "category": "guide",
      "original_file_path": "docs-enterprise.md"
    },
    {
      "file_path": "076-docs-extras-code-quality.md",
      "title": "Code Quality | liteLLM",
      "url": "https://docs.litellm.ai/docs/extras/code_quality",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:10.204814978-03:00",
      "description": "ðŸš… LiteLLM follows the Google Python Style Guide.",
      "summary": "This document outlines the coding standards and development tools used by LiteLLM to maintain code quality, including specific linting, formatting, and typing utilities.",
      "tags": [
        "python-style-guide",
        "linting",
        "formatting",
        "type-checking",
        "ruff",
        "black-formatter",
        "code-quality"
      ],
      "category": "guide",
      "original_file_path": "docs-extras-code-quality.md"
    },
    {
      "file_path": "077-docs-extras-contributing-code.md",
      "title": "Contributing Code | liteLLM",
      "url": "https://docs.litellm.ai/docs/extras/contributing_code",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:13.426070173-03:00",
      "description": "Checklist before submitting a PR",
      "summary": "Outlines the required procedures and technical setup for contributing to the LiteLLM project, including PR checklists, testing protocols, and local environment configuration.",
      "tags": [
        "contribution-guide",
        "pull-request",
        "unit-testing",
        "development-setup",
        "docker",
        "litellm",
        "linting"
      ],
      "category": "guide",
      "original_file_path": "docs-extras-contributing-code.md"
    },
    {
      "file_path": "078-docs-extras-contributing.md",
      "title": "Contributing to Documentation | liteLLM",
      "url": "https://docs.litellm.ai/docs/extras/contributing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:11.970727534-03:00",
      "description": "This website is built using Docusaurus 2, a modern static website generator.",
      "summary": "This document provides instructions for setting up a local development environment to run, preview, and contribute to the LiteLLM documentation using Docusaurus.",
      "tags": [
        "documentation",
        "local-setup",
        "docusaurus",
        "contributing-guide",
        "git-workflow",
        "npm-packages"
      ],
      "category": "guide",
      "original_file_path": "docs-extras-contributing.md"
    },
    {
      "file_path": "079-docs-extras-creating-adapters.md",
      "title": "Call any LiteLLM model in your custom format | liteLLM",
      "url": "https://docs.litellm.ai/docs/extras/creating_adapters",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:16.552261767-03:00",
      "description": "Use this to call any LiteLLM supported .completion() model, in your custom format. Useful if you have a custom API and want to support any LiteLLM supported model.",
      "summary": "This document explains how to create and implement custom adapters in LiteLLM to translate between proprietary API request/response formats and the internal OpenAI-compatible standard.",
      "tags": [
        "litellm",
        "custom-adapters",
        "api-translation",
        "python-sdk",
        "llm-integration",
        "streaming",
        "async-support"
      ],
      "category": "guide",
      "original_file_path": "docs-extras-creating-adapters.md"
    },
    {
      "file_path": "080-docs-extras-gemini-img-migration.md",
      "title": "Gemini Image Generation Migration Guide | liteLLM",
      "url": "https://docs.litellm.ai/docs/extras/gemini_img_migration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:16.683715462-03:00",
      "description": "Who is impacted by this change?",
      "summary": "This document details a breaking change in LiteLLM v1.77.0 that updates the response format for Gemini image generation models from a single field to a list of images. It provides migration examples and configuration steps for both the Python SDK and the LiteLLM Proxy Server.",
      "tags": [
        "litellm",
        "gemini",
        "image-generation",
        "api-change",
        "migration-guide",
        "python-sdk",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-extras-gemini-img-migration.md"
    },
    {
      "file_path": "081-docs-files-endpoints.md",
      "title": "Provider Files Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/files_endpoints",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:18.071431779-03:00",
      "description": "Files are used to upload documents that can be used with features like Assistants, Fine-tuning, and Batch API.",
      "summary": "This document explains how to manage file uploads and operations through the LiteLLM proxy, featuring multi-account routing via encoded file IDs and support for various model providers.",
      "tags": [
        "litellm",
        "file-management",
        "multi-account",
        "openai-api",
        "batch-api",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-files-endpoints.md"
    },
    {
      "file_path": "082-docs-fine-tuning.md",
      "title": "/fine_tuning | liteLLM",
      "url": "https://docs.litellm.ai/docs/fine_tuning",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:18.277278999-03:00",
      "description": "This is an Enterprise only endpoint Get Started with Enterprise here",
      "summary": "This document explains how to configure and use LiteLLM to manage files and execute fine-tuning jobs across multiple providers like Azure OpenAI, Vertex AI, and OpenAI.",
      "tags": [
        "litellm",
        "fine-tuning",
        "llm-proxy",
        "azure-openai",
        "vertex-ai",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-fine-tuning.md"
    },
    {
      "file_path": "083-docs-generateContent.md",
      "title": "/generateContent | liteLLM",
      "url": "https://docs.litellm.ai/docs/generateContent",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:18.97605738-03:00",
      "description": "Use LiteLLM to call Google AI's generateContent endpoints for text generation, multimodal interactions, and streaming responses.",
      "summary": "This document provides instructions and code examples for using LiteLLM to interact with Google Gemini models through the Python SDK and Proxy Server. It covers setup for text generation, streaming responses, and configuration for various integration methods.",
      "tags": [
        "litellm",
        "google-gemini",
        "python-sdk",
        "llm-proxy",
        "streaming",
        "text-generation",
        "google-ai"
      ],
      "category": "guide",
      "original_file_path": "docs-generateContent.md"
    },
    {
      "file_path": "084-docs-guides-code-interpreter.md",
      "title": "Code Interpreter | liteLLM",
      "url": "https://docs.litellm.ai/docs/guides/code_interpreter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:22.081348725-03:00",
      "description": "Use OpenAI's Code Interpreter tool to execute Python code in a secure, sandboxed environment.",
      "summary": "This document provides instructions for using OpenAI's Code Interpreter tool through the LiteLLM Python SDK and AI Gateway, including code execution and file management.",
      "tags": [
        "litellm",
        "openai",
        "code-interpreter",
        "python-sdk",
        "ai-gateway",
        "data-visualization"
      ],
      "category": "guide",
      "original_file_path": "docs-guides-code-interpreter.md"
    },
    {
      "file_path": "085-docs-guides-finetuned-models.md",
      "title": "Calling Finetuned Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/guides/finetuned_models",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:23.480407964-03:00",
      "description": "OpenAI",
      "summary": "This document provides the syntax and configuration details for calling fine-tuned models from OpenAI and Vertex AI using the LiteLLM library.",
      "tags": [
        "litellm",
        "openai",
        "vertex-ai",
        "fine-tuning",
        "model-configuration",
        "python-sdk"
      ],
      "category": "reference",
      "original_file_path": "docs-guides-finetuned-models.md"
    },
    {
      "file_path": "086-docs-guides-security-settings.md",
      "title": "SSL, HTTP Proxy Security Settings | liteLLM",
      "url": "https://docs.litellm.ai/docs/guides/security_settings",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:23.636755269-03:00",
      "description": "If you're in an environment using an older TTS bundle, with an older encryption, follow this guide. By default",
      "summary": "This document explains how to configure SSL and TLS settings in LiteLLM, covering custom CA bundles, verification disabling, security levels, and certificate-based authentication.",
      "tags": [
        "litellm",
        "ssl-configuration",
        "tls-security",
        "ca-bundle",
        "network-settings",
        "proxy-configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-guides-security-settings.md"
    },
    {
      "file_path": "087-docs-image-edits.md",
      "title": "/images/edits | liteLLM",
      "url": "https://docs.litellm.ai/docs/image_edits",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:25.371773891-03:00",
      "description": "LiteLLM provides image editing functionality that maps to OpenAI's /images/edits API endpoint. Now supports both single and multiple image editing.",
      "summary": "This document explains how to use LiteLLM's image editing capabilities across multiple providers like OpenAI and Gemini via the Python SDK and Proxy server.",
      "tags": [
        "image-editing",
        "litellm",
        "openai-api",
        "gemini-api",
        "python-sdk",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-image-edits.md"
    },
    {
      "file_path": "088-docs-integrations-community.md",
      "title": "Be an Integration Partner | liteLLM",
      "url": "https://docs.litellm.ai/docs/integrations/community",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:29.176268257-03:00",
      "description": "Welcome, integration partners! ðŸ‘‹",
      "summary": "This document outlines the onboarding process for LiteLLM integration partners, providing instructions on how to join the community and access direct support.",
      "tags": [
        "litellm",
        "integration-partners",
        "community-support",
        "onboarding",
        "slack-channel"
      ],
      "category": "guide",
      "original_file_path": "docs-integrations-community.md"
    },
    {
      "file_path": "089-docs-integrations-letta.md",
      "title": "Letta Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/integrations/letta",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:32.527182797-03:00",
      "description": "Letta (formerly MemGPT) is a framework for building stateful LLM agents with persistent memory. This guide shows how to integrate both LiteLLM SDK and LiteLLM Proxy with Letta to leverage multiple LLM providers while building memory-enabled agents.",
      "summary": "This guide explains how to integrate the Letta framework with LiteLLM Proxy and SDK to build memory-enabled, stateful AI agents. It covers model configuration, agent creation, and advanced features like load balancing and tool use across different LLM providers.",
      "tags": [
        "letta",
        "litellm",
        "ai-agents",
        "persistent-memory",
        "llm-proxy",
        "model-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-integrations-letta.md"
    },
    {
      "file_path": "090-docs-langchain.md",
      "title": "Using ChatLiteLLM() - Langchain | liteLLM",
      "url": "https://docs.litellm.ai/docs/langchain/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:33.716706976-03:00",
      "description": "Pre-Requisites",
      "summary": "This document provides a guide on integrating LangChain's ChatLiteLLM with various observability tools and implementing advanced metadata tagging for LLM request tracking.",
      "tags": [
        "litellm",
        "langchain",
        "observability",
        "mlflow",
        "metadata-tagging",
        "llm-monitoring",
        "cost-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-langchain.md"
    },
    {
      "file_path": "091-docs-load-test-advanced.md",
      "title": "LiteLLM Proxy - 1K RPS Load test on locust | liteLLM",
      "url": "https://docs.litellm.ai/docs/load_test_advanced",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:34.02511847-03:00",
      "description": "Tutorial on how to get to 1K+ RPS with LiteLLM Proxy on locust",
      "summary": "This document provides instructions and a code example for performing load tests on LLM deployments using the Locust framework to measure throughput and API performance.",
      "tags": [
        "load-testing",
        "llm-performance",
        "locust",
        "api-benchmarking",
        "chat-completions"
      ],
      "category": "tutorial",
      "original_file_path": "docs-load-test-advanced.md"
    },
    {
      "file_path": "092-docs-load-test-rpm.md",
      "title": "Multi-Instance TPM/RPM (litellm.Router) | liteLLM",
      "url": "https://docs.litellm.ai/docs/load_test_rpm",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:38.669428069-03:00",
      "description": "Test if your defined tpm/rpm limits are respected across multiple instances of the Router object.",
      "summary": "This document provides scripts and instructions for load testing the LiteLLM Router and Proxy to verify that RPM and TPM rate limits are correctly enforced across multiple instances.",
      "tags": [
        "litellm",
        "load-testing",
        "rate-limiting",
        "router",
        "proxy-testing",
        "rpm-limits",
        "tpm-limits"
      ],
      "category": "guide",
      "original_file_path": "docs-load-test-rpm.md"
    },
    {
      "file_path": "093-docs-load-test-sdk.md",
      "title": "LiteLLM SDK vs OpenAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/load_test_sdk",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:38.851185311-03:00",
      "description": "Here is a script to load test LiteLLM vs OpenAI",
      "summary": "This document provides a code implementation for performing asynchronous load testing across LiteLLM proxy, router, and Azure OpenAI clients. It demonstrates how to configure multiple LLM providers and execute concurrent chat completion requests to measure performance.",
      "tags": [
        "litellm",
        "load-testing",
        "python",
        "asyncio",
        "azure-openai",
        "llm-proxy",
        "concurrency"
      ],
      "category": "tutorial",
      "original_file_path": "docs-load-test-sdk.md"
    },
    {
      "file_path": "094-docs-load-test.md",
      "title": "LiteLLM Proxy - Locust Load Test | liteLLM",
      "url": "https://docs.litellm.ai/docs/load_test",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:33.870889569-03:00",
      "description": "Locust Load Test LiteLLM Proxy",
      "summary": "This document provides instructions for performing load testing on a LiteLLM Proxy using the Locust framework and a simulated OpenAI endpoint. It covers configuration steps, installation, and how to evaluate response times and health readiness via the Locust web interface.",
      "tags": [
        "litellm",
        "locust",
        "load-testing",
        "performance-benchmarking",
        "openai-proxy",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-load-test.md"
    },
    {
      "file_path": "095-docs-mcp-cost.md",
      "title": "MCP Cost Tracking | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp_cost",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:41.018844265-03:00",
      "description": "LiteLLM provides two ways to track costs for MCP tool calls:",
      "summary": "This document explains how to track and manage costs for Model Context Protocol (MCP) tool calls in LiteLLM using either static configuration or custom dynamic hooks.",
      "tags": [
        "litellm",
        "mcp-protocol",
        "cost-tracking",
        "tool-calling",
        "configuration",
        "python-hooks"
      ],
      "category": "guide",
      "original_file_path": "docs-mcp-cost.md"
    },
    {
      "file_path": "096-docs-mcp-guardrail.md",
      "title": "MCP Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp_guardrail",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:43.869826869-03:00",
      "description": "LiteLLM supports applying guardrails to MCP tool calls to ensure security and compliance. You can configure guardrails to run before or during MCP calls to validate inputs and block or mask sensitive information.",
      "summary": "This document explains how to configure and implement security guardrails for MCP tool calls in LiteLLM to validate inputs and protect sensitive information.",
      "tags": [
        "litellm",
        "mcp-tools",
        "guardrails",
        "data-security",
        "pii-masking",
        "input-validation",
        "model-context-protocol"
      ],
      "category": "guide",
      "original_file_path": "docs-mcp-guardrail.md"
    },
    {
      "file_path": "097-docs-mcp-troubleshoot.md",
      "title": "MCP Troubleshooting Guide | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp_troubleshoot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:43.919879832-03:00",
      "description": "When LiteLLM acts as an MCP proxy, traffic normally flows Client â†’ LiteLLM Proxy â†’ MCP Server, while OAuth-enabled setups add an authorization server for metadata discovery.",
      "summary": "This document provides a comprehensive troubleshooting guide for diagnosing connectivity and configuration issues when using LiteLLM as an MCP proxy. It explains how to isolate failures between client, proxy, and server hops using tools like the MCP Inspector and curl smoke tests.",
      "tags": [
        "litellm",
        "mcp-proxy",
        "troubleshooting",
        "debugging",
        "connectivity-test",
        "oauth-discovery",
        "json-rpc"
      ],
      "category": "guide",
      "original_file_path": "docs-mcp-troubleshoot.md"
    },
    {
      "file_path": "098-docs-migration-policy.md",
      "title": "Migration Policy | liteLLM",
      "url": "https://docs.litellm.ai/docs/migration_policy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:46.794067159-03:00",
      "description": "New Beta Feature Introduction",
      "summary": "This document outlines the policy and support procedures for beta features that may transition to the LiteLLM Enterprise Tier.",
      "tags": [
        "beta-features",
        "enterprise-tier",
        "licensing-policy",
        "software-lifecycle",
        "user-support"
      ],
      "category": "guide",
      "original_file_path": "docs-migration-policy.md"
    },
    {
      "file_path": "099-docs-migration.md",
      "title": "Migration Guide - LiteLLM v1.0.0+ | liteLLM",
      "url": "https://docs.litellm.ai/docs/migration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:45.881710464-03:00",
      "description": "When we have breaking changes (i.e. going from 1.x.x to 2.x.x), we will document those changes here.",
      "summary": "This document outlines the breaking changes and migration steps required for upgrading to LiteLLM version 1.0.0, including dependency updates and API modifications.",
      "tags": [
        "litellm",
        "breaking-changes",
        "migration-guide",
        "openai-v1",
        "api-updates",
        "versioning"
      ],
      "category": "guide",
      "original_file_path": "docs-migration.md"
    },
    {
      "file_path": "100-docs-observability-agentops-integration.md",
      "title": "ðŸ–‡ï¸ AgentOps - LLM Observability Platform | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/agentops_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:51.246535647-03:00",
      "description": "This is community maintained. Please make an issue if you run into a bug:",
      "summary": "This document explains how to integrate AgentOps with LiteLLM using callback functions to enable comprehensive observability and tracing for LLM operations across various providers.",
      "tags": [
        "agentops",
        "litellm",
        "observability",
        "llm-monitoring",
        "tracing",
        "callbacks"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-agentops-integration.md"
    },
    {
      "file_path": "101-docs-observability-argilla.md",
      "title": "Argilla | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/argilla",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:51.865082117-03:00",
      "description": "Argilla is a collaborative annotation tool for AI engineers and domain experts who need to build high-quality datasets for their projects.",
      "summary": "This document provides instructions on how to configure and create a dataset in Argilla using the Python client for collaborative AI data annotation.",
      "tags": [
        "argilla",
        "dataset-creation",
        "python-client",
        "data-annotation",
        "configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-argilla.md"
    },
    {
      "file_path": "102-docs-observability-arize-integration.md",
      "title": "Arize AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/arize_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:52.40740197-03:00",
      "description": "AI Observability and Evaluation Platform",
      "summary": "This document provides instructions for integrating Arize AI with LiteLLM to enable LLM observability, tracing, and logging through SDK callbacks and proxy configurations.",
      "tags": [
        "arize-ai",
        "litellm",
        "llm-observability",
        "tracing",
        "monitoring",
        "callback-functions",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-arize-integration.md"
    },
    {
      "file_path": "103-docs-observability-athina-integration.md",
      "title": "Athina | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/athina_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:53.538831816-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document explains how to integrate the Athina monitoring and evaluation platform with LiteLLM using callbacks to log inference data. It covers configuration steps, environment variables, and detailed metadata fields for advanced request tracking.",
      "tags": [
        "athina-ai",
        "litellm",
        "llm-monitoring",
        "observability",
        "logging-callbacks",
        "inference-analytics"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-athina-integration.md"
    },
    {
      "file_path": "104-docs-observability-azure-sentinel.md",
      "title": "Azure Sentinel | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/azure_sentinel",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:53.57740744-03:00",
      "description": "LiteLLM supports logging to Azure Sentinel via the Azure Monitor Logs Ingestion API. Azure Sentinel uses Log Analytics workspaces for data storage, so logs sent to the workspace will be available in Sentinel for security monitoring and analysis.",
      "summary": "This document provides a comprehensive guide for integrating LiteLLM with Azure Sentinel via the Azure Monitor Logs Ingestion API to enable security monitoring and analysis of LLM calls.",
      "tags": [
        "litellm",
        "azure-sentinel",
        "azure-monitor",
        "logging-integration",
        "security-monitoring",
        "log-analytics",
        "azure-active-directory"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-azure-sentinel.md"
    },
    {
      "file_path": "105-docs-observability-braintrust.md",
      "title": "Braintrust - Evals + Logging | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/braintrust",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:56.270110809-03:00",
      "description": "Braintrust manages evaluations, logging, prompt playground, to data management for AI products.",
      "summary": "This guide explains how to integrate LiteLLM with Braintrust to automate logging, evaluation, and trace management for AI applications. It details setup procedures for both the Python SDK and the OpenAI-compatible proxy, including advanced metadata and span customization.",
      "tags": [
        "litellm",
        "braintrust",
        "observability",
        "ai-logging",
        "llm-proxy",
        "tracing",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-braintrust.md"
    },
    {
      "file_path": "106-docs-observability-cloudzero.md",
      "title": "CloudZero Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/cloudzero",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:58.863596332-03:00",
      "description": "LiteLLM provides an integration with CloudZero's AnyCost API, allowing you to export your LLM usage data to CloudZero for cost tracking analysis.",
      "summary": "This document explains how to integrate LiteLLM with CloudZero's AnyCost API to export and track LLM usage data for cost analysis. It covers configuration steps for environment variables, YAML settings, and UI-based setup, as well as testing procedures.",
      "tags": [
        "litellm",
        "cloudzero",
        "cost-tracking",
        "data-export",
        "usage-monitoring",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-cloudzero.md"
    },
    {
      "file_path": "107-docs-observability-custom-callback.md",
      "title": "Custom Callbacks | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/custom_callback",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:00.641107793-03:00",
      "description": "For PROXY Go Here",
      "summary": "This document explains how to implement custom callback classes and functions in LiteLLM to log events, track costs, and modify responses during LLM API interactions.",
      "tags": [
        "litellm",
        "callbacks",
        "logging",
        "async-hooks",
        "event-handling",
        "monitoring",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-custom-callback.md"
    },
    {
      "file_path": "108-docs-observability-datadog.md",
      "title": "DataDog | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/datadog",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:02.022628059-03:00",
      "description": "LiteLLM Supports logging to the following Datdog Integrations:",
      "summary": "This document provides instructions for integrating LiteLLM with Datadog services including Logs, LLM Observability, and Tracing for monitoring model performance and health.",
      "tags": [
        "litellm",
        "datadog",
        "observability",
        "logging",
        "tracing",
        "monitoring",
        "configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-datadog.md"
    },
    {
      "file_path": "109-docs-observability-deepeval-integration.md",
      "title": "ðŸ”­ DeepEval - Open-Source Evals with Tracing | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/deepeval_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:02.132676244-03:00",
      "description": "What is DeepEval?",
      "summary": "This document explains how to integrate the Confident AI observatory using deepeval to trace and monitor LLM applications via LiteLLM callbacks.",
      "tags": [
        "deepeval",
        "llm-monitoring",
        "llm-tracing",
        "observability",
        "litellm",
        "confident-ai"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-deepeval-integration.md"
    },
    {
      "file_path": "110-docs-observability-focus.md",
      "title": "Focus Export (Experimental) | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/focus",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:02.77683232-03:00",
      "description": "Focus Format export is under active development and currently considered experimental.",
      "summary": "This document describes the experimental feature in LiteLLM for exporting usage data in the FinOps FOCUS v1.2 format to cloud storage like Amazon S3 for cost analysis.",
      "tags": [
        "litellm",
        "finops",
        "focus-format",
        "amazon-s3",
        "cost-analysis",
        "usage-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-focus.md"
    },
    {
      "file_path": "111-docs-observability-gcs-bucket-integration.md",
      "title": "Google Cloud Storage Buckets | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/gcs_bucket_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:03.945654075-03:00",
      "description": "Log LLM Logs to Google Cloud Storage Buckets",
      "summary": "This document provides step-by-step instructions for configuring LiteLLM to log request and response data directly to Google Cloud Storage (GCS) buckets.",
      "tags": [
        "litellm",
        "google-cloud-storage",
        "logging",
        "callbacks",
        "gcs-bucket",
        "cloud-logging"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-gcs-bucket-integration.md"
    },
    {
      "file_path": "112-docs-observability-greenscale-integration.md",
      "title": "Greenscale - Track LLM Spend and Responsible Usage | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/greenscale_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:07.181337601-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document explains how to integrate Greenscale with liteLLM to monitor GenAI spending and usage through automated metadata logging and callbacks.",
      "tags": [
        "litellm",
        "greenscale",
        "llm-monitoring",
        "observability",
        "metadata-logging",
        "callbacks"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-greenscale-integration.md"
    },
    {
      "file_path": "113-docs-observability-helicone-integration.md",
      "title": "Helicone - OSS LLM Observability Platform | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/helicone_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:07.809804678-03:00",
      "description": "This is community maintained. Please make an issue if you run into a bug:",
      "summary": "This document demonstrates how to integrate LiteLLM with Helicone to route requests and configure advanced features such as caching, rate limiting, and custom metadata through headers.",
      "tags": [
        "litellm",
        "helicone",
        "api-integration",
        "caching",
        "rate-limiting",
        "error-handling",
        "request-metadata"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-helicone-integration.md"
    },
    {
      "file_path": "114-docs-observability-humanloop.md",
      "title": "Humanloop | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/humanloop",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:09.035597063-03:00",
      "description": "Humanloop enables product teams to build robust AI features with LLMs, using best-in-class tooling for Evaluation, Prompt Management, and Observability.",
      "summary": "This document provides instructions on how to integrate Humanloop with LiteLLM for prompt management and model completions.",
      "tags": [
        "humanloop",
        "litellm",
        "prompt-management",
        "llm-integration",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-humanloop.md"
    },
    {
      "file_path": "115-docs-observability-lago.md",
      "title": "Lago - Usage Based Billing | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/lago",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:10.531397992-03:00",
      "description": "Lago offers a self-hosted and cloud, metering and usage-based billing solution.",
      "summary": "This document provides instructions for integrating Lago with LiteLLM to enable usage-based billing and automated cost tracking via success callbacks.",
      "tags": [
        "lago",
        "litellm",
        "usage-based-billing",
        "callbacks",
        "cost-tracking",
        "api-logging"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-lago.md"
    },
    {
      "file_path": "116-docs-observability-langfuse-integration.md",
      "title": "ðŸª¢ Langfuse - Logging LLM Input/Output | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/langfuse_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:12.845569285-03:00",
      "description": "What is Langfuse?",
      "summary": "This document explains how to integrate Langfuse with LiteLLM to enable model tracing, prompt management, and application evaluation. It provides setup instructions for both the LiteLLM Proxy and Python SDK, including advanced configuration for custom metadata and trace management.",
      "tags": [
        "langfuse",
        "litellm",
        "llm-observability",
        "tracing",
        "python-sdk",
        "llm-gateway",
        "metadata-management"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-langfuse-integration.md"
    },
    {
      "file_path": "117-docs-observability-langfuse-otel-integration.md",
      "title": "ðŸª¢ Langfuse OpenTelemetry Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/langfuse_otel_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:12.857718636-03:00",
      "description": "The Langfuse OpenTelemetry integration allows you to send LiteLLM traces and observability data to Langfuse using the OpenTelemetry protocol. This provides a standardized way to collect and analyze your LLM usage data.",
      "summary": "This document provides a comprehensive guide for integrating Langfuse with LiteLLM using the OpenTelemetry protocol to collect and analyze LLM trace and observability data.",
      "tags": [
        "langfuse",
        "opentelemetry",
        "litellm",
        "observability",
        "llm-tracing",
        "otel-integration",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-langfuse-otel-integration.md"
    },
    {
      "file_path": "118-docs-observability-langsmith-integration.md",
      "title": "Langsmith - Logging LLM Input/Output | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/langsmith_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:13.776406726-03:00",
      "description": "An all-in-one developer platform for every step of the application lifecycle",
      "summary": "This document explains how to integrate LiteLLM with LangSmith for automated logging and tracing of LLM responses across different providers.",
      "tags": [
        "litellm",
        "langsmith",
        "logging",
        "observability",
        "callbacks",
        "llm-monitoring"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-langsmith-integration.md"
    },
    {
      "file_path": "119-docs-observability-langtrace-integration.md",
      "title": "Langtrace AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/langtrace_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:15.351386766-03:00",
      "description": "Monitor, evaluate & improve your LLM apps",
      "summary": "Explains how to integrate Langtrace with LiteLLM to automatically log LLM responses from various providers using a simple callback configuration.",
      "tags": [
        "litellm",
        "langtrace",
        "logging",
        "observability",
        "integration",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-langtrace-integration.md"
    },
    {
      "file_path": "120-docs-observability-levo-integration.md",
      "title": "Levo AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/levo_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:15.867829984-03:00",
      "description": "Levo is an AI observability and compliance platform that provides comprehensive monitoring, analysis, and compliance tracking for LLM applications.",
      "summary": "This document explains how to integrate the Levo AI observability platform with LiteLLM to enable monitoring, compliance tracking, and performance metrics for LLM applications. It provides setup instructions, configuration details, and troubleshooting steps for sending trace data via OpenTelemetry.",
      "tags": [
        "levo",
        "litellm",
        "observability",
        "opentelemetry",
        "llm-monitoring",
        "compliance",
        "tracing"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-levo-integration.md"
    },
    {
      "file_path": "121-docs-observability-literalai-integration.md",
      "title": "Literal AI - Log, Evaluate, Monitor | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/literalai_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:18.237307029-03:00",
      "description": "Literal AI is a collaborative observability, evaluation and analytics platform for building production-grade LLM apps.",
      "summary": "This document explains how to integrate Literal AI with LiteLLM to enable observability, tracing, and analytics for LLM applications. It provides instructions for using SDK decorators and instrumenting the LiteLLM proxy to log conversations and generations.",
      "tags": [
        "literal-ai",
        "litellm",
        "observability",
        "llm-tracing",
        "python-sdk",
        "monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-literalai-integration.md"
    },
    {
      "file_path": "122-docs-observability-logfire-integration.md",
      "title": "Logfire | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/logfire_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:19.773955765-03:00",
      "description": "Logfire is open Source Observability & Analytics for LLM Apps",
      "summary": "This document provides instructions for integrating Logfire with LiteLLM to enable observability, analytics, and production tracing for LLM applications.",
      "tags": [
        "logfire",
        "litellm",
        "observability",
        "llm-monitoring",
        "analytics",
        "tracing",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-logfire-integration.md"
    },
    {
      "file_path": "123-docs-observability-lunary-integration.md",
      "title": "ðŸŒ™ Lunary - GenAI Observability | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/lunary_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:19.950533922-03:00",
      "description": "Lunary is an open-source platform providing observability, prompt management, and analytics to help team manage and improve LLM chatbots.",
      "summary": "This document provides instructions for integrating Lunary with LiteLLM to enable observability, prompt management, and analytics across various LLM providers. It covers implementation via the Python SDK, LangChain, and the LiteLLM Proxy Server.",
      "tags": [
        "lunary",
        "litellm",
        "observability",
        "llm-monitoring",
        "prompt-management",
        "python-sdk",
        "langchain"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-lunary-integration.md"
    },
    {
      "file_path": "124-docs-observability-mlflow.md",
      "title": "ðŸ” MLflow - OSS LLM Observability and Evaluation | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/mlflow",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:20.840402993-03:00",
      "description": "What is MLflow?",
      "summary": "This document explains how to integrate MLflow with LiteLLM to enable auto-tracing, observability, and evaluation for Large Language Model applications.",
      "tags": [
        "mlflow",
        "litellm",
        "observability",
        "tracing",
        "opentelemetry",
        "mlops",
        "llm-evaluation",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-mlflow.md"
    },
    {
      "file_path": "125-docs-observability-openmeter.md",
      "title": "OpenMeter - Usage-Based Billing | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/openmeter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:22.138846628-03:00",
      "description": "OpenMeter is an Open Source Usage-Based Billing solution for AI/Cloud applications. It integrates with Stripe for easy billing.",
      "summary": "This document provides instructions on integrating OpenMeter with LiteLLM to automatically log LLM usage and costs for usage-based billing applications.",
      "tags": [
        "openmeter",
        "litellm",
        "billing",
        "usage-tracking",
        "callbacks",
        "llm-monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-openmeter.md"
    },
    {
      "file_path": "126-docs-observability-opentelemetry-integration.md",
      "title": "OpenTelemetry - Tracing LLMs with any observability tool | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/opentelemetry_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:25.231115367-03:00",
      "description": "OpenTelemetry is a CNCF standard for observability. It connects to any observability tool, such as Jaeger, Zipkin, Datadog, New Relic, Traceloop, Levo AI and others.",
      "summary": "This document provides instructions for integrating OpenTelemetry with LiteLLM to enable observability and trace logging across various providers. It covers setup procedures, configuration options for redacting sensitive data, and troubleshooting common integration issues.",
      "tags": [
        "opentelemetry",
        "litellm",
        "observability",
        "tracing",
        "python-sdk",
        "data-redaction",
        "troubleshooting"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-opentelemetry-integration.md"
    },
    {
      "file_path": "127-docs-observability-opik-integration.md",
      "title": "Comet Opik - Logging + Evals | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/opik_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:25.648458465-03:00",
      "description": "Opik is an open source end-to-end LLM Evaluation Platform that helps developers track their LLM prompts and responses during both development and production. Users can define and run evaluations to test their LLMs apps before deployment to check for hallucinations, accuracy, context retrevial, and more!",
      "summary": "This document explains how to integrate Opik with LiteLLM to track and evaluate LLM prompts and responses across various providers. It covers configuration for both the SDK and Proxy, including how to pass project-specific metadata and handle tracing.",
      "tags": [
        "opik",
        "litellm",
        "llm-evaluation",
        "tracing",
        "callbacks",
        "observability",
        "llm-monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-opik-integration.md"
    },
    {
      "file_path": "128-docs-observability-phoenix-integration.md",
      "title": "Arize Phoenix OSS | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/phoenix_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:26.690017865-03:00",
      "description": "Open source tracing and evaluation platform",
      "summary": "This document provides instructions for integrating LiteLLM with Arize Phoenix to enable tracing and evaluation of LLM interactions via callbacks and proxy configurations.",
      "tags": [
        "litellm",
        "arize-phoenix",
        "tracing",
        "observability",
        "llm-ops",
        "python",
        "integration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-phoenix-integration.md"
    },
    {
      "file_path": "129-docs-observability-posthog-integration.md",
      "title": "PostHog - Tracking LLM Usage Analytics | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/posthog_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:26.858865343-03:00",
      "description": "What is PostHog?",
      "summary": "This document explains how to integrate PostHog with LiteLLM to track and analyze LLM application metrics, usage, and performance through both the Python SDK and Proxy gateway.",
      "tags": [
        "posthog",
        "litellm",
        "product-analytics",
        "observability",
        "llm-monitoring",
        "logging-callbacks"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-posthog-integration.md"
    },
    {
      "file_path": "130-docs-observability-promptlayer-integration.md",
      "title": "Promptlayer Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/promptlayer_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:27.731730144-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document explains how to integrate liteLLM with Promptlayer using success callbacks to log and track LLM requests and metadata across multiple providers.",
      "tags": [
        "litellm",
        "promptlayer",
        "logging",
        "callbacks",
        "llm-monitoring",
        "integration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-promptlayer-integration.md"
    },
    {
      "file_path": "131-docs-observability-qualifire-integration.md",
      "title": "Qualifire - LLM Evaluation, Guardrails & Observability | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/qualifire_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:30.648243702-03:00",
      "description": "Qualifire provides real-time Agentic evaluations, guardrails and observability for production AI applications.",
      "summary": "This document explains how to integrate Qualifire with LiteLLM to enable real-time evaluations, guardrails, and observability for production AI applications.",
      "tags": [
        "qualifire",
        "litellm",
        "ai-observability",
        "guardrails",
        "llm-monitoring",
        "callbacks"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-qualifire-integration.md"
    },
    {
      "file_path": "132-docs-observability-raw-request-response.md",
      "title": "Raw Request/Response Logging | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/raw_request_response",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:30.672638115-03:00",
      "description": "Logging",
      "summary": "This document provides a code example for integrating LiteLLM with Langfuse to automatically log and track Large Language Model requests and responses using callbacks.",
      "tags": [
        "litellm",
        "langfuse",
        "llm-observability",
        "logging",
        "python",
        "monitoring"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-raw-request-response.md"
    },
    {
      "file_path": "133-docs-observability-scrub-data.md",
      "title": "Scrub Logged Data | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/scrub_data",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:32.177618122-03:00",
      "description": "Redact messages / mask PII before sending data to logging integrations (langfuse/etc.).",
      "summary": "This document explains how to redact sensitive information or mask PII in message logs using custom logging hooks in LiteLLM before data is sent to external integrations.",
      "tags": [
        "litellm",
        "data-masking",
        "pii-redaction",
        "logging",
        "custom-logger",
        "langfuse",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-scrub-data.md"
    },
    {
      "file_path": "134-docs-observability-sentry.md",
      "title": "Sentry - Log LLM Exceptions | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/sentry",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:32.555929112-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document explains how to integrate LiteLLM with Sentry for error monitoring, breadcrumbing, and transaction logging in production applications.",
      "tags": [
        "litellm",
        "sentry",
        "error-monitoring",
        "integration",
        "exception-handling",
        "logging-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-sentry.md"
    },
    {
      "file_path": "135-docs-observability-signoz.md",
      "title": "SigNoz LiteLLM Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/signoz",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:35.274792303-03:00",
      "description": "For more details on setting up observability for LiteLLM, check out the SigNoz LiteLLM observability docs.",
      "summary": "This guide explains how to integrate LiteLLM with SigNoz using OpenTelemetry to capture logs, traces, and metrics for AI applications. It provides step-by-step instructions for instrumenting the LiteLLM SDK to monitor model performance and system-level metrics.",
      "tags": [
        "litellm",
        "signoz",
        "opentelemetry",
        "observability",
        "llm-monitoring",
        "tracing"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-signoz.md"
    },
    {
      "file_path": "136-docs-observability-slack-integration.md",
      "title": "Slack - Logging LLM Input/Output, Exceptions | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/slack_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:35.605270968-03:00",
      "description": "We want to learn how we can make the callbacks better! Meet the LiteLLM founders or",
      "summary": "This document explains how to implement and register a custom Slack alert callback function in LiteLLM for monitoring success and failure events.",
      "tags": [
        "litellm",
        "slack-integration",
        "callbacks",
        "logging",
        "webhooks",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-slack-integration.md"
    },
    {
      "file_path": "137-docs-observability-sumologic-integration.md",
      "title": "Sumo Logic | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/sumologic_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:36.246063958-03:00",
      "description": "Send LiteLLM logs to Sumo Logic for observability, monitoring, and analysis.",
      "summary": "This document provides instructions for integrating LiteLLM with Sumo Logic to enable real-time observability, cost tracking, and performance monitoring of LLM calls.",
      "tags": [
        "litellm",
        "sumo-logic",
        "observability",
        "logging",
        "monitoring",
        "ndjson"
      ],
      "category": "guide",
      "original_file_path": "docs-observability-sumologic-integration.md"
    },
    {
      "file_path": "138-docs-observability-supabase-integration.md",
      "title": "Supabase Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/supabase_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:36.891586369-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document explains how to integrate LiteLLM with Supabase to log LLM requests and track total spend across multiple providers using success and failure callbacks.",
      "tags": [
        "litellm",
        "supabase",
        "llm-logging",
        "cost-tracking",
        "database-integration",
        "error-logging",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-supabase-integration.md"
    },
    {
      "file_path": "139-docs-observability-wandb-integration.md",
      "title": "Weights & Biases - Logging LLM Input/Output | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/wandb_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:41.805082524-03:00",
      "description": "This is community maintained, Please make an issue if you run into a bug",
      "summary": "This document provides instructions on integrating Weights & Biases with LiteLLM to log model responses across multiple providers using success callbacks.",
      "tags": [
        "litellm",
        "weights-and-biases",
        "wandb",
        "observability",
        "llm-logging",
        "callbacks"
      ],
      "category": "tutorial",
      "original_file_path": "docs-observability-wandb-integration.md"
    },
    {
      "file_path": "140-docs-oidc.md",
      "title": "[BETA] OpenID Connect (OIDC) | liteLLM",
      "url": "https://docs.litellm.ai/docs/oidc",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:43.936268849-03:00",
      "description": "LiteLLM supports using OpenID Connect (OIDC) for authentication to upstream services . This allows you to avoid storing sensitive credentials in your configuration files.",
      "summary": "This document explains how to configure LiteLLM to use OpenID Connect (OIDC) for secure authentication to upstream LLM services without storing static credentials. It details supported identity providers and provides configuration steps for integrating with platforms like Amazon Bedrock and Azure OpenAI.",
      "tags": [
        "litellm",
        "oidc",
        "authentication",
        "security",
        "amazon-bedrock",
        "azure-openai",
        "identity-provider"
      ],
      "category": "guide",
      "original_file_path": "docs-oidc.md"
    },
    {
      "file_path": "141-docs-pass-through-anthropic-completion.md",
      "title": "Anthropic Passthrough | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/anthropic_completion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:44.756141585-03:00",
      "description": "Pass-through endpoints for Anthropic - call provider-specific endpoint, in native format (no translation).",
      "summary": "Explains how to use LiteLLM Proxy as a pass-through for native Anthropic API endpoints to enable features like cost tracking, logging, and virtual key management without format translation.",
      "tags": [
        "litellm-proxy",
        "anthropic",
        "api-passthrough",
        "cost-tracking",
        "streaming",
        "batch-processing",
        "virtual-keys"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-anthropic-completion.md"
    },
    {
      "file_path": "142-docs-pass-through-assembly-ai.md",
      "title": "Assembly AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/assembly_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:47.972868614-03:00",
      "description": "Pass-through endpoints for Assembly AI - call Assembly AI endpoints, in native format (no translation).",
      "summary": "This document explains how to use LiteLLM Proxy as a pass-through for Assembly AI endpoints, enabling features like cost tracking and logging while using the native Assembly AI SDK.",
      "tags": [
        "litellm",
        "assembly-ai",
        "api-proxy",
        "pass-through",
        "cost-tracking",
        "logging"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-assembly-ai.md"
    },
    {
      "file_path": "143-docs-pass-through-azure-passthrough.md",
      "title": "Azure Passthrough | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/azure_passthrough",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:49.651876351-03:00",
      "description": "Pass-through endpoints for /azure",
      "summary": "This document explains how to use LiteLLM pass-through endpoints to access Azure OpenAI features not yet natively supported, such as the Assistants and Threads APIs.",
      "tags": [
        "litellm",
        "azure-openai",
        "pass-through",
        "assistants-api",
        "proxy-server",
        "api-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-azure-passthrough.md"
    },
    {
      "file_path": "144-docs-pass-through-bedrock.md",
      "title": "Bedrock (boto3) SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/bedrock",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:50.794814914-03:00",
      "description": "Pass-through endpoints for Bedrock - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to use LiteLLM's pass-through endpoints to call AWS Bedrock services in their native format, including instructions for model routing and direct service access. It details how to configure the LiteLLM proxy for load balancing, cost tracking, and streaming across various Bedrock APIs.",
      "tags": [
        "aws-bedrock",
        "litellm-proxy",
        "pass-through-api",
        "load-balancing",
        "streaming",
        "model-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-bedrock.md"
    },
    {
      "file_path": "145-docs-pass-through-cohere.md",
      "title": "Cohere SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/cohere",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:51.419257782-03:00",
      "description": "Pass-through endpoints for Cohere - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to use LiteLLM Proxy as a pass-through for Cohere's native API endpoints to enable features like logging and cost tracking without changing request formats. It provides setup instructions and cURL examples for chat, rerank, and embedding endpoints.",
      "tags": [
        "litellm",
        "cohere",
        "pass-through-endpoints",
        "api-proxy",
        "rerank-api",
        "cost-tracking",
        "llm-ops"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-cohere.md"
    },
    {
      "file_path": "146-docs-pass-through-google-ai-studio.md",
      "title": "Google AI Studio SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/google_ai_studio",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:52.839626359-03:00",
      "description": "Pass-through endpoints for Google AI Studio - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to configure and use LiteLLM Proxy as a pass-through for Google AI Studio, enabling features like cost tracking and logging with native provider endpoints.",
      "tags": [
        "litellm-proxy",
        "google-ai-studio",
        "gemini-api",
        "pass-through-api",
        "usage-tracking",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-google-ai-studio.md"
    },
    {
      "file_path": "147-docs-pass-through-langfuse.md",
      "title": "Langfuse SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/langfuse",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:54.51651951-03:00",
      "description": "Pass-through endpoints for Langfuse - call langfuse endpoints with LiteLLM Virtual Key.",
      "summary": "This document explains how to configure LiteLLM Proxy as a pass-through for Langfuse endpoints, allowing users to route traces using LiteLLM virtual keys. It provides setup instructions for both basic pass-through and advanced usage with database-backed virtual keys.",
      "tags": [
        "litellm-proxy",
        "langfuse",
        "observability",
        "virtual-keys",
        "pass-through-endpoints",
        "tracing"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-langfuse.md"
    },
    {
      "file_path": "148-docs-pass-through-mistral.md",
      "title": "Mistral | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/mistral",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:56.08329868-03:00",
      "description": "Pass-through endpoints for Mistral - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to use LiteLLM Proxy as a pass-through for native Mistral API endpoints, allowing users to call provider-specific routes without translation while utilizing features like logging and virtual keys.",
      "tags": [
        "mistral-ai",
        "litellm-proxy",
        "pass-through-endpoints",
        "api-gateway",
        "virtual-keys",
        "ocr-api",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-mistral.md"
    },
    {
      "file_path": "149-docs-pass-through-openai-passthrough.md",
      "title": "OpenAI Passthrough | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/openai_passthrough",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:56.498784047-03:00",
      "description": "Pass-through endpoints for /openai",
      "summary": "This document explains how to use LiteLLM's pass-through endpoints to access newer OpenAI features like the Assistants API by routing requests through the LiteLLM proxy.",
      "tags": [
        "litellm",
        "openai-api",
        "assistants-api",
        "proxy-configuration",
        "pass-through"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-openai-passthrough.md"
    },
    {
      "file_path": "150-docs-pass-through-vertex-ai-live-websocket.md",
      "title": "Vertex AI Live API WebSocket Passthrough | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/vertex_ai_live_websocket",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:59.103391022-03:00",
      "description": "LiteLLM now supports WebSocket passthrough for the Vertex AI Live API, enabling real-time bidirectional communication with Gemini models.",
      "summary": "This document explains how to configure and use LiteLLM's WebSocket passthrough for the Vertex AI Live API, enabling real-time bidirectional communication and cost tracking for Gemini models.",
      "tags": [
        "vertex-ai",
        "websocket",
        "litellm",
        "gemini-live",
        "real-time-api",
        "cost-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-vertex-ai-live-websocket.md"
    },
    {
      "file_path": "151-docs-pass-through-vertex-ai-search-datastores.md",
      "title": "Vertex AI Search Datastores | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/vertex_ai_search_datastores",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:00.292658067-03:00",
      "description": "Call Vertex AI Discovery Engine Search API through LiteLLM.",
      "summary": "This document provides instructions on how to integrate and use the Vertex AI Discovery Engine Search API via LiteLLM. It covers setting up credentials, configuring managed vector stores, and performing searches through various endpoints.",
      "tags": [
        "vertex-ai",
        "discovery-engine",
        "search-api",
        "litellm-proxy",
        "vector-store",
        "google-cloud"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-vertex-ai-search-datastores.md"
    },
    {
      "file_path": "152-docs-pass-through-vertex-ai.md",
      "title": "Vertex AI SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/vertex_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:57.695642208-03:00",
      "description": "Pass-through endpoints for Vertex AI - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to use LiteLLM's pass-through endpoints to call Vertex AI services in their native format while enabling features like cost tracking and logging.",
      "tags": [
        "vertex-ai",
        "litellm-proxy",
        "pass-through",
        "gemini-api",
        "websockets",
        "google-cloud"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-vertex-ai.md"
    },
    {
      "file_path": "153-docs-pass-through-vllm.md",
      "title": "VLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/pass_through/vllm",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:02.23882353-03:00",
      "description": "Pass-through endpoints for VLLM - call provider-specific endpoint, in native format (no translation).",
      "summary": "This document explains how to configure and use LiteLLM Proxy as a pass-through for native VLLM endpoints to enable logging and access control while maintaining provider-specific API formats.",
      "tags": [
        "litellm-proxy",
        "vllm",
        "pass-through",
        "api-gateway",
        "logging",
        "virtual-keys"
      ],
      "category": "guide",
      "original_file_path": "docs-pass-through-vllm.md"
    },
    {
      "file_path": "154-docs-projects-Google-ADK.md",
      "title": "Google ADK (Agent Development Kit) | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Google%20ADK",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:14.909770776-03:00",
      "description": "Google ADK is an open-source, code-first Python framework for building, evaluating, and deploying sophisticated AI agents. While optimized for Gemini, ADK is model-agnostic and supports LiteLLM for using 100+ providers.",
      "summary": "Google ADK is an open-source Python framework for building, evaluating, and deploying AI agents with support for Gemini and multiple other model providers via LiteLLM.",
      "tags": [
        "google-adk",
        "python",
        "ai-agents",
        "llm-framework",
        "gemini",
        "litellm"
      ],
      "category": "guide",
      "original_file_path": "docs-projects-Google-ADK.md"
    },
    {
      "file_path": "155-docs-projects-GPT-Migrate.md",
      "title": "GPT Migrate | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/GPT%20Migrate",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:15.651744414-03:00",
      "description": "Easily migrate your codebase from one framework or language to another.",
      "summary": "This document provides a high-level overview of strategies and tools for migrating an existing codebase between different programming frameworks or languages.",
      "tags": [
        "code-migration",
        "software-modernization",
        "framework-transition",
        "language-migration",
        "refactoring"
      ],
      "category": "guide",
      "original_file_path": "docs-projects-GPT-Migrate.md"
    },
    {
      "file_path": "156-docs-projects-Harbor.md",
      "title": "Harbor | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Harbor",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:19.292651921-03:00",
      "description": "Harbor is a framework from the creators of Terminal-Bench for evaluating and optimizing agents and language models. It uses LiteLLM to call 100+ LLM providers.",
      "summary": "Harbor is a framework designed for evaluating, benchmarking, and optimizing language model agents across various providers using LiteLLM. It supports parallel experiment execution in the cloud and generating rollouts for reinforcement learning optimization.",
      "tags": [
        "agent-evaluation",
        "llm-benchmarking",
        "agent-optimization",
        "litellm",
        "cloud-computing",
        "reinforcement-learning"
      ],
      "category": "guide",
      "original_file_path": "docs-projects-Harbor.md"
    },
    {
      "file_path": "157-docs-projects-openai-agents.md",
      "title": "OpenAI Agents SDK | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/openai-agents",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:26.55544727-03:00",
      "description": "The OpenAI Agents SDK is a lightweight framework for building multi-agent workflows.",
      "summary": "This document explains how to use the LiteLLM extension with the OpenAI Agents SDK to integrate various model providers into multi-agent workflows.",
      "tags": [
        "openai-agents-sdk",
        "litellm",
        "multi-agent-systems",
        "python-sdk",
        "llm-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-projects-openai-agents.md"
    },
    {
      "file_path": "158-docs-provider-registration.md",
      "title": "Integrate as a Model Provider | liteLLM",
      "url": "https://docs.litellm.ai/docs/provider_registration/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:41.831433093-03:00",
      "description": "Quick Start for OpenAI-Compatible Providers",
      "summary": "This document provides a step-by-step technical guide for integrating new chat providers into the liteLLM framework by creating adapter classes and registering them across the codebase.",
      "tags": [
        "litellm",
        "provider-integration",
        "custom-llm",
        "adapter-pattern",
        "api-wrapper",
        "llm-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-provider-registration.md"
    },
    {
      "file_path": "159-docs-providers-ai21.md",
      "title": "AI21 | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/ai21",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:46.821224321-03:00",
      "description": "LiteLLM supports the following AI21 models:",
      "summary": "This document provides instructions and configuration details for using AI21 models through LiteLLM's Python SDK and Proxy Server. It covers supported model identifiers, API key setup, and how to pass standard or provider-specific parameters.",
      "tags": [
        "litellm",
        "ai21",
        "python-sdk",
        "proxy-server",
        "llm-integration",
        "model-parameters"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-ai21.md"
    },
    {
      "file_path": "160-docs-providers-aiml.md",
      "title": "AI/ML API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/aiml",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:47.932100381-03:00",
      "description": "https://aimlapi.com/",
      "summary": "This document provides instructions and code examples for integrating the AI/ML API with LiteLLM to perform text completion, image generation, and embeddings. It covers environment setup, model selection, and implementation of both synchronous and asynchronous operations.",
      "tags": [
        "aiml-api",
        "litellm-integration",
        "text-generation",
        "image-generation",
        "embeddings",
        "python-sdk",
        "async-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-aiml.md"
    },
    {
      "file_path": "161-docs-providers-amazon-nova.md",
      "title": "Amazon Nova | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/amazon_nova",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:50.117898173-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides instructions and code examples for integrating Amazon Nova foundation models with LiteLLM, covering authentication, basic usage, streaming, and tool calling.",
      "tags": [
        "amazon-nova",
        "litellm",
        "api-integration",
        "python-sdk",
        "model-comparison",
        "streaming",
        "tool-calling"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-amazon-nova.md"
    },
    {
      "file_path": "162-docs-providers-anthropic-programmatic-tool-calling.md",
      "title": "Anthropic Programmatic Tool Calling | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anthropic_programmatic_tool_calling",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:54.10356265-03:00",
      "description": "Programmatic tool calling allows Claude to write code that calls your tools programmatically within a code execution container, rather than requiring round trips through the model for each tool invocation. This reduces latency for multi-tool workflows and decreases token consumption by allowing Claude to filter or process data before it reaches the model's context window.",
      "summary": "This document explains programmatic tool calling, a feature that enables Claude to invoke tools via a sandboxed code execution container to reduce latency and token consumption.",
      "tags": [
        "programmatic-tool-calling",
        "code-execution",
        "litellm",
        "function-calling",
        "anthropic-claude",
        "latency-optimization"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-anthropic-programmatic-tool-calling.md"
    },
    {
      "file_path": "163-docs-providers-anthropic-tool-input-examples.md",
      "title": "Anthropic Tool Input Examples | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anthropic_tool_input_examples",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:54.115366857-03:00",
      "description": "Provide concrete examples of valid tool inputs to help Claude understand how to use your tools more effectively. This is particularly useful for complex tools with nested objects, optional parameters, or format-sensitive inputs.",
      "summary": "This document explains how to use the input_examples field in LiteLLM to provide Claude models with concrete examples of valid tool inputs, improving accuracy for complex parameters and nested objects.",
      "tags": [
        "litellm",
        "claude",
        "tool-use",
        "function-calling",
        "prompt-engineering",
        "input-examples",
        "anthropic"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-anthropic-tool-input-examples.md"
    },
    {
      "file_path": "164-docs-providers-anthropic-tool-search.md",
      "title": "Anthropic Tool Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anthropic_tool_search",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:56.442280753-03:00",
      "description": "Tool search enables Claude to dynamically discover and load tools on-demand from large tool catalogs (10,000+ tools). Instead of loading all tool definitions into the context window upfront, Claude searches your tool catalog and loads only the tools it needs.",
      "summary": "This document explains how to implement dynamic tool search with Claude models using LiteLLM to efficiently manage and load tools from large catalogs on-demand. It covers configuration for major cloud providers, search algorithm variants, and optimization strategies like deferred loading.",
      "tags": [
        "litellm",
        "claude-ai",
        "tool-search",
        "function-calling",
        "anthropic-api",
        "context-optimization"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-anthropic-tool-search.md"
    },
    {
      "file_path": "165-docs-providers-anyscale.md",
      "title": "Anyscale | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anyscale",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:57.711381213-03:00",
      "description": "https://app.endpoints.anyscale.com/",
      "summary": "This document provides instructions and code examples for integrating Anyscale Endpoints with LiteLLM, including API key setup, completion usage, and supported models.",
      "tags": [
        "anyscale",
        "litellm",
        "api-integration",
        "python",
        "llm",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-anyscale.md"
    },
    {
      "file_path": "166-docs-providers-aws-polly.md",
      "title": "AWS Polly Text to Speech (tts) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/aws_polly",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:01.650147039-03:00",
      "description": "Overview",
      "summary": "This document explains how to integrate AWS Polly for text-to-speech synthesis using the LiteLLM SDK and Proxy, covering engine selection, voice mappings, and SSML support.",
      "tags": [
        "aws-polly",
        "litellm",
        "text-to-speech",
        "tts",
        "ssml",
        "aws-authentication",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-aws-polly.md"
    },
    {
      "file_path": "167-docs-providers-aws-sagemaker.md",
      "title": "AWS Sagemaker | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/aws_sagemaker",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:02.470296303-03:00",
      "description": "LiteLLM supports All Sagemaker Huggingface Jumpstart Models",
      "summary": "This document provides instructions and examples for integrating LiteLLM with AWS Sagemaker Huggingface Jumpstart models, covering SDK usage, proxy configuration, and parameter customization. It explains how to handle authentication, streaming, and provider-specific configurations for Sagemaker endpoints.",
      "tags": [
        "aws-sagemaker",
        "litellm",
        "huggingface-jumpstart",
        "api-integration",
        "model-deployment",
        "python-sdk",
        "streaming",
        "configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-aws-sagemaker.md"
    },
    {
      "file_path": "168-docs-providers-azure-ai-agents.md",
      "title": "Azure AI Foundry Agents | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai_agents",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:03.657961835-03:00",
      "description": "Call Azure AI Foundry Agents in the OpenAI Request/Response format.",
      "summary": "This document provides instructions for integrating Azure AI Foundry Agents with LiteLLM, covering authentication methods, Python SDK usage, and thread management for stateful conversations.",
      "tags": [
        "azure-ai-foundry",
        "litellm",
        "agents",
        "authentication",
        "python-sdk",
        "azure-ad",
        "thread-management"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-agents.md"
    },
    {
      "file_path": "169-docs-providers-azure-ai-azure-ai-vector-stores-passthrough.md",
      "title": "Azure AI Search - Vector Store (Passthrough API) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai/azure_ai_vector_stores_passthrough",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:10.219111725-03:00",
      "description": "Use this to allow developers to create and search vector stores using the Azure AI Search API in the native Azure AI Search API format, without giving them the Azure AI credentials.",
      "summary": "This document explains how to configure and use LiteLLM as a proxy for Azure AI Search, allowing developers to manage vector stores and perform searches using virtual indexes without direct access to credentials.",
      "tags": [
        "litellm",
        "azure-ai-search",
        "vector-store",
        "proxy-server",
        "api-passthrough",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-azure-ai-vector-stores-passthrough.md"
    },
    {
      "file_path": "170-docs-providers-azure-ai-azure-model-router.md",
      "title": "Azure Model Router | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai/azure_model_router",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:11.085902874-03:00",
      "description": "Azure Model Router is a feature in Azure AI Foundry that automatically routes your requests to the best available model based on your requirements. This allows you to use a single endpoint that intelligently selects the optimal model for each request.",
      "summary": "This document explains how to integrate Azure Model Router with LiteLLM, covering configuration via the Python SDK, Proxy server, and Admin UI. It details how to set up automatic model selection, cost tracking, and streaming support for Azure AI Foundry deployments.",
      "tags": [
        "azure-model-router",
        "litellm",
        "azure-ai-foundry",
        "model-routing",
        "python-sdk",
        "api-gateway",
        "cost-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-azure-model-router.md"
    },
    {
      "file_path": "171-docs-providers-azure-ai-img-edit.md",
      "title": "Azure AI Image Editing | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai_img_edit",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:07.833933243-03:00",
      "description": "Azure AI provides powerful image editing capabilities using FLUX models from Black Forest Labs to modify existing images based on text descriptions.",
      "summary": "This document provides a comprehensive guide on using Azure AI's FLUX models for image editing via the LiteLLM SDK and Proxy Server. It covers configuration steps, model-specific details, and API parameters for modifying images based on text descriptions.",
      "tags": [
        "azure-ai",
        "flux-models",
        "image-editing",
        "litellm",
        "python-sdk",
        "api-configuration",
        "image-processing"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-img-edit.md"
    },
    {
      "file_path": "172-docs-providers-azure-ai-img.md",
      "title": "Azure AI Image Generation (Black Forest Labs - Flux) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai_img",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:06.853564813-03:00",
      "description": "Azure AI provides powerful image generation capabilities using FLUX models from Black Forest Labs to create high-quality images from text descriptions.",
      "summary": "This document provides a comprehensive guide for integrating Azure AI FLUX models for image generation and editing using the LiteLLM Python SDK and Proxy server. It includes configuration steps, supported model details, and code examples for both basic and advanced image operations.",
      "tags": [
        "azure-ai",
        "flux-models",
        "image-generation",
        "litellm",
        "image-editing",
        "python-sdk",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-img.md"
    },
    {
      "file_path": "173-docs-providers-azure-ai-speech.md",
      "title": "Azure AI Speech (Cognitive Services) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai_speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:08.898099997-03:00",
      "description": "Azure AI Speech is Azure's Cognitive Services text-to-speech API, separate from Azure OpenAI. It provides high-quality neural voices with broader language support and advanced speech customization.",
      "summary": "This document provides a comprehensive guide on integrating Azure AI Speech with LiteLLM for text-to-speech tasks, including setup, cost tracking, and voice mapping. It details how to use both the LiteLLM SDK and Proxy to access neural voices and implement advanced features like SSML.",
      "tags": [
        "azure-ai-speech",
        "litellm",
        "text-to-speech",
        "ssml",
        "api-integration",
        "cost-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-speech.md"
    },
    {
      "file_path": "174-docs-providers-azure-ai-vector-stores.md",
      "title": "Azure AI Search - Vector Store (Unified API) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai_vector_stores",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:09.709316121-03:00",
      "description": "Use this to search Azure AI Search Vector Stores, with LiteLLM's unified /chat/completions API.",
      "summary": "Explains how to perform vector searches on Azure AI Search using LiteLLM's unified API, covering configuration, search parameters, and integration with embedding models.",
      "tags": [
        "litellm",
        "azure-ai-search",
        "vector-store",
        "python-sdk",
        "semantic-search",
        "embeddings"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ai-vector-stores.md"
    },
    {
      "file_path": "175-docs-providers-azure-ai.md",
      "title": "Azure AI Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:03.610471279-03:00",
      "description": "LiteLLM supports all models on Azure AI Studio",
      "summary": "This document provides a code example for implementing tool use and function calling with LiteLLM using Azure AI hosted models.",
      "tags": [
        "litellm",
        "azure-ai",
        "function-calling",
        "tool-use",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-azure-ai.md"
    },
    {
      "file_path": "176-docs-providers-azure-azure-anthropic.md",
      "title": "Azure Anthropic (Claude via Azure Foundry) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/azure_anthropic",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:16.425605407-03:00",
      "description": "LiteLLM supports Claude models deployed via Microsoft Azure Foundry, including Claude Sonnet 4.5, Claude Haiku 4.5, and Claude Opus 4.1.",
      "summary": "This document explains how to integrate and use Anthropic Claude models deployed via Microsoft Azure Foundry using LiteLLM, covering authentication, configuration, and SDK usage.",
      "tags": [
        "litellm",
        "azure-foundry",
        "claude",
        "anthropic",
        "api-integration",
        "azure-ai",
        "authentication"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-azure-anthropic.md"
    },
    {
      "file_path": "177-docs-providers-azure-azure-embedding.md",
      "title": "Azure OpenAI Embeddings | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/azure_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:17.932308889-03:00",
      "description": "API keys",
      "summary": "This document provides instructions on how to configure and use Azure OpenAI embedding models using the LiteLLM library and its proxy server.",
      "tags": [
        "litellm",
        "azure-openai",
        "embeddings",
        "proxy-server",
        "python-sdk",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-azure-embedding.md"
    },
    {
      "file_path": "178-docs-providers-azure-document-intelligence.md",
      "title": "Azure Document Intelligence OCR | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_document_intelligence",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:12.460160385-03:00",
      "description": "Overview",
      "summary": "This document provides a comprehensive guide on integrating Azure Document Intelligence with LiteLLM for OCR and document analysis, covering SDK usage, proxy configuration, and automated polling mechanisms.",
      "tags": [
        "azure-document-intelligence",
        "litellm",
        "ocr",
        "text-extraction",
        "document-analysis",
        "python-sdk",
        "api-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-document-intelligence.md"
    },
    {
      "file_path": "179-docs-providers-azure-ocr.md",
      "title": "Azure AI OCR (Mistral) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure_ocr",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:14.264673024-03:00",
      "description": "Overview",
      "summary": "This document provides instructions and code examples for using Azure AI OCR models through LiteLLM to extract text from PDF and image files via SDK and proxy interfaces.",
      "tags": [
        "azure-ai",
        "ocr",
        "litellm",
        "mistral",
        "text-extraction",
        "document-processing",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-ocr.md"
    },
    {
      "file_path": "180-docs-providers-azure-videos.md",
      "title": "Azure Video Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:21.620167661-03:00",
      "description": "LiteLLM supports Azure OpenAI's video generation models including Sora with full end-to-end integration.",
      "summary": "This document provides instructions for integrating Azure OpenAI's video generation models, including Sora, using the LiteLLM SDK and proxy server.",
      "tags": [
        "azure-openai",
        "video-generation",
        "sora",
        "litellm-proxy",
        "python-sdk",
        "observability"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure-videos.md"
    },
    {
      "file_path": "181-docs-providers-azure.md",
      "title": "Azure OpenAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:16.074242375-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Azure OpenAI Service and Azure Foundry models with LiteLLM using the Python SDK and Proxy Server.",
      "tags": [
        "azure-openai",
        "litellm",
        "python-sdk",
        "api-integration",
        "azure-foundry",
        "chat-completions"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-azure.md"
    },
    {
      "file_path": "182-docs-providers-baseten.md",
      "title": "Baseten | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/baseten",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:23.198647646-03:00",
      "description": "LiteLLM supports both Baseten Model APIs and dedicated deployments with automatic routing.",
      "summary": "This document explains how to integrate and use Baseten Model APIs and dedicated deployments within the LiteLLM framework, including automatic routing and proxy configuration.",
      "tags": [
        "litellm",
        "baseten",
        "api-integration",
        "dedicated-deployments",
        "model-routing",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-baseten.md"
    },
    {
      "file_path": "183-docs-providers-bedrock-agentcore.md",
      "title": "Bedrock AgentCore | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_agentcore",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:25.748174586-03:00",
      "description": "Call Bedrock AgentCore in the OpenAI Request/Response format.",
      "summary": "This document explains how to integrate and call Amazon Bedrock AgentCore runtimes using the LiteLLM SDK and Proxy with OpenAI-compatible request formats.",
      "tags": [
        "amazon-bedrock",
        "agentcore",
        "litellm",
        "aws",
        "python-sdk",
        "llm-proxy",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-agentcore.md"
    },
    {
      "file_path": "184-docs-providers-bedrock-agents.md",
      "title": "Bedrock Agents | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_agents",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:26.810873714-03:00",
      "description": "Call Bedrock Agents in the OpenAI Request/Response format.",
      "summary": "This document provides instructions for calling Amazon Bedrock Agents through LiteLLM using the OpenAI request/response format via the Python SDK and Proxy.",
      "tags": [
        "amazon-bedrock",
        "bedrock-agents",
        "litellm",
        "aws",
        "openai-compatibility",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-agents.md"
    },
    {
      "file_path": "185-docs-providers-bedrock-batches.md",
      "title": "Bedrock Batches | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_batches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:27.70823701-03:00",
      "description": "Use Amazon Bedrock Batch Inference API through LiteLLM.",
      "summary": "This document provides a comprehensive guide on configuring and using LiteLLM to execute asynchronous batch inference jobs through Amazon Bedrock. It covers administrative setup including S3 bucket configuration and IAM roles, alongside developer workflows for file management and batch job execution.",
      "tags": [
        "amazon-bedrock",
        "litellm",
        "batch-inference",
        "aws-s3",
        "asynchronous-processing",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-batches.md"
    },
    {
      "file_path": "186-docs-providers-bedrock-image-gen.md",
      "title": "AWS Bedrock - Image Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_image_gen",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:29.558344329-03:00",
      "description": "Use Bedrock for image generation with Stable Diffusion, Amazon Titan Image Generator, and Amazon Nova Canvas models.",
      "summary": "This document provides instructions on using the LiteLLM library to perform image generation through AWS Bedrock, covering supported models, parameter configuration, and authentication.",
      "tags": [
        "aws-bedrock",
        "image-generation",
        "litellm",
        "stable-diffusion",
        "amazon-titan",
        "amazon-nova"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-image-gen.md"
    },
    {
      "file_path": "187-docs-providers-bedrock-imported.md",
      "title": "Bedrock Imported Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_imported",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:29.791375212-03:00",
      "description": "Bedrock Imported Models (Deepseek, Deepseek R1, Qwen, OpenAI-compatible models)",
      "summary": "This document outlines how to configure and use various AWS Bedrock imported models, including Deepseek and Qwen architectures, through the LiteLLM SDK and proxy.",
      "tags": [
        "aws-bedrock",
        "litellm",
        "deepseek",
        "qwen",
        "model-import",
        "openai-compatible"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-imported.md"
    },
    {
      "file_path": "188-docs-providers-bedrock-vector-store.md",
      "title": "Bedrock Knowledge Bases | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_vector_store",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:32.56353009-03:00",
      "description": "AWS Bedrock Knowledge Bases allows you to connect your LLM's to your organization's data, letting your models retrieve and reference information specific to your business.",
      "summary": "This document explains how to integrate and use AWS Bedrock Knowledge Bases as a vector store within LiteLLM for retrieval-augmented generation.",
      "tags": [
        "aws-bedrock",
        "knowledge-bases",
        "litellm",
        "vector-store",
        "rag",
        "file-search"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-vector-store.md"
    },
    {
      "file_path": "189-docs-providers-bedrock-writer.md",
      "title": "Bedrock - Writer Palmyra | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_writer",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:33.330321565-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Writer Palmyra foundation models from Amazon Bedrock using LiteLLM, covering chat completions, tool calling, and PDF document processing.",
      "tags": [
        "amazon-bedrock",
        "litellm",
        "writer-palmyra",
        "tool-calling",
        "document-processing",
        "llm-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock-writer.md"
    },
    {
      "file_path": "190-docs-providers-bedrock.md",
      "title": "AWS Bedrock | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:23.997917424-03:00",
      "description": "ALL Bedrock models (Anthropic, Meta, Deepseek, Mistral, Amazon, etc.) are Supported",
      "summary": "This document provides a comprehensive guide for integrating Amazon Bedrock foundation models using the LiteLLM library, covering authentication, SDK usage, and advanced features like tool calling and vision.",
      "tags": [
        "amazon-bedrock",
        "litellm",
        "aws-sdk",
        "llm-integration",
        "tool-calling",
        "vision-models",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bedrock.md"
    },
    {
      "file_path": "191-docs-providers-bytez.md",
      "title": "Bytez | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bytez",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:36.773469607-03:00",
      "description": "LiteLLM supports all chat models on Bytez!",
      "summary": "This document provides instructions for integrating LiteLLM with Bytez models, covering SDK and proxy configurations for chat and multi-modal tasks.",
      "tags": [
        "litellm",
        "bytez",
        "multi-modal",
        "python-sdk",
        "api-integration",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-bytez.md"
    },
    {
      "file_path": "192-docs-providers-cerebras.md",
      "title": "Cerebras | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/cerebras",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:37.135524858-03:00",
      "description": "https://inference-docs.cerebras.ai/api-reference/chat-completions",
      "summary": "This document provides examples of using LiteLLM to perform synchronous and streaming completions with Cerebras models, including how to request JSON-formatted outputs.",
      "tags": [
        "litellm",
        "cerebras",
        "python",
        "json-mode",
        "streaming",
        "llm-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-cerebras.md"
    },
    {
      "file_path": "193-docs-providers-chatgpt.md",
      "title": "ChatGPT Subscription | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/chatgpt",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:39.01181084-03:00",
      "description": "Use ChatGPT Pro/Max subscription models through LiteLLM with OAuth device flow authentication.",
      "summary": "This document explains how to integrate ChatGPT Pro/Max subscription models with LiteLLM using OAuth device flow authentication and describes supported API endpoints and configuration settings.",
      "tags": [
        "litellm",
        "chatgpt-api",
        "oauth-device-flow",
        "python-sdk",
        "llm-proxy",
        "authentication"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-chatgpt.md"
    },
    {
      "file_path": "194-docs-providers-chutes.md",
      "title": "Chutes | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/chutes",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:39.505696436-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating the Chutes AI deployment platform with LiteLLM, covering API configuration, supported frameworks like vLLM and SGLang, and hardware optimization for scaling LLM applications.",
      "tags": [
        "chutes",
        "litellm",
        "ai-deployment",
        "vllm",
        "sglang",
        "openai-compatible",
        "llm-scaling"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-chutes.md"
    },
    {
      "file_path": "195-docs-providers-clarifai.md",
      "title": "Clarifai | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/clarifai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:39.63495613-03:00",
      "description": "Anthropic, OpenAI, Qwen, xAI, Gemini and most of Open soured LLMs are Supported on Clarifai.",
      "summary": "This document demonstrates how to use the LiteLLM library to integrate various Clarifai-hosted large language models into applications, covering basic completion, streaming, and tool calling.",
      "tags": [
        "clarifai",
        "litellm",
        "llm-integration",
        "streaming",
        "tool-calling",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-clarifai.md"
    },
    {
      "file_path": "196-docs-providers-cloudflare-workers.md",
      "title": "Cloudflare Workers AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/cloudflare_workers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:43.666115279-03:00",
      "description": "https://developers.cloudflare.com/workers-ai/models/text-generation/",
      "summary": "This document provides instructions and code samples for integrating Cloudflare Workers AI text generation models using the LiteLLM library, covering environment setup and both standard and streaming usage.",
      "tags": [
        "cloudflare-workers-ai",
        "litellm",
        "text-generation",
        "python",
        "llm-streaming",
        "api-configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-cloudflare-workers.md"
    },
    {
      "file_path": "197-docs-providers-cohere.md",
      "title": "Cohere | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/cohere",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:45.552183344-03:00",
      "description": "API KEYS",
      "summary": "This document provides instructions and code examples for integrating Cohere's chat, embedding, and reranking models using the LiteLLM Python SDK and Proxy server.",
      "tags": [
        "litellm",
        "cohere",
        "python-sdk",
        "api-integration",
        "embeddings",
        "rerank",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-cohere.md"
    },
    {
      "file_path": "198-docs-providers-cometapi.md",
      "title": "CometAPI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/cometapi",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:45.738930107-03:00",
      "description": "LiteLLM supports all AI models from CometAPI. CometAPI provides access to 500+ AI models through a unified API interface, including cutting-edge models like GPT-5, Claude Opus 4.1, and various other state-of-the-art language models.",
      "summary": "This document explains how to integrate LiteLLM with CometAPI to access over 500 AI models through a unified interface. It provides comprehensive examples for authentication, synchronous and asynchronous completions, streaming, and error handling.",
      "tags": [
        "litellm",
        "cometapi",
        "llm-integration",
        "python-sdk",
        "streaming-api",
        "async-completion"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-cometapi.md"
    },
    {
      "file_path": "199-docs-providers-custom-llm-server.md",
      "title": "Custom API Server (Custom Format) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/custom_llm_server",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:48.733410238-03:00",
      "description": "Call your custom torch-serve / internal LLM APIs via LiteLLM",
      "summary": "This document provides instructions on integrating custom LLM providers and internal APIs into LiteLLM by extending the CustomLLM class and configuring the custom provider map. It covers implementation details for chat completions, streaming, and image generation routes within both local Python environments and the LiteLLM proxy.",
      "tags": [
        "litellm",
        "custom-llm",
        "api-integration",
        "python",
        "llm-proxy",
        "custom-provider"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-custom-llm-server.md"
    },
    {
      "file_path": "200-docs-providers-dashscope.md",
      "title": "Dashscope (Qwen API) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/dashscope",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:48.912481728-03:00",
      "description": "https://dashscope.console.aliyun.com/",
      "summary": "This document provides instructions for integrating Alibaba DashScope Qwen models with LiteLLM, including API key configuration and code examples for standard and streaming completions.",
      "tags": [
        "dashscope",
        "qwen",
        "litellm",
        "api-integration",
        "python",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-dashscope.md"
    },
    {
      "file_path": "201-docs-providers-databricks.md",
      "title": "Databricks | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/databricks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:50.748595358-03:00",
      "description": "LiteLLM supports all models on Databricks",
      "summary": "This document explains how to integrate and use models hosted on Databricks with LiteLLM, covering various authentication methods, configuration settings, and advanced features like reasoning content.",
      "tags": [
        "databricks",
        "litellm",
        "authentication",
        "oauth-m2m",
        "python-sdk",
        "reasoning-content",
        "model-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-databricks.md"
    },
    {
      "file_path": "202-docs-providers-datarobot.md",
      "title": "DataRobot | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/datarobot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:51.815390536-03:00",
      "description": "LiteLLM supports all models from DataRobot. Select datarobot as the provider to route your request through the datarobot OpenAI-compatible endpoint using the upstream official OpenAI Python API library.",
      "summary": "This document provides instructions and code examples for integrating LiteLLM with DataRobot models via an OpenAI-compatible endpoint.",
      "tags": [
        "litellm",
        "datarobot",
        "llm-gateway",
        "python-sdk",
        "model-routing",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-datarobot.md"
    },
    {
      "file_path": "203-docs-providers-deepgram.md",
      "title": "Deepgram | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/deepgram",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:54.288500421-03:00",
      "description": "LiteLLM supports Deepgram's /listen endpoint.",
      "summary": "This document provides instructions for integrating Deepgram's speech-to-text services using LiteLLM, covering both Python SDK usage and proxy configuration.",
      "tags": [
        "litellm",
        "deepgram",
        "audio-transcription",
        "speech-to-text",
        "api-integration",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-deepgram.md"
    },
    {
      "file_path": "204-docs-providers-deepinfra.md",
      "title": "DeepInfra | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/deepinfra",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:54.850617052-03:00",
      "description": "https://deepinfra.com/",
      "summary": "This document provides instructions and code examples for integrating DeepInfra's chat and rerank models using the LiteLLM library, covering configuration, streaming, and supported API parameters.",
      "tags": [
        "deepinfra",
        "litellm",
        "llm-integration",
        "rerank-api",
        "chat-completion",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-deepinfra.md"
    },
    {
      "file_path": "205-docs-providers-deepseek.md",
      "title": "Deepseek | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/deepseek",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:55.125522429-03:00",
      "description": "https://deepseek.com/",
      "summary": "This document provides instructions and code samples for integrating Deepseek AI models via LiteLLM, including support for chat, coder, and reasoning models with specialized thinking modes.",
      "tags": [
        "deepseek",
        "litellm",
        "python",
        "llm-api",
        "streaming",
        "reasoning-models"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-deepseek.md"
    },
    {
      "file_path": "206-docs-providers-docker-model-runner.md",
      "title": "Docker Model Runner | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/docker_model_runner",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:55.790588512-03:00",
      "description": "Overview",
      "summary": "This document provides instructions on how to integrate and use LiteLLM with Docker Model Runner for running local large language models. It covers installation, environment variable configuration, and implementation using both the Python SDK and LiteLLM Proxy.",
      "tags": [
        "docker-model-runner",
        "litellm",
        "local-llm",
        "docker-desktop",
        "python-sdk",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-docker-model-runner.md"
    },
    {
      "file_path": "207-docs-providers-elevenlabs.md",
      "title": "ElevenLabs | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/elevenlabs",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:56.077942777-03:00",
      "description": "ElevenLabs provides high-quality AI voice technology, including speech-to-text capabilities through their transcription API.",
      "summary": "This document provides instructions for integrating ElevenLabs speech-to-text and text-to-speech services using the LiteLLM Python SDK and proxy server.",
      "tags": [
        "elevenlabs",
        "litellm",
        "speech-to-text",
        "text-to-speech",
        "audio-transcription",
        "voice-mapping"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-elevenlabs.md"
    },
    {
      "file_path": "208-docs-providers-empower.md",
      "title": "Empower | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/empower",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:59.207766116-03:00",
      "description": "LiteLLM supports all models on Empower.",
      "summary": "This document provides instructions and code examples for integrating Empower models with LiteLLM, covering standard completions, streaming, and function calling.",
      "tags": [
        "litellm",
        "empower",
        "python-sdk",
        "llm-integration",
        "function-calling",
        "streaming-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-empower.md"
    },
    {
      "file_path": "209-docs-providers-fal-ai.md",
      "title": "Fal AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/fal_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:00.540460534-03:00",
      "description": "Fal AI provides fast, scalable access to state-of-the-art image generation models including FLUX, Stable Diffusion, Imagen, and more.",
      "summary": "This document explains how to integrate Fal AI image generation models with LiteLLM using both the Python SDK and Proxy Server. It provides instructions for setup, model selection, and passing model-specific parameters for high-performance image generation.",
      "tags": [
        "fal-ai",
        "litellm",
        "image-generation",
        "text-to-image",
        "python-sdk",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-fal-ai.md"
    },
    {
      "file_path": "210-docs-providers-featherless-ai.md",
      "title": "Featherless AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/featherless_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:02.688853776-03:00",
      "description": "https://featherless.ai/",
      "summary": "This document provides instructions and code samples for integrating Featherless AI models with LiteLLM, including API key configuration and streaming usage.",
      "tags": [
        "litellm",
        "featherless-ai",
        "python-sdk",
        "chat-completion",
        "api-integration",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-featherless-ai.md"
    },
    {
      "file_path": "211-docs-providers-fireworks-ai.md",
      "title": "Fireworks AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/fireworks_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:02.742994333-03:00",
      "description": "We support ALL Fireworks AI models, just set fireworks_ai/ as a prefix when sending completion requests",
      "summary": "This document provides instructions on how to integrate LiteLLM with Fireworks AI for chat completions, embeddings, and audio transcriptions. It covers serverless connections, direct-route deployments, and configuration settings for the LiteLLM proxy and advanced features like document inlining.",
      "tags": [
        "litellm",
        "fireworks-ai",
        "llm-integration",
        "api-configuration",
        "serverless-models",
        "embeddings",
        "proxy-server",
        "document-inlining"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-fireworks-ai.md"
    },
    {
      "file_path": "212-docs-providers-friendliai.md",
      "title": "FriendliAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/friendliai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:04.54036515-03:00",
      "description": "We support ALL FriendliAI models, just set friendliai/ as a prefix when sending completion requests",
      "summary": "This document provides instructions and code examples for integrating FriendliAI models with LiteLLM, covering authentication, model naming conventions, and both standard and streaming completion requests.",
      "tags": [
        "friendliai",
        "litellm-integration",
        "llm-inference",
        "api-configuration",
        "python-sdk",
        "streaming-completions"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-friendliai.md"
    },
    {
      "file_path": "213-docs-providers-galadriel.md",
      "title": "Galadriel | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/galadriel",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:05.132134538-03:00",
      "description": "https://docs.galadriel.com/api-reference/chat-completion-API",
      "summary": "This document explains how to integrate Galadriel AI models with LiteLLM, including environment variable setup and model naming conventions.",
      "tags": [
        "litellm",
        "galadriel",
        "model-integration",
        "python-sdk",
        "api-key"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-galadriel.md"
    },
    {
      "file_path": "214-docs-providers-gemini-file-search.md",
      "title": "Gemini File Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/gemini_file_search",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:10.07154157-03:00",
      "description": "Use Google Gemini's File Search for Retrieval Augmented Generation (RAG) with LiteLLM.",
      "summary": "This document explains how to use LiteLLM to interface with Google Gemini's File Search for document ingestion, indexing, and Retrieval Augmented Generation (RAG).",
      "tags": [
        "litellm",
        "google-gemini",
        "rag",
        "vector-store",
        "file-search",
        "document-indexing",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-gemini-file-search.md"
    },
    {
      "file_path": "215-docs-providers-gemini-videos.md",
      "title": "Gemini Video Generation (Veo) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/gemini/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:10.251431349-03:00",
      "description": "LiteLLM supports Google's Veo video generation models through a unified API interface.",
      "summary": "This document provides a comprehensive guide on using Google's Veo video generation models via LiteLLM, including implementation details for generation, status tracking, and content retrieval.",
      "tags": [
        "litellm",
        "google-veo",
        "video-generation",
        "python-sdk",
        "api-integration",
        "gemini-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-gemini-videos.md"
    },
    {
      "file_path": "216-docs-providers-gemini.md",
      "title": "Gemini - Google AI Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/gemini",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:06.957763268-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides technical instructions for integrating Google AI Studio's Gemini models using LiteLLM, detailing authentication, parameter mapping for reasoning, and cost optimization.",
      "tags": [
        "google-ai-studio",
        "gemini-api",
        "litellm",
        "reasoning-content",
        "text-to-speech",
        "model-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-gemini.md"
    },
    {
      "file_path": "217-docs-providers-github-copilot.md",
      "title": "GitHub Copilot | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/github_copilot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:14.275920672-03:00",
      "description": "https://docs.github.com/en/copilot",
      "summary": "This document provides instructions for integrating GitHub Copilot with LiteLLM, covering authentication via OAuth device flow, SDK usage for chat and embeddings, and proxy configuration.",
      "tags": [
        "github-copilot",
        "litellm",
        "authentication",
        "python-sdk",
        "chat-completion",
        "embeddings",
        "litellm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-github-copilot.md"
    },
    {
      "file_path": "218-docs-providers-github.md",
      "title": "Github | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/github",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:13.405824566-03:00",
      "description": "https://github.com/marketplace/models",
      "summary": "This document explains how to integrate GitHub-hosted models with LiteLLM, demonstrating basic completion, streaming, and tool-calling implementations.",
      "tags": [
        "litellm",
        "github-models",
        "function-calling",
        "streaming",
        "python",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-github.md"
    },
    {
      "file_path": "219-docs-providers-google-ai-studio-files.md",
      "title": "[BETA] Google AI Studio (Gemini) Files API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/google_ai_studio/files",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:15.46696212-03:00",
      "description": "Use this to upload files to Google AI Studio (Gemini).",
      "summary": "This document demonstrates how to upload audio files and perform multimodal completions using the LiteLLM library with the Gemini API.",
      "tags": [
        "litellm",
        "gemini-api",
        "audio-processing",
        "multimodal",
        "python-sdk",
        "file-upload"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-google-ai-studio-files.md"
    },
    {
      "file_path": "220-docs-providers-google-ai-studio-image-gen.md",
      "title": "Google AI Studio Image Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/google_ai_studio/image_gen",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:16.945985725-03:00",
      "description": "Google AI Studio provides powerful image generation capabilities using Google's Imagen models to create high-quality images from text descriptions.",
      "summary": "This document explains how to integrate and use Google AI Studio's Imagen models for image generation through the LiteLLM library and proxy server. It covers API configuration, environment setup, and code examples for both the LiteLLM and OpenAI Python SDKs.",
      "tags": [
        "google-ai-studio",
        "imagen",
        "litellm",
        "image-generation",
        "text-to-image",
        "api-setup",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-google-ai-studio-image-gen.md"
    },
    {
      "file_path": "221-docs-providers-google-ai-studio-realtime.md",
      "title": "Gemini Realtime API - Google AI Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/google_ai_studio/realtime",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:17.220946305-03:00",
      "description": "| Feature | Description | Comments |",
      "summary": "This document provides a code example demonstrating how to connect to the LiteLLM Realtime API using WebSockets in a Node.js environment.",
      "tags": [
        "litellm",
        "websockets",
        "node-js",
        "realtime-api",
        "openai-compatible"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-google-ai-studio-realtime.md"
    },
    {
      "file_path": "222-docs-providers-gradient-ai.md",
      "title": "GradientAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/gradient_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:18.783191878-03:00",
      "description": "https://digitalocean.com/products/gradientai",
      "summary": "This document explains how to integrate and use GradientAI models within the LiteLLM framework, covering API key setup and basic request patterns.",
      "tags": [
        "litellm",
        "gradient-ai",
        "python-sdk",
        "llm-integration",
        "streaming-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-gradient-ai.md"
    },
    {
      "file_path": "223-docs-providers-groq.md",
      "title": "Groq | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/groq",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:18.842337809-03:00",
      "description": "https://groq.com/",
      "summary": "This document provides a comprehensive guide on integrating Groq models with LiteLLM, including setup for API keys, streaming, proxy configuration, function calling, vision, and speech-to-text.",
      "tags": [
        "litellm",
        "groq",
        "api-integration",
        "llama-models",
        "function-calling",
        "streaming",
        "vision-api",
        "whisper-v3"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-groq.md"
    },
    {
      "file_path": "224-docs-providers-helicone.md",
      "title": "Helicone | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/helicone",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:21.598470994-03:00",
      "description": "Overview",
      "summary": "This document provides instructions and code examples for integrating the Helicone AI gateway with LiteLLM to enable observability, caching, and cost tracking.",
      "tags": [
        "helicone",
        "litellm",
        "ai-gateway",
        "observability",
        "llm-monitoring",
        "caching",
        "request-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-helicone.md"
    },
    {
      "file_path": "225-docs-providers-heroku.md",
      "title": "Heroku | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/heroku",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:22.549828246-03:00",
      "description": "Provision a Model",
      "summary": "This document explains how to provision and integrate Heroku-hosted AI models with LiteLLM using specific environment variables and configuration settings.",
      "tags": [
        "heroku",
        "litellm",
        "model-provisioning",
        "environment-variables",
        "claude-models",
        "inference-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-heroku.md"
    },
    {
      "file_path": "226-docs-providers-huggingface-rerank.md",
      "title": "HuggingFace Rerank | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/huggingface_rerank",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:23.064906226-03:00",
      "description": "HuggingFace Rerank allows you to use reranking models hosted on Hugging Face infrastructure or your custom endpoints to reorder documents based on their relevance to a query.",
      "summary": "This document explains how to integrate and use HuggingFace reranking models via LiteLLM, including instructions for the Python SDK, async implementation, and proxy server configuration.",
      "tags": [
        "huggingface",
        "rerank",
        "litellm",
        "python-sdk",
        "api-proxy",
        "semantic-search"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-huggingface-rerank.md"
    },
    {
      "file_path": "227-docs-providers-huggingface.md",
      "title": "Hugging Face | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/huggingface",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:22.709240734-03:00",
      "description": "LiteLLM supports running inference across multiple services for models hosted on the Hugging Face Hub.",
      "summary": "This document explains how to access Hugging Face models through various inference providers using LiteLLM and a single Hugging Face token. It details the model naming convention and demonstrates features like streaming, vision capabilities, and tool calling.",
      "tags": [
        "litellm",
        "huggingface",
        "inference-api",
        "multi-provider",
        "chat-completion",
        "vision-models",
        "function-calling",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-huggingface.md"
    },
    {
      "file_path": "228-docs-providers-hyperbolic.md",
      "title": "Hyperbolic | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/hyperbolic",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:24.719786516-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Hyperbolic's AI models with LiteLLM, covering configuration, supported models like DeepSeek and Qwen, and usage via the Python SDK or Proxy.",
      "tags": [
        "hyperbolic-ai",
        "litellm",
        "llm-api",
        "python-sdk",
        "api-integration",
        "function-calling",
        "deepseek"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-hyperbolic.md"
    },
    {
      "file_path": "229-docs-providers-infinity.md",
      "title": "Infinity | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/infinity",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:27.276989192-03:00",
      "description": "| Property                  | Details                                                                                                    |",
      "summary": "This document explains how to integrate and use the Infinity text-embedding and reranking API through the LiteLLM Python SDK and Proxy server.",
      "tags": [
        "infinity",
        "litellm",
        "embeddings",
        "reranking",
        "python-sdk",
        "api-integration",
        "rest-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-infinity.md"
    },
    {
      "file_path": "230-docs-providers-jina-ai.md",
      "title": "Jina AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/jina_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:27.507868156-03:00",
      "description": "https://jina.ai/embeddings/",
      "summary": "This document provides a code example for using the LiteLLM rerank function to order documents by relevance using the Jina AI reranker model.",
      "tags": [
        "litellm",
        "rerank",
        "jina-ai",
        "python-sdk",
        "search-optimization"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-jina-ai.md"
    },
    {
      "file_path": "231-docs-providers-langgraph.md",
      "title": "LangGraph | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/langgraph",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:31.554487428-03:00",
      "description": "Call LangGraph agents through LiteLLM using the OpenAI chat completions format.",
      "summary": "This document explains how to integrate LangGraph agents with LiteLLM, allowing users to call agents through the OpenAI chat completions format using Python, the LiteLLM Proxy, or the A2A Gateway UI.",
      "tags": [
        "langgraph",
        "litellm",
        "openai-format",
        "agent-integration",
        "python-sdk",
        "llm-proxy",
        "a2a-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-langgraph.md"
    },
    {
      "file_path": "232-docs-providers-lemonade.md",
      "title": "Lemonade | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/lemonade",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:33.214492314-03:00",
      "description": "Lemonade Server is an OpenAI-compatible local language model inference provider optimized for AMD GPUs and NPUs. The lemonade litellm provider supports standard chat completions with full OpenAI API compatibility.",
      "summary": "This document provides a guide for integrating Lemonade Server with LiteLLM, detailing how to configure local inference on AMD hardware using OpenAI-compatible APIs.",
      "tags": [
        "lemonade-server",
        "litellm",
        "openai-compatible",
        "amd-gpu",
        "inference-engine",
        "python-sdk",
        "function-calling"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-lemonade.md"
    },
    {
      "file_path": "233-docs-providers-litellm-proxy.md",
      "title": "LiteLLM Proxy (LLM Gateway) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/litellm_proxy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:33.465840994-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides instructions and code examples for routing LLM requests through the LiteLLM Proxy using an OpenAI-compatible gateway. It covers environment configuration, supported endpoints like completions and embeddings, and global proxy routing settings.",
      "tags": [
        "litellm",
        "proxy-gateway",
        "llm-api",
        "openai-compatibility",
        "embeddings",
        "image-generation",
        "api-routing"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-litellm-proxy.md"
    },
    {
      "file_path": "234-docs-providers-llamafile.md",
      "title": "Llamafile | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/llamafile",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:33.888737767-03:00",
      "description": "LiteLLM supports all models on Llamafile.",
      "summary": "This document explains how to integrate and use llamafile with LiteLLM for chat completions and embeddings via direct SDK calls or the LiteLLM Proxy Server.",
      "tags": [
        "litellm",
        "llamafile",
        "openai-compatible",
        "chat-completions",
        "embeddings",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-llamafile.md"
    },
    {
      "file_path": "235-docs-providers-lm-studio.md",
      "title": "LM Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/lm_studio",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:35.923181728-03:00",
      "description": "https://lmstudio.ai/docs/basics/server",
      "summary": "This document provides instructions for integrating LM Studio models with LiteLLM, covering configuration, chat completions, streaming, embeddings, and structured outputs.",
      "tags": [
        "lm-studio",
        "litellm",
        "local-llm",
        "python-sdk",
        "embeddings",
        "structured-outputs",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-lm-studio.md"
    },
    {
      "file_path": "236-docs-providers-manus.md",
      "title": "Manus | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/manus",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:37.255404468-03:00",
      "description": "Use Manus AI agents through LiteLLM's OpenAI-compatible Responses API.",
      "summary": "This document demonstrates how to manage files and use them with the Manus provider through the LiteLLM library, covering file upload, retrieval, deletion, and integration with the responses API.",
      "tags": [
        "litellm",
        "manus-ai",
        "file-management",
        "python",
        "async-api",
        "agentic-ai"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-manus.md"
    },
    {
      "file_path": "237-docs-providers-meta-llama.md",
      "title": "Meta Llama | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/meta_llama",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:38.741929382-03:00",
      "description": "| Property | Details |",
      "summary": "This document demonstrates how to implement tool use and function calling with Meta Llama models using the LiteLLM library.",
      "tags": [
        "litellm",
        "meta-llama",
        "function-calling",
        "tool-use",
        "python-sdk",
        "llm-api"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-meta-llama.md"
    },
    {
      "file_path": "238-docs-providers-minimax.md",
      "title": "MiniMax | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/minimax",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:41.817246086-03:00",
      "description": "Overview",
      "summary": "This document explains how to integrate MiniMax language models using LiteLLM, detailing support for both Anthropic and OpenAI-compatible API specifications, including tool calling and thinking features.",
      "tags": [
        "minimax",
        "litellm",
        "anthropic-api",
        "openai-api",
        "tool-calling",
        "api-integration",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-minimax.md"
    },
    {
      "file_path": "239-docs-providers-mistral.md",
      "title": "Mistral AI API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/mistral",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:41.848239533-03:00",
      "description": "https://docs.mistral.ai/api/",
      "summary": "This document provides code examples and instructions for integrating Mistral AI models with LiteLLM, covering chat completions, streaming, function calling, embeddings, and reasoning parameters.",
      "tags": [
        "litellm",
        "mistral-ai",
        "python",
        "function-calling",
        "embeddings",
        "llm-reasoning",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-mistral.md"
    },
    {
      "file_path": "240-docs-providers-moonshot.md",
      "title": "Moonshot AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/moonshot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:43.740541068-03:00",
      "description": "Overview",
      "summary": "This document explains how to integrate and use Moonshot AI models with LiteLLM through the Python SDK and proxy, including configuration steps and automated limitation handling.",
      "tags": [
        "moonshot-ai",
        "litellm",
        "python-sdk",
        "api-integration",
        "llm-proxy",
        "streaming"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-moonshot.md"
    },
    {
      "file_path": "241-docs-providers-morph.md",
      "title": "Morph | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/morph",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:43.740217475-03:00",
      "description": "LiteLLM supports all models on Morph",
      "summary": "This document explains how to integrate and use Morph AI models with LiteLLM, covering environment setup, basic and streaming usage, and proxy server configuration.",
      "tags": [
        "litellm",
        "morph-ai",
        "api-integration",
        "python-sdk",
        "llm-proxy",
        "model-deployment"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-morph.md"
    },
    {
      "file_path": "242-docs-providers-nano-gpt.md",
      "title": "NanoGPT | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nano-gpt",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:47.685350912-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating the NanoGPT model provider with LiteLLM, covering environment setup, Python SDK usage, and proxy server configuration.",
      "tags": [
        "nanogpt",
        "litellm",
        "openai-compatible",
        "api-integration",
        "python-sdk",
        "llm-provider"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-nano-gpt.md"
    },
    {
      "file_path": "243-docs-providers-nebius.md",
      "title": "Nebius AI Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nebius",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:48.331400968-03:00",
      "description": "https://docs.nebius.com/studio/inference/quickstart",
      "summary": "This document provides code examples for performing standard and streaming chat completions using the LiteLLM library with the Nebius AI Studio provider.",
      "tags": [
        "litellm",
        "nebius-ai-studio",
        "python",
        "chat-completion",
        "streaming-api",
        "ai-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-nebius.md"
    },
    {
      "file_path": "244-docs-providers-nlp-cloud.md",
      "title": "NLP Cloud | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nlp_cloud",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:49.059291066-03:00",
      "description": "LiteLLM supports all LLMs on NLP Cloud.",
      "summary": "This document provides instructions on how to integrate and use LiteLLM with NLP Cloud, covering API key configuration, basic usage, streaming, and custom model mapping.",
      "tags": [
        "litellm",
        "nlp-cloud",
        "python-sdk",
        "api-integration",
        "streaming",
        "llm-provider"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-nlp-cloud.md"
    },
    {
      "file_path": "245-docs-providers-novita.md",
      "title": "Novita AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/novita",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:50.357158596-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides a code example for implementing tool calling with Novita AI models using the LiteLLM library.",
      "tags": [
        "litellm",
        "novita-ai",
        "tool-calling",
        "function-calling",
        "python",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-novita.md"
    },
    {
      "file_path": "246-docs-providers-nscale.md",
      "title": "Nscale (EU Sovereign) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nscale",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:50.691331723-03:00",
      "description": "https://docs.nscale.com/docs/inference/chat",
      "summary": "This document provides instructions for integrating Nscale's AI inference services with LiteLLM to perform text and image generation using the Python SDK or Proxy server.",
      "tags": [
        "nscale",
        "litellm",
        "text-generation",
        "image-generation",
        "api-integration",
        "serverless-inference",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-nscale.md"
    },
    {
      "file_path": "247-docs-providers-nvidia-nim-rerank.md",
      "title": "Nvidia NIM - Rerank | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nvidia_nim_rerank",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:54.214753388-03:00",
      "description": "Use Nvidia NIM Rerank models through LiteLLM.",
      "summary": "This document provides instructions for integrating Nvidia NIM Rerank models with LiteLLM using the Python SDK and Proxy server. It explains how to configure endpoints, handle model-specific prefixes, and use parameters for semantic search and RAG optimization.",
      "tags": [
        "nvidia-nim",
        "litellm",
        "rerank-api",
        "semantic-search",
        "python-sdk",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-nvidia-nim-rerank.md"
    },
    {
      "file_path": "248-docs-providers-oci.md",
      "title": "Oracle Cloud Infrastructure (OCI) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/oci",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:55.399684484-03:00",
      "description": "LiteLLM supports the following models for OCI on-demand GenAI API.",
      "summary": "This document provides instructions on integrating LiteLLM with Oracle Cloud Infrastructure (OCI) GenAI services, covering supported models, authentication methods, and configuration for on-demand or dedicated endpoints.",
      "tags": [
        "litellm",
        "oracle-cloud",
        "oci-genai",
        "llm-integration",
        "api-configuration",
        "authentication"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-oci.md"
    },
    {
      "file_path": "249-docs-providers-ollama.md",
      "title": "Ollama | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/ollama",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:55.58591666-03:00",
      "description": "LiteLLM supports all models from Ollama",
      "summary": "This document provides a guide for using LiteLLM to interface with Ollama models, detailing setup, streaming, tool calling, and vision capabilities.",
      "tags": [
        "litellm",
        "ollama",
        "python-sdk",
        "llm-integration",
        "streaming",
        "tool-calling",
        "vision-models",
        "json-mode"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-ollama.md"
    },
    {
      "file_path": "250-docs-providers-openai-compatible.md",
      "title": "OpenAI-Compatible Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openai_compatible",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:59.267448724-03:00",
      "description": "Selecting openai as the provider routes your request to an OpenAI-compatible endpoint using the upstream",
      "summary": "This document explains how to configure and use LiteLLM to route requests to OpenAI-compatible endpoints using the official OpenAI Python library. It covers model prefixing, API base configuration, and setting up the LiteLLM Proxy Server for these providers.",
      "tags": [
        "litellm",
        "openai-compatible",
        "llm-proxy",
        "api-configuration",
        "text-completion",
        "embeddings"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-openai-compatible.md"
    },
    {
      "file_path": "251-docs-providers-openai-responses-api.md",
      "title": "OpenAI - Response API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openai/responses_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:00.921768183-03:00",
      "description": "Usage",
      "summary": "This document provides instructions and code examples for using the LiteLLM Python SDK and Proxy server to manage AI model responses, covering streaming, image generation, and response lifecycle operations.",
      "tags": [
        "litellm-python-sdk",
        "litellm-proxy",
        "openai-sdk",
        "streaming",
        "image-generation",
        "api-integration",
        "computer-use"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-openai-responses-api.md"
    },
    {
      "file_path": "252-docs-providers-openai-text-to-speech.md",
      "title": "OpenAI - Text-to-speech | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openai/text_to_speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:01.105985581-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for implementing text-to-speech and audio transcription services using the LiteLLM Python SDK and Proxy server. It covers synchronous and asynchronous usage, supported models, and enterprise configuration options like file size limits.",
      "tags": [
        "litellm",
        "text-to-speech",
        "python-sdk",
        "audio-transcription",
        "openai-compatible",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-openai-text-to-speech.md"
    },
    {
      "file_path": "253-docs-providers-openai-videos.md",
      "title": "OpenAI Video Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openai/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:01.209104722-03:00",
      "description": "LiteLLM supports OpenAI's video generation models including Sora.",
      "summary": "This document provides a comprehensive guide on using LiteLLM to interface with OpenAI's video generation models, covering SDK usage, proxy configuration, and API endpoints for generating, editing, and retrieving videos.",
      "tags": [
        "litellm",
        "openai-sora",
        "video-generation",
        "api-proxy",
        "python-sdk",
        "video-editing",
        "rest-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-openai-videos.md"
    },
    {
      "file_path": "254-docs-providers-openai.md",
      "title": "OpenAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:56.462782058-03:00",
      "description": "LiteLLM supports OpenAI Chat + Embedding calls.",
      "summary": "This document provides instructions and code examples for integrating OpenAI chat, vision, and embedding models through LiteLLM, including proxy server configuration and PDF parsing.",
      "tags": [
        "litellm",
        "openai",
        "chat-completion",
        "python-sdk",
        "proxy-server",
        "vision-models",
        "pdf-parsing"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-openai.md"
    },
    {
      "file_path": "255-docs-providers-openrouter.md",
      "title": "OpenRouter | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/openrouter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:03.414699558-03:00",
      "description": "LiteLLM supports all the text / chat / vision / embedding models from OpenRouter",
      "summary": "This document provides instructions and code examples for integrating LiteLLM with OpenRouter to perform text completion, embeddings, and image generation across various models.",
      "tags": [
        "litellm",
        "openrouter",
        "text-completion",
        "image-generation",
        "embeddings",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-openrouter.md"
    },
    {
      "file_path": "256-docs-providers-ovhcloud.md",
      "title": "ðŸ†• OVHCloud AI Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/ovhcloud",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:04.352679117-03:00",
      "description": "Leading French Cloud provider in Europe with data sovereignty and privacy.",
      "summary": "This document provides instructions and code samples for integrating OVHCloud AI Endpoints with LiteLLM, covering features such as chat completion, vision, embeddings, and audio transcription.",
      "tags": [
        "ovhcloud",
        "litellm",
        "ai-endpoints",
        "python",
        "chat-completion",
        "embeddings",
        "transcription",
        "tool-calling"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-ovhcloud.md"
    },
    {
      "file_path": "257-docs-providers-petals.md",
      "title": "Petals | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/petals",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:07.252165738-03:00",
      "description": "Petals//github.com/bigscience-workshop/petals",
      "summary": "This document provides instructions and code examples for integrating and running Petals large language models using the LiteLLM library, including support for streaming responses.",
      "tags": [
        "petals",
        "litellm",
        "python",
        "llm-integration",
        "streaming",
        "decentralized-inference"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-petals.md"
    },
    {
      "file_path": "258-docs-providers-poe.md",
      "title": "Poe | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/poe",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:09.55242852-03:00",
      "description": "Overview",
      "summary": "This document explains how to integrate and use Quora's Poe AI platform with LiteLLM, covering environment setup, Python SDK usage, and proxy server configuration.",
      "tags": [
        "poe-api",
        "litellm-integration",
        "python-sdk",
        "proxy-server",
        "ai-models",
        "environment-variables"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-poe.md"
    },
    {
      "file_path": "259-docs-providers-predibase.md",
      "title": "Predibase | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/predibase",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:09.680933142-03:00",
      "description": "LiteLLM supports all models on Predibase",
      "summary": "This document provides instructions for integrating Predibase models with LiteLLM, covering setup for both SDK and proxy configurations. It explains how to handle authentication, apply custom prompt templates, and pass provider-specific parameters such as adapter IDs.",
      "tags": [
        "litellm",
        "predibase",
        "llm-integration",
        "api-configuration",
        "prompt-templates",
        "python-sdk",
        "model-deployment"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-predibase.md"
    },
    {
      "file_path": "260-docs-providers-publicai.md",
      "title": "PublicAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/publicai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:09.868431476-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating PublicAI models into LiteLLM using the Python SDK or the LiteLLM Proxy. It details the required environment variables, model prefixes, and configuration steps for both streaming and non-streaming requests.",
      "tags": [
        "publicai",
        "litellm",
        "python-sdk",
        "llm-api",
        "streaming",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-publicai.md"
    },
    {
      "file_path": "261-docs-providers-pydantic-ai-agent.md",
      "title": "Pydantic AI Agents | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/pydantic_ai_agent",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:11.04126452-03:00",
      "description": "Call Pydantic AI Agents via LiteLLM's A2A Gateway.",
      "summary": "This document provides a step-by-step guide on how to integrate Pydantic AI agents with the LiteLLM A2A Gateway using the native A2A protocol support.",
      "tags": [
        "pydantic-ai",
        "litellm",
        "a2a-gateway",
        "ai-agents",
        "python",
        "fastapi",
        "agent-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-pydantic-ai-agent.md"
    },
    {
      "file_path": "262-docs-providers-ragflow-vector-store.md",
      "title": "RAGFlow Vector Stores | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/ragflow_vector_store",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:14.631479672-03:00",
      "description": "Litellm support creation and management of datasets for document processing and knowledge base management in Ragflow.",
      "summary": "This document explains how to integrate and manage RAGFlow datasets using LiteLLM, covering configuration, dataset creation, and specific chunking parameters for knowledge base management.",
      "tags": [
        "litellm",
        "ragflow",
        "vector-store",
        "dataset-management",
        "rag-applications",
        "knowledge-base",
        "chunking-methods"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-ragflow-vector-store.md"
    },
    {
      "file_path": "263-docs-providers-ragflow.md",
      "title": "RAGFlow | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/ragflow",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:11.500950089-03:00",
      "description": "Litellm supports Ragflow's chat completions APIs",
      "summary": "This document explains how to integrate RAGFlow with LiteLLM, detailing the specific model naming conventions, authentication methods, and configuration for both chat and agent endpoints.",
      "tags": [
        "litellm",
        "ragflow",
        "api-integration",
        "chat-completions",
        "llm-proxy",
        "streaming-responses"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-ragflow.md"
    },
    {
      "file_path": "264-docs-providers-recraft.md",
      "title": "Recraft | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/recraft",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:16.700126543-03:00",
      "description": "https://www.recraft.ai/",
      "summary": "This document provides a guide for integrating Recraft AI with LiteLLM to perform image generation and editing via Python SDK and Proxy Server. It details setup procedures, supported models, and the implementation of both OpenAI-compatible and Recraft-specific parameters.",
      "tags": [
        "litellm",
        "recraft",
        "image-generation",
        "image-editing",
        "python-sdk",
        "api-integration",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-recraft.md"
    },
    {
      "file_path": "265-docs-providers-replicate.md",
      "title": "Replicate | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/replicate",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:17.730832589-03:00",
      "description": "LiteLLM supports all models on Replicate",
      "summary": "This document explains how to integrate and use Replicate models through LiteLLM, including authentication, custom prompt formatting, and calling specific deployments.",
      "tags": [
        "litellm",
        "replicate",
        "python-sdk",
        "llm-api",
        "prompt-engineering",
        "model-deployment"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-replicate.md"
    },
    {
      "file_path": "266-docs-providers-runwayml-images.md",
      "title": "RunwayML - Image Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/runwayml/images",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:19.257202127-03:00",
      "description": "Overview",
      "summary": "This document provides a comprehensive guide for integrating and using RunwayML's Gen-4 image generation API through LiteLLM, including setup, parameter support, and cost tracking.",
      "tags": [
        "litellm",
        "runwayml",
        "image-generation",
        "api-integration",
        "python",
        "asynchronous-processing",
        "image-api"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-runwayml-images.md"
    },
    {
      "file_path": "267-docs-providers-runwayml-text-to-speech.md",
      "title": "RunwayML - Text-to-Speech | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/runwayml/text-to-speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:19.601921496-03:00",
      "description": "Overview",
      "summary": "This document provides a comprehensive guide for integrating RunwayML's text-to-speech API with LiteLLM, covering authentication, voice mapping, and proxy configuration. It explains how LiteLLM simplifies the process by handling task polling and response transformation automatically.",
      "tags": [
        "litellm",
        "runwayml",
        "text-to-speech",
        "api-integration",
        "audio-generation",
        "python",
        "async-api",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-runwayml-text-to-speech.md"
    },
    {
      "file_path": "268-docs-providers-runwayml-videos.md",
      "title": "RunwayML - Video Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/runwayml/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:22.359985572-03:00",
      "description": "LiteLLM supports RunwayML's Gen-4 video generation API, allowing you to generate videos from text prompts and images.",
      "summary": "This document provides a comprehensive guide on using LiteLLM to generate videos via the RunwayML Gen-4 API, covering text-to-video, image-to-video, and status polling workflows.",
      "tags": [
        "litellm",
        "runwayml",
        "video-generation",
        "python-sdk",
        "api-integration",
        "async-programming",
        "cost-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-runwayml-videos.md"
    },
    {
      "file_path": "269-docs-providers-sambanova.md",
      "title": "SambaNova | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/sambanova",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:22.671905994-03:00",
      "description": "https://cloud.sambanova.ai/",
      "summary": "This document provides a comprehensive guide for integrating SambaNova AI models with LiteLLM, covering authentication, proxy configuration, streaming, tool calling, and multimodal capabilities.",
      "tags": [
        "sambanova",
        "litellm",
        "python-sdk",
        "llm-api",
        "tool-calling",
        "embeddings",
        "computer-vision",
        "structured-output"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-sambanova.md"
    },
    {
      "file_path": "270-docs-providers-sap.md",
      "title": "SAP Generative AI Hub | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/sap",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:22.725419321-03:00",
      "description": "LiteLLM supports SAP Generative AI Hub's Orchestration Service.",
      "summary": "This document provides a guide for integrating LiteLLM with SAP Generative AI Hub, covering authentication via service keys, environment variable configuration, and model naming conventions.",
      "tags": [
        "litellm",
        "sap-ai-core",
        "generative-ai-hub",
        "orchestration-service",
        "sap-btp",
        "llm-integration",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-sap.md"
    },
    {
      "file_path": "271-docs-providers-snowflake.md",
      "title": "Snowflake | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/snowflake",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:25.126334288-03:00",
      "description": "| Property                   | Details                                                                                                   |",
      "summary": "This document provides instructions on how to authenticate and execute completion and embedding calls to Snowflake models using the LiteLLM library.",
      "tags": [
        "snowflake",
        "litellm",
        "authentication",
        "jwt-token",
        "python-sdk",
        "llm-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-snowflake.md"
    },
    {
      "file_path": "272-docs-providers-stability.md",
      "title": "Stability AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/stability",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:25.85104505-03:00",
      "description": "https://stability.ai/",
      "summary": "This document provides a guide for using LiteLLM to integrate with Stability AI's REST API for image generation and editing tasks, including setup, parameter mapping, and proxy server configuration.",
      "tags": [
        "stability-ai",
        "litellm",
        "image-generation",
        "image-editing",
        "stable-diffusion",
        "python-sdk",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-stability.md"
    },
    {
      "file_path": "273-docs-providers-synthetic.md",
      "title": "Synthetic | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/synthetic",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:28.466357107-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Synthetic's privacy-focused AI models with LiteLLM, covering environment setup, Python SDK usage, and proxy server configuration.",
      "tags": [
        "synthetic",
        "litellm",
        "ai-integration",
        "privacy-first",
        "python-sdk",
        "llm-proxy",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-synthetic.md"
    },
    {
      "file_path": "274-docs-providers-text-completion-openai.md",
      "title": "OpenAI (Text Completion) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/text_completion_openai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:29.542518211-03:00",
      "description": "LiteLLM supports OpenAI text completion models",
      "summary": "This document explains how to use OpenAI text completion and instruct models with LiteLLM, covering direct integration, environment setup, and proxy server configuration.",
      "tags": [
        "openai",
        "litellm",
        "text-completion",
        "instruct-models",
        "proxy-server",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-text-completion-openai.md"
    },
    {
      "file_path": "275-docs-providers-triton-inference-server.md",
      "title": "Triton Inference Server | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/triton-inference-server",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:32.646690155-03:00",
      "description": "LiteLLM supports Embedding Models on Triton Inference Servers",
      "summary": "This document explains how to use LiteLLM to interface with NVIDIA Triton Inference Server for chat completions and embedding models. It covers routing requests to specific Triton endpoints including /generate, /infer, and /embeddings.",
      "tags": [
        "triton-inference-server",
        "litellm",
        "chat-completion",
        "embeddings",
        "nvidia",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-triton-inference-server.md"
    },
    {
      "file_path": "276-docs-providers-v0.md",
      "title": "v0 | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/v0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:36.053963198-03:00",
      "description": "Overview",
      "summary": "This document provides instructions and code examples for integrating v0.dev AI models with LiteLLM, covering authentication, streaming, vision support, and function calling.",
      "tags": [
        "v0-dev",
        "litellm",
        "code-generation",
        "python-sdk",
        "api-integration",
        "ai-models"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-v0.md"
    },
    {
      "file_path": "277-docs-providers-vertex-ai-agent-engine.md",
      "title": "Vertex AI Agent Engine | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_ai_agent_engine",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:38.197680748-03:00",
      "description": "Call Vertex AI Agent Engine (Reasoning Engines) in the OpenAI Request/Response format.",
      "summary": "This document provides instructions for integrating Vertex AI Agent Engine with LiteLLM, covering Python SDK usage, proxy configuration, and the visual A2A Gateway setup.",
      "tags": [
        "vertex-ai",
        "agent-engine",
        "litellm",
        "reasoning-engines",
        "openai-compatibility",
        "agentic-workflows",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-vertex-ai-agent-engine.md"
    },
    {
      "file_path": "278-docs-providers-vertex-ai-videos.md",
      "title": "Vertex AI Video Generation (Veo) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_ai/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:38.922997437-03:00",
      "description": "LiteLLM supports Vertex AI's Veo video generation models using the unified OpenAI video API surface.",
      "summary": "This document provides instructions and code examples for integrating LiteLLM with Google Cloud's Vertex AI Veo models for video generation. It details authentication procedures, synchronous and asynchronous generation workflows, status polling, and retrieving generated video content.",
      "tags": [
        "litellm",
        "vertex-ai",
        "veo",
        "video-generation",
        "google-cloud-platform",
        "python",
        "asyncio"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex-ai-videos.md"
    },
    {
      "file_path": "279-docs-providers-vertex-batch.md",
      "title": "Vertex Batch APIs | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_batch",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:42.470379538-03:00",
      "description": "Just add the following Vertex env vars to your environment.",
      "summary": "This document provides a step-by-step guide for performing batch predictions on Vertex AI using an OpenAI-compatible workflow, covering environment setup, file management, and job tracking.",
      "tags": [
        "vertex-ai",
        "batch-prediction",
        "litellm",
        "google-cloud-storage",
        "openai-api",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-vertex-batch.md"
    },
    {
      "file_path": "280-docs-providers-vertex-image.md",
      "title": "Vertex AI Image Generation | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_image",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:44.092588358-03:00",
      "description": "Vertex AI supports two types of image generation:",
      "summary": "This document provides instructions and code examples for performing image generation through Google Vertex AI using LiteLLM with Gemini and Imagen models.",
      "tags": [
        "vertex-ai",
        "image-generation",
        "litellm",
        "gemini",
        "imagen",
        "python",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex-image.md"
    },
    {
      "file_path": "281-docs-providers-vertex-ocr.md",
      "title": "Vertex AI OCR | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_ocr",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:46.36683072-03:00",
      "description": "Overview",
      "summary": "This document provides a comprehensive guide for implementing Vertex AI OCR using LiteLLM, covering authentication methods, SDK and proxy configurations, and document processing for PDFs and images.",
      "tags": [
        "vertex-ai",
        "ocr",
        "litellm",
        "mistral",
        "document-processing",
        "python-sdk",
        "data-extraction"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex-ocr.md"
    },
    {
      "file_path": "282-docs-providers-vertex-self-deployed.md",
      "title": "Vertex AI - Self Deployed Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_self_deployed",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:48.241730021-03:00",
      "description": "Deploy and use your own models on Vertex AI through Model Garden or custom endpoints.",
      "summary": "This document explains how to deploy and access models from Vertex AI Model Garden and custom endpoints using LiteLLM. It provides configuration details for both SDK and proxy usage, specifically for OpenAI-compatible models and Gemma-based custom deployments.",
      "tags": [
        "vertex-ai",
        "google-cloud-platform",
        "model-garden",
        "litellm",
        "gemma",
        "custom-endpoints",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex-self-deployed.md"
    },
    {
      "file_path": "283-docs-providers-vertex-speech.md",
      "title": "Vertex AI Text to Speech | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:50.584611158-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides a comprehensive guide for integrating Google Cloud Text-to-Speech (Chirp3 HD) and Gemini TTS using LiteLLM's Python SDK and AI Gateway, covering configuration, voice mapping, and SSML support.",
      "tags": [
        "google-cloud",
        "vertex-ai",
        "text-to-speech",
        "litellm",
        "gemini-tts",
        "audio-generation",
        "ssml"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex-speech.md"
    },
    {
      "file_path": "284-docs-providers-vertex.md",
      "title": "VertexAI [Gemini] | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:36.710773669-03:00",
      "description": "Overview",
      "summary": "This document provides technical instructions for integrating Google Vertex AI with LiteLLM, covering authentication methods, supported model operations, and advanced features like function calling and JSON schema validation.",
      "tags": [
        "vertex-ai",
        "litellm",
        "google-cloud",
        "gemini-api",
        "function-calling",
        "json-schema",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vertex.md"
    },
    {
      "file_path": "285-docs-providers-vllm-batches.md",
      "title": "vLLM - Batch + Files API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vllm_batches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:53.401437053-03:00",
      "description": "LiteLLM supports vLLM's Batch and Files API for processing large volumes of requests asynchronously.",
      "summary": "This document explains how LiteLLM integrates with vLLM's Batch and Files API to support asynchronous large-scale request processing and multi-tenant routing.",
      "tags": [
        "litellm",
        "vllm",
        "batch-api",
        "asynchronous-processing",
        "proxy-routing",
        "multi-tenancy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vllm-batches.md"
    },
    {
      "file_path": "286-docs-providers-vllm.md",
      "title": "VLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vllm",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:51.947357054-03:00",
      "description": "LiteLLM supports all models on VLLM.",
      "summary": "This document explains how to integrate LiteLLM with vLLM models using OpenAI-compatible endpoints, covering setup for completions, embeddings, reranking, and video processing.",
      "tags": [
        "vllm",
        "litellm",
        "openai-compatible",
        "llm-inference",
        "embeddings",
        "rerank",
        "video-processing"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-vllm.md"
    },
    {
      "file_path": "287-docs-providers-volcano.md",
      "title": "Volcano Engine (Volcengine) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/volcano",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:54.012232836-03:00",
      "description": "https://www.volcengine.com/docs/82379/1263482",
      "summary": "This document provides instructions and code samples for integrating Volcengine (Doubao) chat and embedding models into LiteLLM, including API key setup and proxy configuration.",
      "tags": [
        "volcengine",
        "litellm",
        "python-sdk",
        "llm-api",
        "embeddings",
        "chat-completion",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-volcano.md"
    },
    {
      "file_path": "288-docs-providers-wandb-inference.md",
      "title": "Weights & Biases Inference | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/wandb_inference",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:55.932586953-03:00",
      "description": "https://weave-docs.wandb.ai/quickstart-inference",
      "summary": "This document explains how to integrate LiteLLM with the W&B Inference service to perform text generation and streaming tasks. It provides setup instructions for API authentication, proxy server configuration, and common error resolution.",
      "tags": [
        "litellm",
        "wandb-inference",
        "text-generation",
        "streaming",
        "api-configuration",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-wandb-inference.md"
    },
    {
      "file_path": "289-docs-providers-watsonx-audio-transcription.md",
      "title": "WatsonX Audio Transcription | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/watsonx/audio_transcription",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:58.309891014-03:00",
      "description": "Overview",
      "summary": "This document provides a code example for transcribing audio files using the LiteLLM library with the IBM WatsonX Whisper model.",
      "tags": [
        "litellm",
        "audio-transcription",
        "ibm-watsonx",
        "whisper-v3",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-providers-watsonx-audio-transcription.md"
    },
    {
      "file_path": "290-docs-providers-watsonx.md",
      "title": "IBM watsonx.ai | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/watsonx/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:57.589554734-03:00",
      "description": "LiteLLM supports all IBM watsonx.ai foundational models and embeddings.",
      "summary": "This document provides a comprehensive guide for integrating IBM watsonx.ai models and embeddings with LiteLLM, covering authentication, environment setup, and various usage patterns.",
      "tags": [
        "ibm-watsonx",
        "litellm",
        "chat-completion",
        "embeddings",
        "python-sdk",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-watsonx.md"
    },
    {
      "file_path": "291-docs-providers-xiaomi-mimo.md",
      "title": "Xiaomi MiMo | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/xiaomi_mimo",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:01.212898682-03:00",
      "description": "https://platform.xiaomimimo.com/#/docs",
      "summary": "This document provides instructions for integrating Xiaomi MiMo models using LiteLLM, covering API key configuration, streaming implementation, and proxy server setup.",
      "tags": [
        "xiaomi-mimo",
        "litellm",
        "api-integration",
        "python",
        "streaming",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-xiaomi-mimo.md"
    },
    {
      "file_path": "292-docs-providers-xinference.md",
      "title": "Xinference [Xorbits Inference] | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/xinference",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:01.770141011-03:00",
      "description": "https://inference.readthedocs.io/en/latest/index.html",
      "summary": "This document provides instructions and code examples for integrating LiteLLM with Xinference to perform text embedding and image generation tasks.",
      "tags": [
        "xinference",
        "litellm",
        "embeddings",
        "image-generation",
        "python-sdk",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-xinference.md"
    },
    {
      "file_path": "293-docs-providers-zai.md",
      "title": "Z.AI (Zhipu AI) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/zai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:02.365791104-03:00",
      "description": "https://z.ai/",
      "summary": "This document provides instructions and code examples for integrating Z.AI GLM text and chat models with LiteLLM, covering API configuration, streaming, and model pricing.",
      "tags": [
        "z-ai",
        "litellm",
        "glm-models",
        "api-integration",
        "python-sdk",
        "streaming",
        "llm-pricing"
      ],
      "category": "guide",
      "original_file_path": "docs-providers-zai.md"
    },
    {
      "file_path": "294-docs-proxy-ai-hub.md",
      "title": "AI Hub | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ai_hub",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:10.325133883-03:00",
      "description": "Share models and agents with your organization. Show developers what's available without needing to rebuild them.",
      "summary": "This document explains how administrators can share and manage models, agents, and MCP servers across an organization by making them discoverable through a public AI Hub.",
      "tags": [
        "ai-hub",
        "model-sharing",
        "agent-discovery",
        "mcp-servers",
        "admin-ui",
        "resource-management"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-ai-hub.md"
    },
    {
      "file_path": "295-docs-proxy-alerting.md",
      "title": "Alerting / Webhooks | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/alerting",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:10.700925578-03:00",
      "description": "Get alerts for:",
      "summary": "This document provides instructions for configuring and managing real-time alerts for LLM proxy performance, budget tracking, and system health across platforms like Slack, Discord, and Microsoft Teams.",
      "tags": [
        "litellm",
        "alerting",
        "monitoring",
        "slack-webhook",
        "budget-management",
        "llm-performance",
        "system-health"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-alerting.md"
    },
    {
      "file_path": "296-docs-proxy-api.md",
      "title": "ðŸ”‘ LiteLLM Keys (Access Claude-2, Llama2-70b, etc.) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:06.016910011-03:00",
      "description": "Use this if you're trying to add support for new LLMs and need access for testing. We provide a free $10 community-key for testing all providers on LiteLLM:",
      "summary": "This document explains how to use the free LiteLLM community key to test various language model providers and lists the specific models currently supported by the key.",
      "tags": [
        "litellm",
        "community-key",
        "llm-testing",
        "model-support",
        "api-integration",
        "open-interpreter"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-api.md"
    },
    {
      "file_path": "297-docs-proxy-arize-phoenix-prompts.md",
      "title": "Arize Phoenix Prompt Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/arize_phoenix_prompts",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:14.398541454-03:00",
      "description": "Use prompt versions from Arize Phoenix with LiteLLM SDK and Proxy.",
      "summary": "This document provides instructions for integrating Arize Phoenix prompt management with the LiteLLM SDK and Proxy, covering setup, configuration, and variable templating.",
      "tags": [
        "arize-phoenix",
        "litellm",
        "prompt-management",
        "python-sdk",
        "llm-proxy",
        "prompt-templates"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-arize-phoenix-prompts.md"
    },
    {
      "file_path": "298-docs-proxy-auto-routing.md",
      "title": "Auto Routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/auto_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:14.699148366-03:00",
      "description": "LiteLLM can auto select the best model for a request based on rules you define.",
      "summary": "This document explains how to configure and implement auto routing in LiteLLM to automatically select the best model for a request based on semantic matching of input content.",
      "tags": [
        "litellm",
        "auto-routing",
        "model-selection",
        "semantic-router",
        "embeddings",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-auto-routing.md"
    },
    {
      "file_path": "299-docs-proxy-billing.md",
      "title": "Billing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/billing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:15.633104605-03:00",
      "description": "Bill internal teams, external customers for their usage",
      "summary": "This document provides instructions for integrating LiteLLM with Lago to enable usage-based billing for internal teams and external customers.",
      "tags": [
        "litellm",
        "lago",
        "usage-based-billing",
        "cost-tracking",
        "api-proxy",
        "billing-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-billing.md"
    },
    {
      "file_path": "300-docs-proxy-call-hooks.md",
      "title": "Modify / Reject Incoming Requests | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/call_hooks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:20.001843549-03:00",
      "description": "- Modify data before making llm api calls on proxy",
      "summary": "This document explains how to implement and configure callback hooks in LiteLLM Proxy to intercept, modify, or reject data during the LLM request-response lifecycle. It covers specific hooks for pre-call modifications, parallel moderation checks, and post-call transformations.",
      "tags": [
        "litellm-proxy",
        "callback-hooks",
        "middleware",
        "request-transformation",
        "moderation",
        "python-sdk",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-call-hooks.md"
    },
    {
      "file_path": "301-docs-proxy-cli-sso.md",
      "title": "CLI Authentication | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/cli_sso",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:22.163949614-03:00",
      "description": "Use the litellm cli to authenticate to the LiteLLM Gateway. This is great if you're trying to give a large number of developers self-serve access to the LiteLLM Gateway.",
      "summary": "This guide explains how to install and use the LiteLLM CLI to authenticate to the LiteLLM Gateway via SSO for self-serve access.",
      "tags": [
        "litellm-cli",
        "sso-authentication",
        "gateway-access",
        "cli-tool",
        "proxy-setup"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-cli-sso.md"
    },
    {
      "file_path": "302-docs-proxy-control-plane-and-data-plane.md",
      "title": "Control Plane for Multi-region Architecture (Enterprise) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/control_plane_and_data_plane",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:28.329938359-03:00",
      "description": "Learn how to deploy LiteLLM across multiple regions while maintaining centralized administration and avoiding duplication of management overhead.",
      "summary": "This guide explains how to implement a distributed LiteLLM architecture by separating regional worker instances from a centralized administrative instance to optimize performance and management.",
      "tags": [
        "litellm",
        "multi-region",
        "deployment-architecture",
        "configuration",
        "scalability",
        "distributed-systems"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-control-plane-and-data-plane.md"
    },
    {
      "file_path": "303-docs-proxy-cost-tracking.md",
      "title": "Spend Tracking | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/cost_tracking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:29.29906074-03:00",
      "description": "Track spend for keys, users, and teams across 100+ LLMs.",
      "summary": "This document provides instructions on how to set up and manage spend tracking for various LLMs using LiteLLM, including database integration and usage monitoring.",
      "tags": [
        "cost-tracking",
        "litellm-proxy",
        "spend-management",
        "api-usage",
        "budgeting",
        "usage-reports"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-cost-tracking.md"
    },
    {
      "file_path": "304-docs-proxy-custom-auth.md",
      "title": "Custom Auth | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/custom_auth",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:31.089817649-03:00",
      "description": "You can now override the default api key auth.",
      "summary": "This document explains how to implement custom API key authentication for the LiteLLM proxy by defining a custom auth function and utilizing the UserAPIKeyAuth object for fine-grained access control.",
      "tags": [
        "litellm",
        "api-key-auth",
        "custom-authentication",
        "proxy-configuration",
        "rate-limiting",
        "budget-management",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-custom-auth.md"
    },
    {
      "file_path": "305-docs-proxy-custom-prompt-management.md",
      "title": "Custom Prompt Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/custom_prompt_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:34.825921529-03:00",
      "description": "Connect LiteLLM to your prompt management system with custom hooks.",
      "summary": "This document explains how to integrate custom prompt management systems with LiteLLM by implementing and registering a custom hook class for automated prompt retrieval and formatting.",
      "tags": [
        "litellm",
        "prompt-management",
        "custom-hooks",
        "python-sdk",
        "llm-integration",
        "callbacks"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-custom-prompt-management.md"
    },
    {
      "file_path": "306-docs-proxy-custom-sso.md",
      "title": "âœ¨ Event Hooks for SSO Login | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/custom_sso",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:35.318459744-03:00",
      "description": "âœ¨ SSO is free for up to 5 users. After that, an enterprise license is required. Get Started with Enterprise here",
      "summary": "This document explains how to implement custom SSO login handlers and integrate OAuth proxies with the LiteLLM Admin UI using Python.",
      "tags": [
        "litellm",
        "sso-authentication",
        "oauth-proxy",
        "custom-handler",
        "admin-ui"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-custom-sso.md"
    },
    {
      "file_path": "307-docs-proxy-customer-routing.md",
      "title": "[DEPRECATED] Region-based Routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/customer_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:36.825466291-03:00",
      "description": "This is deprecated, please use Tag Based Routing instead",
      "summary": "This document explains how to configure LiteLLM to route specific customers to AI models located in designated geographic regions for compliance and data residency purposes.",
      "tags": [
        "litellm",
        "model-routing",
        "data-residency",
        "regional-routing",
        "api-proxy",
        "customer-segmentation"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-customer-routing.md"
    },
    {
      "file_path": "308-docs-proxy-customer-usage.md",
      "title": "Customer Usage | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/customer_usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:38.95731219-03:00",
      "description": "Track and visualize end-user spend directly in the dashboard. Monitor customer-level usage analytics, spend logs, and activity metrics to understand how your customers are using your LLM services.",
      "summary": "This document explains how to track and visualize end-user spend and usage metrics by associating API requests with customer IDs. It provides instructions for monitoring customer-level analytics, filtering logs, and managing billing within the platform's Admin UI.",
      "tags": [
        "customer-usage",
        "spend-tracking",
        "usage-analytics",
        "billing",
        "llm-monitoring",
        "admin-ui"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-customer-usage.md"
    },
    {
      "file_path": "309-docs-proxy-customers.md",
      "title": "Customers / End-User Budgets | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/customers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:39.129595654-03:00",
      "description": "Track spend, set budgets for your customers.",
      "summary": "This guide explains how to track and manage LLM usage spend for customers using LiteLLM Proxy, including setting budget limits and pricing tiers.",
      "tags": [
        "litellm-proxy",
        "spend-tracking",
        "budget-management",
        "rate-limiting",
        "usage-quotas",
        "customer-management"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-customers.md"
    },
    {
      "file_path": "310-docs-proxy-debugging.md",
      "title": "Debugging | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/debugging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:44.368580037-03:00",
      "description": "2 levels of debugging supported.",
      "summary": "This document explains how to enable and use debugging features in LiteLLM to monitor raw API requests and responses or capture logs during errors.",
      "tags": [
        "litellm",
        "debugging",
        "logging",
        "api-requests",
        "troubleshooting"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-debugging.md"
    },
    {
      "file_path": "311-docs-proxy-deleted-keys-teams.md",
      "title": "Deleted Keys & Teams Audit Logs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/deleted_keys_teams",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:45.047893825-03:00",
      "description": "View deleted API keys and teams along with their spend and budget information at the time of deletion for auditing and compliance purposes.",
      "summary": "This document explains how to view audit trails for deleted API keys and teams in LiteLLM, including captured spend and budget data for compliance and financial auditing.",
      "tags": [
        "litellm-proxy",
        "audit-logs",
        "api-keys",
        "compliance",
        "spend-tracking",
        "admin-ui"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-deleted-keys-teams.md"
    },
    {
      "file_path": "312-docs-proxy-deploy.md",
      "title": "Docker, Helm, Terraform | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/deploy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:45.880683641-03:00",
      "description": "There are no limits on the number of users, keys, or teams you can create on LiteLLM OSS.",
      "summary": "This document provides comprehensive instructions for deploying and configuring the LiteLLM OSS proxy server using Docker, Kubernetes, Terraform, and Helm charts.",
      "tags": [
        "litellm",
        "docker",
        "kubernetes",
        "helm",
        "deployment",
        "proxy-server",
        "oss",
        "infrastructure"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-deploy.md"
    },
    {
      "file_path": "313-docs-proxy-dynamic-logging.md",
      "title": "Dynamic Callback Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/dynamic_logging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:47.500369102-03:00",
      "description": "âœ¨ This is an enterprise feature.",
      "summary": "This document explains how to dynamically manage and disable LiteLLM callbacks using request headers to control logging and observability on a per-request basis.",
      "tags": [
        "litellm",
        "callback-management",
        "logging-control",
        "api-headers",
        "observability",
        "enterprise-compliance"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-dynamic-logging.md"
    },
    {
      "file_path": "314-docs-proxy-dynamic-rate-limit.md",
      "title": "Dynamic TPM/RPM Allocation | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/dynamic_rate_limit",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:49.159552974-03:00",
      "description": "Prevent projects from gobbling too much tpm/rpm.",
      "summary": "This document provides a code example for demonstrating and testing rate limiting across multiple API keys sharing a single model via a proxy server. It illustrates how token quotas are enforced per key and how to handle RateLimitError exceptions in a concurrent environment.",
      "tags": [
        "rate-limiting",
        "openai-proxy",
        "token-management",
        "api-key-generation",
        "python",
        "error-handling"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-dynamic-rate-limit.md"
    },
    {
      "file_path": "315-docs-proxy-email.md",
      "title": "Email Notifications | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/email",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:50.474939616-03:00",
      "description": "<Image",
      "summary": "This document provides instructions for configuring and customizing automated email notifications for LiteLLM Proxy events like user creation, API key management, and budget alerts.",
      "tags": [
        "litellm",
        "email-notifications",
        "smtp-configuration",
        "budget-alerts",
        "branding-customization",
        "security-settings"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-email.md"
    },
    {
      "file_path": "316-docs-proxy-embedding.md",
      "title": "Embeddings - /embeddings | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:53.055119947-03:00",
      "description": "See supported Embedding Providers & Models here",
      "summary": "Provides a quick-start guide for setting up the LiteLLM proxy to handle embedding requests for multiple models and cloud providers using a YAML configuration.",
      "tags": [
        "litellm",
        "embedding-models",
        "proxy-server",
        "configuration",
        "aws-bedrock",
        "azure-openai",
        "sagemaker"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-embedding.md"
    },
    {
      "file_path": "317-docs-proxy-endpoint-activity.md",
      "title": "Endpoint Activity | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/endpoint_activity",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:53.379938315-03:00",
      "description": "Track and visualize API endpoint usage directly in the dashboard. Monitor endpoint-level activity analytics, spend breakdowns, and performance metrics to understand which endpoints are receiving the most traffic and how they're performing.",
      "summary": "This document explains how LiteLLM automatically tracks and visualizes usage, cost, and performance metrics for individual API endpoints within the Admin UI.",
      "tags": [
        "endpoint-activity",
        "usage-analytics",
        "cost-tracking",
        "litellm-proxy",
        "performance-monitoring",
        "admin-ui"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-endpoint-activity.md"
    },
    {
      "file_path": "318-docs-proxy-enterprise.md",
      "title": "âœ¨ Enterprise Features | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/enterprise",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:55.283376821-03:00",
      "description": "To get a license, get in touch with us here",
      "summary": "This document explains how to configure and use LiteLLM enterprise features for security, budget tracking, and automated guardrails.",
      "tags": [
        "litellm",
        "enterprise-features",
        "security",
        "spend-tracking",
        "guardrails",
        "secret-detection",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-enterprise.md"
    },
    {
      "file_path": "319-docs-proxy-error-diagnosis.md",
      "title": "Diagnosing Errors - Provider vs Gateway | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/error_diagnosis",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:56.185612111-03:00",
      "description": "Having trouble diagnosing if an error is from the LLM Provider (OpenAI, Anthropic, etc.) or from the LiteLLM AI Gateway itself? Here's how to tell.",
      "summary": "This guide explains how to identify whether an error message originates from an LLM provider or the LiteLLM AI Gateway by looking for specific exception prefixes.",
      "tags": [
        "litellm",
        "error-handling",
        "debugging",
        "troubleshooting",
        "exception-mapping",
        "api-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-error-diagnosis.md"
    },
    {
      "file_path": "320-docs-proxy-guardrails-aim-security.md",
      "title": "Aim Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/aim_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:00.639549064-03:00",
      "description": "Quick Start",
      "summary": "This document explains how to set up and integrate Aim Guard with LiteLLM to enforce security policies and guardrails for AI applications.",
      "tags": [
        "aim-security",
        "litellm",
        "guardrails",
        "ai-security",
        "pii-detection",
        "llm-gateway"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-aim-security.md"
    },
    {
      "file_path": "321-docs-proxy-guardrails-aporia-api.md",
      "title": "Aporia | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/aporia_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:02.396035423-03:00",
      "description": "Use Aporia to  detect PII in requests and profanity in responses",
      "summary": "This document explains how to integrate Aporia guardrails with LiteLLM to detect PII in requests and profanity in LLM responses. It covers configuration setup, operational modes, and applying guardrails at the API key level.",
      "tags": [
        "aporia",
        "litellm",
        "guardrails",
        "pii-detection",
        "llm-security",
        "content-filtering"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-aporia-api.md"
    },
    {
      "file_path": "322-docs-proxy-guardrails-bedrock.md",
      "title": "Bedrock Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/bedrock",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:05.83042428-03:00",
      "description": "If you haven't set up or authenticated your Bedrock provider yet, see the Bedrock Provider Setup & Authentication Guide.",
      "summary": "This document explains how to integrate and configure AWS Bedrock guardrails within LiteLLM to enforce content policies and PII masking. It covers setup procedures for different execution modes, handling blocked responses, and optimizing performance with experimental flags.",
      "tags": [
        "litellm",
        "aws-bedrock",
        "guardrails",
        "pii-masking",
        "llm-security",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-bedrock.md"
    },
    {
      "file_path": "323-docs-proxy-guardrails-custom-guardrail.md",
      "title": "Custom Guardrail | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/custom_guardrail",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:05.966165673-03:00",
      "description": "Use this if you want to write code to run a custom guardrail",
      "summary": "This document explains how to implement and integrate custom guardrails in LiteLLM by creating a Python class and configuring it within the gateway. It covers the setup of various event hooks to monitor, modify, or block LLM requests and responses based on custom rules.",
      "tags": [
        "litellm",
        "custom-guardrails",
        "api-gateway",
        "python-hooks",
        "moderation",
        "llm-security"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-custom-guardrail.md"
    },
    {
      "file_path": "324-docs-proxy-guardrails-dynamoai.md",
      "title": "DynamoAI Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/dynamoai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:07.965972294-03:00",
      "description": "LiteLLM supports DynamoAI guardrails for content moderation and policy enforcement on LLM inputs and outputs.",
      "summary": "This document explains how to integrate and configure DynamoAI guardrails within LiteLLM for content moderation and policy enforcement. It covers configuration parameters, execution modes, environment variables, and monitoring for LLM inputs and outputs.",
      "tags": [
        "litellm",
        "dynamoai",
        "guardrails",
        "content-moderation",
        "llm-security",
        "api-gateway",
        "llm-observability"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-dynamoai.md"
    },
    {
      "file_path": "325-docs-proxy-guardrails-enkryptai.md",
      "title": "EnkryptAI Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/enkryptai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:08.359062343-03:00",
      "description": "LiteLLM supports EnkryptAI guardrails for content moderation and safety checks on LLM inputs and outputs.",
      "summary": "This document provides instructions for integrating EnkryptAI guardrails with LiteLLM to perform content moderation, PII detection, and safety checks on model inputs and outputs. It covers configuration options, execution modes, and advanced settings like monitor mode and custom policies.",
      "tags": [
        "litellm",
        "enkryptai",
        "content-moderation",
        "llm-guardrails",
        "ai-safety",
        "pii-detection",
        "prompt-injection",
        "observability"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-enkryptai.md"
    },
    {
      "file_path": "326-docs-proxy-guardrails-grayswan.md",
      "title": "Gray Swan Cygnal Guardrail | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/grayswan",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:10.839231147-03:00",
      "description": "Use Gray Swan Cygnal to continuously monitor conversations for policy violations, indirect prompt injection (IPI), jailbreak attempts, and other safety risks.",
      "summary": "This document explains how to integrate Gray Swan Cygnal as a safety guardrail within LiteLLM to monitor and manage LLM requests for policy violations and prompt injections.",
      "tags": [
        "litellm",
        "gray-swan-cygnal",
        "guardrails",
        "llm-security",
        "prompt-injection",
        "safety-monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-grayswan.md"
    },
    {
      "file_path": "327-docs-proxy-guardrails-guardrail-load-balancing.md",
      "title": "Guardrail Load Balancing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/guardrail_load_balancing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:12.057762665-03:00",
      "description": "Load balance guardrail requests across multiple guardrail deployments. This is useful when you have rate limits on guardrail providers (e.g., AWS Bedrock Guardrails) and want to distribute requests across multiple accounts or regions.",
      "summary": "This document explains how to configure LiteLLM to load balance guardrail requests across multiple deployments, regions, or accounts to handle provider rate limits and ensure redundancy.",
      "tags": [
        "litellm",
        "guardrails",
        "load-balancing",
        "aws-bedrock",
        "rate-limiting",
        "multi-account",
        "high-availability"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-guardrail-load-balancing.md"
    },
    {
      "file_path": "328-docs-proxy-guardrails-guardrails-ai.md",
      "title": "Guardrails AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/guardrails_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:13.438736841-03:00",
      "description": "Use Guardrails AI (guardrailsai.com) to add checks to LLM output.",
      "summary": "This document provides instructions on integrating Guardrails AI with LiteLLM to implement output validation and manage security checks through configuration and API keys.",
      "tags": [
        "guardrails-ai",
        "litellm",
        "llm-security",
        "proxy-server",
        "api-gateway",
        "configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-guardrails-ai.md"
    },
    {
      "file_path": "329-docs-proxy-guardrails-hiddenlayer.md",
      "title": "HiddenLayer Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/hiddenlayer",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:13.487680319-03:00",
      "description": "LiteLLM ships with a native integration for HiddenLayer. The proxy sends every request/response to HiddenLayerâ€™s /detection/v1/interactions endpoint so you can block or redact unsafe content before it reaches your users.",
      "summary": "This document explains how to integrate LiteLLM with HiddenLayer to implement AI security guardrails for blocking or redacting unsafe content during model interactions.",
      "tags": [
        "litellm",
        "hiddenlayer",
        "security-guardrails",
        "prompt-injection",
        "ai-safety",
        "content-filtering"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-hiddenlayer.md"
    },
    {
      "file_path": "330-docs-proxy-guardrails-ibm-guardrails.md",
      "title": "IBM Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/ibm_guardrails",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:15.861203401-03:00",
      "description": "LiteLLM works with IBM's FMS Guardrails for content safety. You can use it to detect jailbreaks, PII, hate speech, and more.",
      "summary": "This document explains how to integrate LiteLLM with IBM's FMS Guardrails to implement content safety checks such as jailbreak and PII detection for LLM inputs and outputs.",
      "tags": [
        "litellm",
        "ibm-guardrails",
        "content-safety",
        "jailbreak-detection",
        "pii-protection",
        "llm-security",
        "configuration-guide"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-ibm-guardrails.md"
    },
    {
      "file_path": "331-docs-proxy-guardrails-javelin.md",
      "title": "Javelin Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/javelin",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:16.761305298-03:00",
      "description": "Javelin provides AI safety and content moderation services with support for prompt injection detection, trust & safety violations, and language detection.",
      "summary": "This document provides instructions for integrating Javelin AI safety guardrails into LiteLLM to enable prompt injection detection, content moderation, and language verification.",
      "tags": [
        "ai-safety",
        "litellm",
        "content-moderation",
        "prompt-injection",
        "guardrails",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-javelin.md"
    },
    {
      "file_path": "332-docs-proxy-guardrails-litellm-content-filter.md",
      "title": "LiteLLM Content Filter (Built-in Guardrails) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/litellm_content_filter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:19.471945069-03:00",
      "description": "Built-in guardrail for detecting and filtering sensitive information using regex patterns and keyword matching. No external dependencies required.",
      "summary": "This document explains how to implement LiteLLM's built-in content filter guardrail to detect and filter sensitive information using regex and keyword matching. It provides detailed instructions for configuration via UI and YAML, covering PII protection, custom patterns, and real-time streaming support.",
      "tags": [
        "litellm",
        "content-filtering",
        "guardrails",
        "pii-protection",
        "data-masking",
        "regex-patterns",
        "llm-security"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-litellm-content-filter.md"
    },
    {
      "file_path": "333-docs-proxy-guardrails-noma-security.md",
      "title": "Noma Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/noma_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:22.01252412-03:00",
      "description": "Use Noma Security to protect your LLM applications with comprehensive AI content moderation and safety guardrails.",
      "summary": "This document provides instructions for integrating Noma Security guardrails into LiteLLM to enable AI content moderation, safety filtering, and PII anonymization.",
      "tags": [
        "noma-security",
        "litellm",
        "ai-safety",
        "content-moderation",
        "guardrails",
        "llm-security",
        "pii-anonymization"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-noma-security.md"
    },
    {
      "file_path": "334-docs-proxy-guardrails-onyx-security.md",
      "title": "Onyx Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/onyx_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:24.24077064-03:00",
      "description": "Quick Start",
      "summary": "This document provides a step-by-step guide for integrating Onyx Guard into LiteLLM to implement AI security guardrails. It explains how to configure the proxy settings, define execution modes for input and output scanning, and verify the setup using test requests.",
      "tags": [
        "litellm",
        "onyx-guard",
        "ai-security",
        "guardrails",
        "prompt-injection",
        "llm-gateway",
        "content-filtering"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-onyx-security.md"
    },
    {
      "file_path": "335-docs-proxy-guardrails-openai-moderation.md",
      "title": "OpenAI Moderation | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/openai_moderation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:25.404689788-03:00",
      "description": "Overview",
      "summary": "This guide explains how to integrate and configure OpenAI's Moderation API as a guardrail in LiteLLM to filter harmful content from user inputs and model responses. It details various execution modes, streaming support, and best practices for implementing content safety policies.",
      "tags": [
        "litellm",
        "openai-moderation",
        "guardrails",
        "content-safety",
        "llm-security",
        "streaming-support"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-openai-moderation.md"
    },
    {
      "file_path": "336-docs-proxy-guardrails-panw-prisma-airs.md",
      "title": "PANW Prisma AIRS | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/panw_prisma_airs",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:27.07423023-03:00",
      "description": "LiteLLM supports PANW Prisma AIRS (AI Runtime Security) guardrails via the Prisma AIRS Scan API. This integration provides Security-as-Code for AI applications using Palo Alto Networks' AI security platform.",
      "summary": "This document provides a guide for integrating PANW Prisma AIRS guardrails with LiteLLM to provide security-as-code features like prompt injection detection and data loss prevention.",
      "tags": [
        "litellm",
        "prisma-airs",
        "guardrails",
        "ai-security",
        "data-loss-prevention",
        "prompt-injection",
        "palo-alto-networks"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-panw-prisma-airs.md"
    },
    {
      "file_path": "337-docs-proxy-guardrails-pii-masking-v2.md",
      "title": "PII, PHI Masking - Presidio | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/pii_masking_v2",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:30.031127748-03:00",
      "description": "Overview",
      "summary": "This document explains how to configure and deploy the Microsoft Presidio guardrail in LiteLLM to mask or block sensitive PII and PHI data in LLM requests and responses.",
      "tags": [
        "pii-masking",
        "presidio",
        "litellm",
        "data-privacy",
        "security-guardrails",
        "phi-protection"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-pii-masking-v2.md"
    },
    {
      "file_path": "338-docs-proxy-guardrails-pillar-security.md",
      "title": "Pillar Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/pillar_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:31.330888368-03:00",
      "description": "Pillar Security integrates with LiteLLM Proxy via the Generic Guardrail API, providing comprehensive AI security scanning for your LLM applications.",
      "summary": "This document explains how to integrate Pillar Security with LiteLLM Proxy to implement AI security guardrails for prompt injection, jailbreak detection, and sensitive data masking.",
      "tags": [
        "litellm",
        "pillar-security",
        "ai-security",
        "guardrails",
        "prompt-injection",
        "data-protection"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-guardrails-pillar-security.md"
    },
    {
      "file_path": "339-docs-proxy-guardrails-prompt-injection.md",
      "title": "In-memory Prompt Injection Detection | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/prompt_injection",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:31.437379127-03:00",
      "description": "LiteLLM Supports the following methods for detecting prompt injection attacks",
      "summary": "This document explains how to configure and use LiteLLM's prompt injection detection features, including similarity-based checking and LLM-assisted verification.",
      "tags": [
        "prompt-injection",
        "security",
        "litellm",
        "llm-security",
        "request-filtering",
        "injection-detection"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-prompt-injection.md"
    },
    {
      "file_path": "340-docs-proxy-guardrails-prompt-security.md",
      "title": "Prompt Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/prompt_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:32.239244233-03:00",
      "description": "Use Prompt Security to protect your LLM applications from prompt injection attacks, jailbreaks, harmful content, PII leakage, and malicious file uploads through comprehensive input and output validation.",
      "summary": "This document provides instructions for integrating Prompt Security guardrails with LiteLLM to protect LLM applications from prompt injection, PII leakage, and malicious file uploads. It details configuration options for input/output validation, file sanitization, and real-time streaming security.",
      "tags": [
        "prompt-security",
        "litellm",
        "guardrails",
        "prompt-injection",
        "pii-redaction",
        "file-sanitization",
        "llm-security"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-prompt-security.md"
    },
    {
      "file_path": "341-docs-proxy-guardrails-secret-detection.md",
      "title": "âœ¨ Secret Detection/Redaction (Enterprise-only) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/secret_detection",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:37.569458978-03:00",
      "description": "â“ Use this to REDACT API Keys, Secrets sent in requests to an LLM.",
      "summary": "This document explains how to use and configure LiteLLM's secret detection feature to automatically redact sensitive information and API keys from requests sent to LLM providers.",
      "tags": [
        "litellm",
        "secret-detection",
        "data-redaction",
        "api-security",
        "detect-secrets",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-secret-detection.md"
    },
    {
      "file_path": "342-docs-proxy-guardrails-test-playground.md",
      "title": "Guardrail Testing Playground | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/test_playground",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:38.633177745-03:00",
      "description": "Test and compare multiple guardrails in real-time with an interactive playground interface.",
      "summary": "This document provides instructions on how to use the LiteLLM Guardrail Testing Playground to compare multiple guardrail responses and validate security configurations in real-time.",
      "tags": [
        "litellm",
        "guardrails",
        "security-testing",
        "playground",
        "llm-security",
        "configuration-validation"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-test-playground.md"
    },
    {
      "file_path": "343-docs-proxy-guardrails-tool-permission.md",
      "title": "LiteLLM Tool Permission Guardrail | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/tool_permission",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:39.149119171-03:00",
      "description": "LiteLLM provides the LiteLLM Tool Permission Guardrail that lets you control which tool calls a model is allowed to invoke, using configurable allow/deny rules. This offers fine-grained, provider-agnostic control over tool execution (e.g., OpenAI Chat Completions toolcalls, Anthropic Messages tooluse, MCP tools).",
      "summary": "This document explains how to configure LiteLLM Tool Permission Guardrails to control and restrict model tool calls using regex-based allow/deny rules. It details setup via the UI or configuration files, including argument validation and custom error handling.",
      "tags": [
        "litellm",
        "guardrails",
        "tool-calling",
        "security",
        "access-control",
        "llm-governance"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-tool-permission.md"
    },
    {
      "file_path": "344-docs-proxy-guardrails-zscaler-ai-guard.md",
      "title": "Zscaler AI Guard | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/zscaler_ai_guard",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:40.69572803-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Zscaler AI Guard with LiteLLM to monitor and control AI traffic through security guardrails. It covers configuration steps, policy enforcement for prompts and responses, and advanced options for custom policies and user data analysis.",
      "tags": [
        "zscaler-ai-guard",
        "litellm-integration",
        "security-guardrails",
        "ai-governance",
        "configuration-guide",
        "prompt-security"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-guardrails-zscaler-ai-guard.md"
    },
    {
      "file_path": "345-docs-proxy-litellm-managed-files.md",
      "title": "[BETA] LiteLLM Managed Files | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/litellm_managed_files",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:47.10099448-03:00",
      "description": "- Reuse the same file across different providers.",
      "summary": "This document explains how to use LiteLLM Enterprise to manage file uploads across multiple AI providers and implement file-level permissions for secure user access.",
      "tags": [
        "litellm",
        "file-management",
        "access-control",
        "cross-provider",
        "enterprise-features",
        "proxy-server"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-litellm-managed-files.md"
    },
    {
      "file_path": "346-docs-proxy-litellm-prompt-management.md",
      "title": "LiteLLM AI Gateway Prompt Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/litellm_prompt_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:48.061484188-03:00",
      "description": "Use the LiteLLM AI Gateway to create, manage and version your prompts.",
      "summary": "This document explains how to use the LiteLLM AI Gateway dashboard to create, manage, and version dynamic prompts for integration into applications via API.",
      "tags": [
        "litellm-gateway",
        "prompt-management",
        "version-control",
        "prompt-engineering",
        "llm-ops",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-litellm-prompt-management.md"
    },
    {
      "file_path": "347-docs-proxy-logging.md",
      "title": "Logging | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/logging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:48.50905168-03:00",
      "description": "Log Proxy input, output, and exceptions using:",
      "summary": "This document provides instructions on how to configure and manage logging, request tracking, and data redaction within the LiteLLM Proxy. It covers integration with multiple monitoring providers like Langfuse and OpenTelemetry, as well as granular controls for privacy and conditional logging.",
      "tags": [
        "litellm-proxy",
        "logging",
        "monitoring",
        "data-redaction",
        "observability",
        "langfuse",
        "request-tracking"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-logging.md"
    },
    {
      "file_path": "348-docs-proxy-managed-batches.md",
      "title": "[BETA] LiteLLM Managed Files with Batches | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/managed_batches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:51.990674483-03:00",
      "description": "This is a free LiteLLM Enterprise feature.",
      "summary": "This document explains how to configure and use LiteLLM's Enterprise Batch feature to load balance across multiple deployments and manage batch processing via the LiteLLM Proxy.",
      "tags": [
        "litellm-proxy",
        "batch-processing",
        "azure-openai",
        "load-balancing",
        "enterprise-features",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-managed-batches.md"
    },
    {
      "file_path": "349-docs-proxy-managed-finetuning.md",
      "title": "âœ¨ [BETA] LiteLLM Managed Files with Finetuning | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/managed_finetuning",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:53.380390289-03:00",
      "description": "This is a free LiteLLM Enterprise feature.",
      "summary": "This document explains how to configure and use the LiteLLM Proxy to manage fine-tuning jobs across multiple providers such as OpenAI, Azure, and Vertex AI. It provides instructions for proxy administration and developer workflows for file management and fine-tuning CRUD operations.",
      "tags": [
        "litellm",
        "fine-tuning",
        "openai-proxy",
        "model-management",
        "enterprise-features",
        "vertex-ai",
        "azure-openai"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-managed-finetuning.md"
    },
    {
      "file_path": "350-docs-proxy-management-cli.md",
      "title": "LiteLLM Proxy CLI | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/management_cli",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:54.939560265-03:00",
      "description": "The litellm-proxy CLI is a command-line tool for managing your LiteLLM proxy",
      "summary": "This document explains how to use the litellm-proxy CLI tool to manage models, credentials, and users on a LiteLLM proxy server. It provides instructions for installation, authentication via SSO, and executing common management commands.",
      "tags": [
        "litellm-proxy",
        "cli",
        "model-management",
        "api-keys",
        "user-management",
        "authentication",
        "llm-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-management-cli.md"
    },
    {
      "file_path": "351-docs-proxy-master-key-rotations.md",
      "title": "Rotating Master Key | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/master_key_rotations",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:55.537609389-03:00",
      "description": "Here are our recommended steps for rotating your master key.",
      "summary": "This document outlines the step-by-step procedure for rotating the master key used to encrypt models within the Proxy_ModelTable database.",
      "tags": [
        "master-key-rotation",
        "database-encryption",
        "litellm",
        "security-management",
        "key-regeneration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-master-key-rotations.md"
    },
    {
      "file_path": "352-docs-proxy-model-access-groups.md",
      "title": "Model Access Groups | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_access_groups",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:58.629763295-03:00",
      "description": "Overview",
      "summary": "This document explains how to organize multiple AI models into named groups to simplify access control and permission management for API keys and teams.",
      "tags": [
        "access-control",
        "model-management",
        "api-key-permissions",
        "wildcard-matching",
        "litellm-proxy",
        "model-governance"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-model-access-groups.md"
    },
    {
      "file_path": "353-docs-proxy-model-access-guide.md",
      "title": "How Model Access Works | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_access_guide",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:00.41590596-03:00",
      "description": "Concept",
      "summary": "This document explains the organization of model deployments into model groups within LiteLLM to enable features like load balancing, access control, and failover handling.",
      "tags": [
        "litellm",
        "model-groups",
        "load-balancing",
        "access-control",
        "model-deployment",
        "failover-management"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-model-access-guide.md"
    },
    {
      "file_path": "354-docs-proxy-model-access.md",
      "title": "Restrict Model Access | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_access",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:56.8286848-03:00",
      "description": "Restrict models by Virtual Key",
      "summary": "This document explains how to manage access to specific models using virtual keys and team IDs, while also describing how to discover model metadata and fallbacks via the API.",
      "tags": [
        "access-control",
        "litellm-proxy",
        "model-restriction",
        "api-keys",
        "team-management",
        "fallback-models"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-model-access.md"
    },
    {
      "file_path": "355-docs-proxy-model-compare-ui.md",
      "title": "Model Compare Playground UI | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_compare_ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:01.145199516-03:00",
      "description": "Compare multiple LLM models side-by-side in an interactive playground interface. Evaluate model responses, performance metrics, and costs to make informed decisions about which models work best for your use case.",
      "summary": "This document explains how to use the Model Compare Playground UI to perform side-by-side evaluations of multiple LLM models based on response quality, performance metrics, and cost.",
      "tags": [
        "model-comparison",
        "llm-playground",
        "performance-metrics",
        "litellm-proxy",
        "model-evaluation",
        "cost-analysis",
        "benchmarking"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-model-compare-ui.md"
    },
    {
      "file_path": "356-docs-proxy-model-discovery.md",
      "title": "Model Discovery | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_discovery",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:01.403491619-03:00",
      "description": "Use this to give users an accurate list of models available behind provider endpoint, when calling /v1/models for wildcard models.",
      "summary": "This document explains how to configure and use a proxy to retrieve an accurate list of available models via the /v1/models endpoint.",
      "tags": [
        "proxy-configuration",
        "api-endpoint",
        "model-listing",
        "v1-models",
        "wildcard-models"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-model-discovery.md"
    },
    {
      "file_path": "357-docs-proxy-multiple-admins.md",
      "title": "âœ¨ Audit Logs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/multiple_admins",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:06.668023977-03:00",
      "description": "<Image",
      "summary": "This document explains how Proxy Admins can use audit logs to track entity changes and perform management actions on behalf of users for compliance and monitoring purposes.",
      "tags": [
        "audit-logs",
        "proxy-admin",
        "compliance",
        "management-api",
        "litellm",
        "user-impersonation"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-multiple-admins.md"
    },
    {
      "file_path": "358-docs-proxy-native-litellm-prompt.md",
      "title": "LiteLLM Prompt Management (GitOps) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/native_litellm_prompt",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:06.770721869-03:00",
      "description": "Store prompts as .prompt files in your repository and use them directly with LiteLLM. No external services required.",
      "summary": "This document explains how to use LiteLLM to load and execute prompts stored as .prompt files from local directories, BitBucket, or GitLab repositories.",
      "tags": [
        "litellm",
        "prompt-management",
        "dotprompt",
        "version-control-integration",
        "gitlab-integration",
        "bitbucket-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-native-litellm-prompt.md"
    },
    {
      "file_path": "359-docs-proxy-oauth2.md",
      "title": "Oauth 2.0 Authentication | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/oauth2",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:07.623319261-03:00",
      "description": "Use this if you want to use an Oauth2.0 token to make /chat, /embeddings requests to the LiteLLM Proxy",
      "summary": "This document provides instructions for configuring the LiteLLM Proxy to authenticate chat and embedding requests using OAuth2.0 tokens.",
      "tags": [
        "litellm-proxy",
        "oauth2-authentication",
        "security-configuration",
        "token-validation",
        "api-gateway",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-oauth2.md"
    },
    {
      "file_path": "360-docs-proxy-pass-through-guardrails.md",
      "title": "Guardrails on Pass-Through Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/pass_through_guardrails",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:11.768480724-03:00",
      "description": "Overview",
      "summary": "This document explains how to enable and configure security guardrails for LiteLLM pass-through endpoints using the UI or YAML configuration files. It covers field-level targeting with JSONPath, inheritance from organizational levels, and pre-call and post-call validation flows.",
      "tags": [
        "litellm",
        "guardrails",
        "pass-through-endpoints",
        "api-security",
        "jsonpath",
        "content-moderation",
        "endpoint-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-pass-through-guardrails.md"
    },
    {
      "file_path": "361-docs-proxy-pricing-calculator.md",
      "title": "Pricing Calculator (Cost Estimation) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/pricing_calculator",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:14.79622245-03:00",
      "description": "Estimate LLM costs based on expected token usage and request volume. This tool helps developers and platform teams forecast spending before deploying models to production.",
      "summary": "This document provides a step-by-step walkthrough for using the Pricing Calculator in the LiteLLM UI to forecast model costs based on token usage and request volume.",
      "tags": [
        "litellm",
        "cost-estimation",
        "llm-pricing",
        "token-usage",
        "budget-planning",
        "litellm-dashboard"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-pricing-calculator.md"
    },
    {
      "file_path": "362-docs-proxy-prometheus.md",
      "title": "ðŸ“ˆ Prometheus metrics | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/prometheus",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:16.752565708-03:00",
      "description": "LiteLLM Exposes a /metrics endpoint for Prometheus to Poll",
      "summary": "This document explains how to configure and use the LiteLLM Prometheus metrics endpoint to monitor proxy usage, spend, and performance across teams and virtual keys.",
      "tags": [
        "litellm",
        "prometheus-metrics",
        "monitoring",
        "proxy-configuration",
        "budget-tracking",
        "observability"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-prometheus.md"
    },
    {
      "file_path": "363-docs-proxy-provider-margins.md",
      "title": "Fee/Price Margin on LLM Costs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/provider_margins",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:22.712683676-03:00",
      "description": "Apply percentage-based or fixed-amount margins to specific providers or globally. This is useful for enterprises that need to add operational overhead costs to bill internal consumers.",
      "summary": "This document provides a step-by-step guide for configuring percentage-based or fixed-amount price margins on LLM providers using the LiteLLM UI. These margins facilitate internal chargebacks and operational cost recovery for enterprise AI platforms.",
      "tags": [
        "litellm-ui",
        "cost-tracking",
        "price-margins",
        "internal-chargebacks",
        "cost-recovery",
        "billing-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-provider-margins.md"
    },
    {
      "file_path": "364-docs-proxy-public-teams.md",
      "title": "[BETA] Public Teams | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/public_teams",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:23.200152963-03:00",
      "description": "Expose available teams to your users to join on signup.",
      "summary": "This guide explains how to configure LiteLLM to expose specific teams to users for selection during the signup or SSO onboarding process.",
      "tags": [
        "litellm",
        "team-management",
        "user-signup",
        "sso-integration",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-public-teams.md"
    },
    {
      "file_path": "365-docs-proxy-rate-limit-tiers.md",
      "title": "âœ¨ Budget / Rate Limit Tiers | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/rate_limit_tiers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:25.54865934-03:00",
      "description": "Define tiers with rate limits. Assign them to keys.",
      "summary": "This document provides instructions on defining tiers with rate limits and assigning these budgets to API keys for centralized access and resource control.",
      "tags": [
        "litellm",
        "rate-limiting",
        "budget-management",
        "api-keys",
        "enterprise-features",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-rate-limit-tiers.md"
    },
    {
      "file_path": "366-docs-proxy-reliability.md",
      "title": "Fallbacks | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/reliability",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:30.051835763-03:00",
      "description": "If a call fails after num_retries, fallback to another model group.",
      "summary": "This document explains how to configure and manage model fallbacks in LiteLLM to handle API failures, content policy violations, and context window limits.",
      "tags": [
        "litellm",
        "model-fallbacks",
        "error-handling",
        "llm-routing",
        "configuration",
        "reliability"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-reliability.md"
    },
    {
      "file_path": "367-docs-proxy-rules.md",
      "title": "Post-Call Rules | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/rules",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:35.131897637-03:00",
      "description": "Use this to fail a request based on the output of an llm api call.",
      "summary": "This document explains how to implement custom post-call validation rules in LiteLLM Proxy to evaluate and potentially reject LLM responses based on user-defined criteria.",
      "tags": [
        "litellm",
        "proxy-rules",
        "post-call-validation",
        "response-filtering",
        "custom-logic",
        "error-handling"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-rules.md"
    },
    {
      "file_path": "368-docs-proxy-self-serve.md",
      "title": "Internal User Self-Serve | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/self_serve",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:36.433616461-03:00",
      "description": "Allow users to create their own keys on Proxy UI.",
      "summary": "This document explains how to manage user roles, permissions, and automated team provisioning within the LiteLLM Proxy UI, including SSO integration and budget controls.",
      "tags": [
        "litellm-proxy",
        "user-management",
        "sso-integration",
        "access-control",
        "budget-management",
        "team-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-self-serve.md"
    },
    {
      "file_path": "369-docs-proxy-server.md",
      "title": "[OLD PROXY ðŸ‘‰ [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy_server",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:06.880730757-03:00",
      "description": "A fast, and lightweight OpenAI-compatible server to call 100+ LLM APIs.",
      "summary": "This document explains how to set up and use LiteLLM Proxy, an OpenAI-compatible server that enables standardized access to over 100 different LLM providers.",
      "tags": [
        "litellm",
        "openai-compatibility",
        "llm-proxy",
        "api-gateway",
        "model-deployment",
        "local-inference",
        "python"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-server.md"
    },
    {
      "file_path": "370-docs-proxy-service-accounts.md",
      "title": "[Beta] Service Accounts | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/service_accounts",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:36.43630751-03:00",
      "description": "Use this if you want to create Virtual Keys that are not owned by a specific user but instead created for production projects",
      "summary": "This document explains how to create and manage service account keys for production projects to ensure key persistence and apply team-based usage limits. It provides instructions for generating keys via API, configuring specific settings, and enforcing request parameters.",
      "tags": [
        "service-account",
        "api-key-management",
        "litellm-proxy",
        "access-control",
        "authentication",
        "team-limits"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-service-accounts.md"
    },
    {
      "file_path": "371-docs-proxy-shared-health-check.md",
      "title": "Shared Health Check State Across Pods | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/shared_health_check",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:38.6953655-03:00",
      "description": "This feature enables coordination of health checks across multiple LiteLLM proxy pods to avoid duplicate health checks and reduce costs.",
      "summary": "This document explains how to coordinate health checks across multiple LiteLLM proxy pods using Redis to prevent duplicate checks and reduce operational costs.",
      "tags": [
        "litellm",
        "health-checks",
        "redis",
        "distributed-systems",
        "kubernetes",
        "configuration",
        "performance-optimization"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-shared-health-check.md"
    },
    {
      "file_path": "372-docs-proxy-sync-models-github.md",
      "title": "Auto Sync New Models (Day-0 Launches) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/sync_models_github",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:41.930603373-03:00",
      "description": "Automatically keep your model pricing and context window data up to date without restarting your service. This allows you to add day-0 support for new models without restarting your service.",
      "summary": "This document explains how to use LiteLLM's auto-sync feature to update model pricing and context window data dynamically without restarting the service.",
      "tags": [
        "litellm",
        "model-pricing",
        "auto-sync",
        "dynamic-configuration",
        "api-management",
        "zero-downtime"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-sync-models-github.md"
    },
    {
      "file_path": "373-docs-proxy-tag-budgets.md",
      "title": "Setting Tag Budgets | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/tag_budgets",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:42.920519004-03:00",
      "description": "Track spend and set budgets for your API requests using tags. Tags allow you to categorize and monitor costs across different cost centers, projects, and departments.",
      "summary": "This document explains how to use tags to monitor costs and enforce budget limits for LLM API requests across different projects, departments, or customers.",
      "tags": [
        "cost-tracking",
        "budget-management",
        "llm-usage",
        "api-proxy",
        "usage-limits",
        "tagging-system"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-tag-budgets.md"
    },
    {
      "file_path": "374-docs-proxy-tag-routing.md",
      "title": "Tag Based Routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/tag_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:44.14301711-03:00",
      "description": "Route requests based on tags.",
      "summary": "This document explains how to implement tag-based request routing in LiteLLM Proxy to manage model access for different user tiers or teams.",
      "tags": [
        "litellm",
        "tag-routing",
        "request-routing",
        "access-control",
        "proxy-configuration",
        "team-management"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-tag-routing.md"
    },
    {
      "file_path": "375-docs-proxy-team-based-routing.md",
      "title": "[DEPRECATED] Team-based Routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/team_based_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:44.611847669-03:00",
      "description": "This is deprecated, please use Tag Based Routing instead",
      "summary": "This document explains how to implement team-based model routing and aliases within the LiteLLM proxy configuration to direct calls to specific model groups.",
      "tags": [
        "litellm",
        "model-routing",
        "team-management",
        "model-aliases",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-team-based-routing.md"
    },
    {
      "file_path": "376-docs-proxy-team-budgets.md",
      "title": "Setting Team Budgets | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/team_budgets",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:45.026279995-03:00",
      "description": "- You must set up a Postgres database (e.g. Supabase, Neon, etc.)",
      "summary": "This document explains how to configure and enforce budget limits for teams in LiteLLM, including setup for JWT-based auto-generation and monitoring via Prometheus metrics.",
      "tags": [
        "litellm",
        "team-management",
        "budget-limits",
        "cost-control",
        "usage-tracking",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-team-budgets.md"
    },
    {
      "file_path": "377-docs-proxy-team-logging.md",
      "title": "Team/Key Based Logging | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/team_logging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:46.986598703-03:00",
      "description": "Overview",
      "summary": "This document explains how to configure team-specific and key-specific logging callbacks to enable granular control over LLM request logs and compliance requirements across different projects and providers.",
      "tags": [
        "logging-callbacks",
        "team-management",
        "litellm-proxy",
        "api-key-configuration",
        "compliance-logging",
        "langfuse-integration",
        "custom-callbacks"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-team-logging.md"
    },
    {
      "file_path": "378-docs-proxy-team-model-add.md",
      "title": "âœ¨ Allow Teams to Add Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/team_model_add",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:47.310624336-03:00",
      "description": "This is an Enterprise feature.",
      "summary": "This document explains how to allow teams to register their own custom models and API keys using the model creation endpoint and team-specific identifiers.",
      "tags": [
        "model-management",
        "team-configuration",
        "api-integration",
        "custom-models",
        "litellm-proxy",
        "model-routing"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-team-model-add.md"
    },
    {
      "file_path": "379-docs-proxy-temporary-budget-increase.md",
      "title": "âœ¨ Temporary Budget Increase | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/temporary_budget_increase",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:48.852311902-03:00",
      "description": "Set temporary budget increase for a LiteLLM Virtual Key. Use this if you get asked to increase the budget for a key temporarily.",
      "summary": "This document provides instructions on how to create a LiteLLM Virtual Key and apply a temporary budget increase with an expiration date.",
      "tags": [
        "litellm",
        "virtual-key",
        "budget-management",
        "api-configuration",
        "cost-control",
        "key-management"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-temporary-budget-increase.md"
    },
    {
      "file_path": "380-docs-proxy-token-auth.md",
      "title": "OIDC - JWT-based Auth | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/token_auth",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:53.010343971-03:00",
      "description": "Use JWT's to auth admins / users / projects into the proxy.",
      "summary": "This document explains how to configure and use JWT authentication for the LiteLLM proxy, including integration with OpenID providers and Kubernetes ServiceAccounts.",
      "tags": [
        "jwt-auth",
        "oidc-integration",
        "litellm-proxy",
        "kubernetes-auth",
        "security-configuration",
        "access-control"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-token-auth.md"
    },
    {
      "file_path": "381-docs-proxy-ui-bulk-edit-users.md",
      "title": "Bulk Edit Users | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ui/bulk_edit_users",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:00.108652545-03:00",
      "description": "Assign existing users to a default team and default model access.",
      "summary": "This document explains how to perform bulk edits to assign multiple existing users to a specific team and manage their default model access.",
      "tags": [
        "user-management",
        "bulk-edit",
        "team-assignment",
        "model-access",
        "admin-controls"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-ui-bulk-edit-users.md"
    },
    {
      "file_path": "382-docs-proxy-ui-credentials.md",
      "title": "Adding LLM Credentials | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ui_credentials",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:53.802547236-03:00",
      "description": "You can add LLM provider credentials on the UI. Once you add credentials you can reuse them when adding new models",
      "summary": "This document provides step-by-step instructions for adding, reusing, and managing LLM provider credentials within the user interface. It also explains how these credentials are encrypted and stored for security purposes.",
      "tags": [
        "llm-credentials",
        "api-keys",
        "provider-management",
        "encryption",
        "litellm",
        "ui-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-ui-credentials.md"
    },
    {
      "file_path": "383-docs-proxy-ui-logs-sessions.md",
      "title": "Session Logs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ui_logs_sessions",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:56.474131806-03:00",
      "description": "Group requests into sessions. This allows you to group related requests together.",
      "summary": "This document explains how to group related API requests into sessions using session IDs or previous response IDs to track and link conversation history.",
      "tags": [
        "litellm",
        "session-management",
        "request-grouping",
        "api-integration",
        "python-sdk"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-ui-logs-sessions.md"
    },
    {
      "file_path": "384-docs-proxy-user-onboarding.md",
      "title": "User Onboarding Guide | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/user_onboarding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:03.090766957-03:00",
      "description": "A step-by-step guide to help admins onboard users to your LiteLLM proxy instance and help users get started with their API key.",
      "summary": "This document provides a comprehensive guide for administrators to onboard users and for end users to start using their API keys with a LiteLLM proxy instance. It covers user creation, permissions management, API key validation, and making initial LLM calls.",
      "tags": [
        "litellm-proxy",
        "user-onboarding",
        "api-key-management",
        "user-management",
        "access-control",
        "llm-api"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-user-onboarding.md"
    },
    {
      "file_path": "385-docs-proxy-veo-video-generation.md",
      "title": "Veo Video Generation with Google AI Studio | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/veo_video_generation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:05.386984562-03:00",
      "description": "Generate videos using Google's Veo model through LiteLLM's pass-through endpoints.",
      "summary": "This document provides a Python script demonstrating the workflow for generating videos with the Gemini Veo 3.0 model via a LiteLLM proxy, including polling and downloading.",
      "tags": [
        "python",
        "video-generation",
        "gemini-api",
        "litellm",
        "rest-api",
        "asynchronous-processing"
      ],
      "category": "tutorial",
      "original_file_path": "docs-proxy-veo-video-generation.md"
    },
    {
      "file_path": "386-docs-proxy-virtual-keys.md",
      "title": "Virtual Keys | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/virtual_keys",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:06.737789112-03:00",
      "description": "Track Spend, and control model access via virtual keys for the proxy",
      "summary": "Explains how to set up and manage virtual keys for the LiteLLM Proxy to track usage spend and control model access through a PostgreSQL database.",
      "tags": [
        "litellm",
        "api-proxy",
        "virtual-keys",
        "spend-tracking",
        "model-aliases",
        "authentication",
        "cost-management"
      ],
      "category": "guide",
      "original_file_path": "docs-proxy-virtual-keys.md"
    },
    {
      "file_path": "387-docs-reasoning-content.md",
      "title": "'Thinking' / 'Reasoning Content' | liteLLM",
      "url": "https://docs.litellm.ai/docs/reasoning_content",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:10.846921519-03:00",
      "description": "Requires LiteLLM v1.63.0+",
      "summary": "This document provides a code implementation for executing function calls using the LiteLLM library, demonstrating the workflow of defining tools and processing their results.",
      "tags": [
        "litellm",
        "function-calling",
        "tool-use",
        "python-sdk",
        "llm-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-reasoning-content.md"
    },
    {
      "file_path": "388-docs-rerank.md",
      "title": "/rerank | liteLLM",
      "url": "https://docs.litellm.ai/docs/rerank",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:11.71056617-03:00",
      "description": "LiteLLM Follows the cohere api request / response for the rerank api",
      "summary": "This document provides instructions for implementing document reranking across multiple providers using LiteLLM's Python SDK and API proxy. It covers setup, asynchronous usage, and configuration for features like load balancing and cost tracking.",
      "tags": [
        "litellm",
        "rerank",
        "python-sdk",
        "api-proxy",
        "search-optimization"
      ],
      "category": "guide",
      "original_file_path": "docs-rerank.md"
    },
    {
      "file_path": "389-docs-response-api.md",
      "title": "/responses | liteLLM",
      "url": "https://docs.litellm.ai/docs/response_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:12.6517756-03:00",
      "description": "LiteLLM provides an endpoint in the spec of OpenAI's /responses API",
      "summary": "This document demonstrates how to configure and use LiteLLM's Router to ensure consistent routing to the same model deployment across sequential requests using response IDs.",
      "tags": [
        "litellm",
        "model-routing",
        "azure-openai",
        "sticky-sessions",
        "responses-api",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "docs-response-api.md"
    },
    {
      "file_path": "390-docs-routing.md",
      "title": "Router - Load Balancing | liteLLM",
      "url": "https://docs.litellm.ai/docs/routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:17.159090421-03:00",
      "description": "LiteLLM manages:",
      "summary": "This document explains how to use the LiteLLM Router to manage load balancing, failover, and reliability across multiple LLM providers and deployments.",
      "tags": [
        "litellm",
        "load-balancing",
        "model-routing",
        "high-availability",
        "llm-ops",
        "api-reliability",
        "request-queueing"
      ],
      "category": "guide",
      "original_file_path": "docs-routing.md"
    },
    {
      "file_path": "391-docs-rules.md",
      "title": "Rules | liteLLM",
      "url": "https://docs.litellm.ai/docs/rules",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:20.439730734-03:00",
      "description": "Use this to fail a request based on the input or output of an llm api call.",
      "summary": "This document explains how to implement custom pre-call and post-call validation rules in LiteLLM to filter inputs and responses or trigger model fallbacks.",
      "tags": [
        "litellm",
        "validation-rules",
        "error-handling",
        "llm-api",
        "python-library",
        "fallback-mechanisms"
      ],
      "category": "guide",
      "original_file_path": "docs-rules.md"
    },
    {
      "file_path": "392-docs-scheduler.md",
      "title": "[BETA] Request Prioritization | liteLLM",
      "url": "https://docs.litellm.ai/docs/scheduler",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:22.20509491-03:00",
      "description": "Beta feature. Use for testing only.",
      "summary": "This document explains how to implement request prioritization in LiteLLM to manage high-traffic LLM API calls using a priority queue system. It provides configuration instructions for both the Python SDK and LiteLLM Proxy, including advanced Redis-based setups for multi-instance environments.",
      "tags": [
        "litellm",
        "request-prioritization",
        "llm-api",
        "priority-queue",
        "redis-caching",
        "load-balancing",
        "api-proxy"
      ],
      "category": "guide",
      "original_file_path": "docs-scheduler.md"
    },
    {
      "file_path": "393-docs-sdk-custom-pricing.md",
      "title": "Custom Pricing - SageMaker, Azure, etc | liteLLM",
      "url": "https://docs.litellm.ai/docs/sdk_custom_pricing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:23.996424303-03:00",
      "description": "Register custom pricing for sagemaker completion model.",
      "summary": "This document explains how to manually register and track custom pricing for SageMaker and Azure models by passing cost parameters directly to the completion function.",
      "tags": [
        "litellm",
        "sagemaker",
        "azure",
        "custom-pricing",
        "cost-tracking",
        "api-parameters"
      ],
      "category": "guide",
      "original_file_path": "docs-sdk-custom-pricing.md"
    },
    {
      "file_path": "394-docs-search-dataforseo.md",
      "title": "DataForSEO Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/dataforseo",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:25.637155334-03:00",
      "description": "Get API Access: DataForSEO",
      "summary": "This document explains how to integrate and use the DataForSEO search provider with LiteLLM via the Python SDK and AI Gateway.",
      "tags": [
        "dataforseo",
        "litellm",
        "search-api",
        "python-sdk",
        "ai-gateway",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-search-dataforseo.md"
    },
    {
      "file_path": "395-docs-search-exa-ai.md",
      "title": "Exa AI Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/exa_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:28.260590587-03:00",
      "description": "Get API Key//exa.ai",
      "summary": "This document provides instructions and code examples for integrating Exa AI Search with LiteLLM using both the Python SDK and the AI Gateway proxy.",
      "tags": [
        "litellm",
        "exa-ai",
        "search-api",
        "ai-gateway",
        "python-sdk",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-search-exa-ai.md"
    },
    {
      "file_path": "396-docs-search-firecrawl.md",
      "title": "Firecrawl Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/firecrawl",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:29.767596716-03:00",
      "description": "Get API Key//firecrawl.dev",
      "summary": "This code example demonstrates how to use the LiteLLM search function with Firecrawl as the provider, including configuration for advanced filtering, geo-targeting, and content scraping.",
      "tags": [
        "litellm",
        "firecrawl",
        "search-api",
        "python-sdk",
        "web-scraping"
      ],
      "category": "guide",
      "original_file_path": "docs-search-firecrawl.md"
    },
    {
      "file_path": "397-docs-search-google-pse.md",
      "title": "Google Programmable Search Engine (PSE) | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/google_pse",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:30.269789388-03:00",
      "description": "Get API Key: Google Cloud Console",
      "summary": "This document demonstrates how to use the litellm library to perform web searches using the Google Programmable Search Engine with advanced filtering parameters.",
      "tags": [
        "litellm",
        "google-pse",
        "search-api",
        "python-library",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-search-google-pse.md"
    },
    {
      "file_path": "398-docs-search-parallel-ai.md",
      "title": "Parallel AI Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/parallel_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:32.091144282-03:00",
      "description": "Get API Key//www.parallel.ai",
      "summary": "This document provides instructions for integrating Parallel AI Search with LiteLLM using the Python SDK and the AI Gateway, including configuration steps and provider-specific parameters.",
      "tags": [
        "litellm",
        "parallel-ai",
        "python-sdk",
        "ai-gateway",
        "search-api",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-search-parallel-ai.md"
    },
    {
      "file_path": "399-docs-search-perplexity.md",
      "title": "Perplexity AI Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/perplexity",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:32.29004043-03:00",
      "description": "Get API Key//www.perplexity.ai/settings/api",
      "summary": "This document provides instructions for integrating Perplexity Search into LiteLLM using the Python SDK and the AI Gateway configuration.",
      "tags": [
        "litellm",
        "perplexity-ai",
        "search-api",
        "ai-gateway",
        "python-sdk",
        "api-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-search-perplexity.md"
    },
    {
      "file_path": "400-docs-search-searxng.md",
      "title": "SearXNG Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/searxng",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:35.368032205-03:00",
      "description": "Open Source//github.com/searxng/searxng",
      "summary": "This document provides instructions on integrating the SearXNG metasearch engine with LiteLLM using the Python SDK and AI Gateway. It covers configuration for self-hosted instances, parameter usage, and advanced search features while maintaining user privacy.",
      "tags": [
        "searxng",
        "litellm",
        "metasearch-engine",
        "search-api",
        "self-hosting",
        "ai-gateway"
      ],
      "category": "guide",
      "original_file_path": "docs-search-searxng.md"
    },
    {
      "file_path": "401-docs-search-tavily.md",
      "title": "Tavily Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/tavily",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:35.924576231-03:00",
      "description": "Get API Key//tavily.com",
      "summary": "This document provides instructions and code examples for integrating Tavily Search using the LiteLLM Python SDK and AI Gateway.",
      "tags": [
        "litellm",
        "tavily-search",
        "python-sdk",
        "ai-gateway",
        "search-api",
        "api-integration"
      ],
      "category": "guide",
      "original_file_path": "docs-search-tavily.md"
    },
    {
      "file_path": "402-docs-secret-managers-custom-secret-manager.md",
      "title": "Custom Secret Manager | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/custom_secret_manager",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:42.178700313-03:00",
      "description": "Integrate your custom secret management system with LiteLLM.",
      "summary": "This document provides instructions for integrating custom secret management systems with LiteLLM by implementing a Python class and updating proxy configurations.",
      "tags": [
        "litellm",
        "secret-management",
        "custom-integration",
        "python",
        "proxy-configuration",
        "security"
      ],
      "category": "guide",
      "original_file_path": "docs-secret-managers-custom-secret-manager.md"
    },
    {
      "file_path": "403-docs-secret-managers-cyberark.md",
      "title": "CyberArk Conjur | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/cyberark",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:42.89863933-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This guide provides instructions for integrating CyberArk Conjur with LiteLLM to manage, read, and write secrets and virtual keys.",
      "tags": [
        "cyberark-conjur",
        "secrets-management",
        "litellm-proxy",
        "key-management",
        "authentication",
        "virtual-keys"
      ],
      "category": "guide",
      "original_file_path": "docs-secret-managers-cyberark.md"
    },
    {
      "file_path": "404-docs-secret-managers-google-kms.md",
      "title": "Google Key Management Service | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/google_kms",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:45.135968277-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This guide explains how to configure and use Google KMS for managing encrypted environment variables and database URLs within the LiteLLM proxy server.",
      "tags": [
        "google-kms",
        "litellm",
        "encryption",
        "proxy-configuration",
        "security",
        "key-management"
      ],
      "category": "guide",
      "original_file_path": "docs-secret-managers-google-kms.md"
    },
    {
      "file_path": "405-docs-skills.md",
      "title": "/skills - Anthropic Skills API | liteLLM",
      "url": "https://docs.litellm.ai/docs/skills",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:50.927552547-03:00",
      "description": "| Feature | Supported |",
      "summary": "This document explains how to create, manage, and use reusable AI capabilities via the Anthropic Skills API using the LiteLLM Python SDK and LiteLLM Proxy.",
      "tags": [
        "litellm",
        "anthropic-skills",
        "python-sdk",
        "api-proxy",
        "skills-management",
        "llm-routing"
      ],
      "category": "guide",
      "original_file_path": "docs-skills.md"
    },
    {
      "file_path": "406-docs-text-to-speech.md",
      "title": "/audio/speech | liteLLM",
      "url": "https://docs.litellm.ai/docs/text_to_speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:54.294561025-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for implementing and configuring Text-to-Speech (TTS) functionality through LiteLLM's Python SDK and Proxy server across various AI providers.",
      "tags": [
        "litellm",
        "text-to-speech",
        "tts",
        "python-sdk",
        "api-proxy",
        "gemini",
        "vertex-ai",
        "async-usage"
      ],
      "category": "guide",
      "original_file_path": "docs-text-to-speech.md"
    },
    {
      "file_path": "407-docs-troubleshoot-cpu-issues.md",
      "title": "CPU Issue Classification & Reproduction | liteLLM",
      "url": "https://docs.litellm.ai/docs/troubleshoot/cpu_issues",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:57.159848014-03:00",
      "description": "1. Classify the CPU Issue",
      "summary": "This document provides a framework for identifying, reproducing, and reporting CPU performance issues in LiteLLM, ensuring developers provide the necessary context for technical support.",
      "tags": [
        "litellm",
        "cpu-usage",
        "performance-troubleshooting",
        "debugging-guide",
        "issue-reporting",
        "system-monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-troubleshoot-cpu-issues.md"
    },
    {
      "file_path": "408-docs-troubleshoot-memory-issues.md",
      "title": "Memory Issue Classification & Reproduction | liteLLM",
      "url": "https://docs.litellm.ai/docs/troubleshoot/memory_issues",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:57.579784046-03:00",
      "description": "1. Classify the Memory Issue",
      "summary": "This document outlines a structured approach for identifying, reproducing, and reporting memory issues such as leaks and OOM events in LiteLLM.",
      "tags": [
        "litellm",
        "memory-management",
        "troubleshooting",
        "oom-events",
        "debugging",
        "performance-monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-troubleshoot-memory-issues.md"
    },
    {
      "file_path": "409-docs-troubleshoot.md",
      "title": "Troubleshooting & Support | liteLLM",
      "url": "https://docs.litellm.ai/docs/troubleshoot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:55.610303516-03:00",
      "description": "Information to Provide When Seeking Help",
      "summary": "This document outlines the specific information required to effectively report issues with LiteLLM and lists available support channels for technical assistance.",
      "tags": [
        "troubleshooting",
        "issue-reporting",
        "debugging",
        "customer-support",
        "litellm-configuration",
        "error-logs"
      ],
      "category": "guide",
      "original_file_path": "docs-troubleshoot.md"
    },
    {
      "file_path": "410-docs-tutorials-anthropic-file-usage.md",
      "title": "Using Anthropic File API with LiteLLM Proxy | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/anthropic_file_usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:58.046170873-03:00",
      "description": "Overview",
      "summary": "This tutorial explains how to upload files and perform data analysis using Claude-4 through the LiteLLM Proxy's Anthropic passthrough and chat completion endpoints.",
      "tags": [
        "litellm-proxy",
        "anthropic-claude",
        "file-management",
        "data-analysis",
        "api-passthrough"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-anthropic-file-usage.md"
    },
    {
      "file_path": "411-docs-tutorials-azure-openai.md",
      "title": "Replacing OpenAI ChatCompletion with Completion() | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/azure_openai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:02.075275253-03:00",
      "description": "* Supported OpenAI LLMs",
      "summary": "This document provides a quick-start guide for implementing chat completions using LiteLLM with OpenAI and Azure OpenAI across various modes including streaming, async, and multi-threading.",
      "tags": [
        "litellm",
        "openai",
        "azure-openai",
        "streaming",
        "async-completion",
        "multi-threading",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-azure-openai.md"
    },
    {
      "file_path": "412-docs-tutorials-claude-code-customer-tracking.md",
      "title": "Claude Code - Granular Cost Tracking | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_code_customer_tracking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:02.044194091-03:00",
      "description": "Track Claude Code usage by customer or tags using LiteLLM proxy. This enables granular cost attribution for billing, budgeting, and analytics.",
      "summary": "This document explains how to track Claude Code usage and attribute costs to specific customers or projects using LiteLLM proxy and custom headers.",
      "tags": [
        "claude-code",
        "litellm-proxy",
        "cost-attribution",
        "usage-tracking",
        "custom-headers",
        "monitoring"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-claude-code-customer-tracking.md"
    },
    {
      "file_path": "413-docs-tutorials-claude-code-max-subscription.md",
      "title": "Using Claude Code Max Subscription | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_code_max_subscription",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:02.225735975-03:00",
      "description": "Route Claude Code Max subscription traffic through LiteLLM AI Gateway.",
      "summary": "This document provides a step-by-step guide for routing Claude Code Max subscription traffic through LiteLLM Gateway to enable cost attribution, rate limiting, and centralized governance.",
      "tags": [
        "litellm",
        "claude-code",
        "ai-gateway",
        "cost-tracking",
        "anthropic",
        "proxy-configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-claude-code-max-subscription.md"
    },
    {
      "file_path": "414-docs-tutorials-claude-code-plugin-marketplace.md",
      "title": "Claude Code Plugin Marketplace | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_code_plugin_marketplace",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:03.021784724-03:00",
      "description": "LiteLLM AI Gateway acts as a central registry for Claude Code plugins. Admins can govern which plugins are available across the organization, and engineers can discover and install approved plugins from a single source.",
      "summary": "This document explains how to set up and manage a centralized Claude Code plugin marketplace using LiteLLM, covering administrator management, engineer installation workflows, and associated API endpoints.",
      "tags": [
        "litellm",
        "claude-code",
        "plugin-management",
        "marketplace-registry",
        "api-gateway",
        "plugin-governance"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-claude-code-plugin-marketplace.md"
    },
    {
      "file_path": "415-docs-tutorials-claude-code-websearch.md",
      "title": "Claude Code - WebSearch Across All Providers | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_code_websearch",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:05.196146451-03:00",
      "description": "Enable Claude Code's web search tool to work with any provider (Bedrock, Azure, Vertex, etc.). LiteLLM automatically intercepts web search requests and executes them server-side.",
      "summary": "This document explains how to configure LiteLLM to enable web search capabilities for Claude Code when using non-Anthropic providers like AWS Bedrock and Azure. It provides instructions for intercepting search requests and routing them through various search tool providers.",
      "tags": [
        "litellm",
        "claude-code",
        "web-search",
        "proxy-configuration",
        "search-interception",
        "aws-bedrock",
        "azure-openai"
      ],
      "category": "configuration",
      "original_file_path": "docs-tutorials-claude-code-websearch.md"
    },
    {
      "file_path": "416-docs-tutorials-claude-mcp.md",
      "title": "Use Claude Code with MCPs | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_mcp",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:06.123554678-03:00",
      "description": "This tutorial shows how to connect MCP servers to Claude Code via LiteLLM Proxy.",
      "summary": "This tutorial provides a step-by-step guide on connecting and authenticating Model Context Protocol (MCP) servers with Claude Code using the LiteLLM Proxy.",
      "tags": [
        "mcp",
        "litellm-proxy",
        "claude-code",
        "oauth",
        "server-integration",
        "configuration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-claude-mcp.md"
    },
    {
      "file_path": "417-docs-tutorials-claude-non-anthropic-models.md",
      "title": "Use Claude Code with Non-Anthropic Models | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/claude_non_anthropic_models",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:09.811400895-03:00",
      "description": "This tutorial shows how to use Claude Code with non-Anthropic models like OpenAI, Gemini, and other LLM providers through LiteLLM proxy.",
      "summary": "This tutorial explains how to integrate Claude Code with non-Anthropic LLM providers using LiteLLM as a proxy to translate requests into the Anthropic Messages API format.",
      "tags": [
        "claude-code",
        "litellm",
        "api-proxy",
        "llm-integration",
        "openai",
        "gemini",
        "multi-model"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-claude-non-anthropic-models.md"
    },
    {
      "file_path": "418-docs-tutorials-compare-llms-2.md",
      "title": "Comparing LLMs on a Test Set using LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/compare_llms_2",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:11.337489267-03:00",
      "description": "LiteLLM allows you to use any LLM as a drop in replacement for",
      "summary": "This document demonstrates how to use the LiteLLM library to call multiple large language models using a unified completion function format.",
      "tags": [
        "litellm",
        "multi-model",
        "python-sdk",
        "api-standardization",
        "llm-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-compare-llms-2.md"
    },
    {
      "file_path": "419-docs-tutorials-compare-llms.md",
      "title": "Benchmark LLMs | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/compare_llms",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:10.850393161-03:00",
      "description": "Easily benchmark LLMs for a given question by viewing",
      "summary": "This document provides instructions for setting up and running performance benchmarks to compare response times, costs, and outputs across various LLM models using LiteLLM.",
      "tags": [
        "litellm",
        "benchmarking",
        "llm-performance",
        "latency-testing",
        "cost-analysis",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-compare-llms.md"
    },
    {
      "file_path": "420-docs-tutorials-cost-tracking-coding.md",
      "title": "Track Usage for Coding Tools | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/cost_tracking_coding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:12.043749504-03:00",
      "description": "Track usage and costs for AI-powered coding tools like Claude Code, Roo Code, Gemini CLI, and OpenAI Codex through LiteLLM.",
      "summary": "This document explains how to track and monitor usage, costs, and engagement for AI-powered coding tools by using User-Agent headers with the LiteLLM proxy.",
      "tags": [
        "litellm-proxy",
        "usage-tracking",
        "cost-monitoring",
        "user-agent",
        "ai-coding-tools",
        "analytics-dashboard"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-cost-tracking-coding.md"
    },
    {
      "file_path": "421-docs-tutorials-cursor-integration.md",
      "title": "Cursor Integration | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/cursor_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:15.284819781-03:00",
      "description": "Route Cursor IDE requests through LiteLLM for unified logging, budget controls, and access to any model.",
      "summary": "This document provides instructions for routing Cursor IDE requests through a LiteLLM proxy to enable unified logging, budget controls, and access to custom AI models.",
      "tags": [
        "cursor-ide",
        "litellm",
        "proxy-configuration",
        "mcp-server",
        "api-integration",
        "virtual-keys"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-cursor-integration.md"
    },
    {
      "file_path": "422-docs-tutorials-default-team-self-serve.md",
      "title": "Onboard Users for AI Exploration | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/default_team_self_serve",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:16.350939547-03:00",
      "description": "v1.73.0 introduces the ability to assign new users to Default Teams. This makes it much easier to enable experimentation with LLMs within your company, by allowing users to sign in and create $10 keys for AI exploration.",
      "summary": "This document explains how to configure default teams to automatically assign new users, allowing them to create API keys within specific model and budget constraints.",
      "tags": [
        "litellm",
        "default-teams",
        "user-onboarding",
        "access-control",
        "budget-management",
        "api-keys"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-default-team-self-serve.md"
    },
    {
      "file_path": "423-docs-tutorials-elasticsearch-logging.md",
      "title": "Elasticsearch Logging with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/elasticsearch_logging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:16.556443264-03:00",
      "description": "Send your LLM requests, responses, costs, and performance data to Elasticsearch for analytics and monitoring using OpenTelemetry.",
      "summary": "This document provides a step-by-step guide for integrating LiteLLM with Elasticsearch via OpenTelemetry to monitor and analyze LLM request performance, costs, and traces.",
      "tags": [
        "litellm",
        "elasticsearch",
        "opentelemetry",
        "monitoring",
        "observability",
        "kibana",
        "otel-collector"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-elasticsearch-logging.md"
    },
    {
      "file_path": "424-docs-tutorials-eval-suites.md",
      "title": "Evaluate LLMs - MLflow Evals, Auto Eval | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/eval_suites",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:17.271152921-03:00",
      "description": "Using LiteLLM with MLflow",
      "summary": "This document explains how to integrate LiteLLM with evaluation tools like MLflow and AutoEvals to benchmark and test large language models using a unified OpenAI-compatible proxy.",
      "tags": [
        "litellm",
        "mlflow",
        "autoevals",
        "llm-evaluation",
        "model-testing",
        "openai-proxy"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-eval-suites.md"
    },
    {
      "file_path": "425-docs-tutorials-fallbacks.md",
      "title": "Using completion() with Fallbacks for Reliability | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/fallbacks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:19.367132338-03:00",
      "description": "This tutorial demonstrates how to employ the completion() function with model fallbacks to ensure reliability. LLM APIs can be unstable, completion() with fallbacks ensures you'll always get a response from your calls",
      "summary": "This document explains how to use model fallbacks and error handling within the completion function to ensure high reliability when calling LLM APIs. It covers implementation details like looping through backup models and managing cooldown periods for rate-limited providers.",
      "tags": [
        "model-fallbacks",
        "error-handling",
        "api-reliability",
        "llm-integration",
        "python",
        "rate-limiting"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-fallbacks.md"
    },
    {
      "file_path": "426-docs-tutorials-finetuned-chat-gpt.md",
      "title": "Using Fine-Tuned gpt-3.5-turbo | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/finetuned_chat_gpt",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:20.916584779-03:00",
      "description": "LiteLLM allows you to call completion with your fine-tuned gpt-3.5-turbo models",
      "summary": "This document explains how to use the LiteLLM library to call fine-tuned OpenAI models and configure environment variables such as the API key and Organization ID.",
      "tags": [
        "litellm",
        "openai",
        "fine-tuned-models",
        "api-configuration",
        "python-sdk",
        "environment-variables"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-finetuned-chat-gpt.md"
    },
    {
      "file_path": "427-docs-tutorials-first-playground.md",
      "title": "Create your first LLM playground | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/first_playground",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:21.536432291-03:00",
      "description": "Create a playground to evaluate multiple LLM Providers in less than 10 minutes. If you want to see this in prod, check out our website.",
      "summary": "This tutorial provides a step-by-step guide to building a functional LLM playground by setting up a Flask backend with LiteLLM and connecting it to a Streamlit frontend template.",
      "tags": [
        "litellm",
        "llm-playground",
        "flask",
        "streamlit",
        "python",
        "multi-model",
        "api-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-first-playground.md"
    },
    {
      "file_path": "428-docs-tutorials-gemini-realtime-with-audio.md",
      "title": "Call Gemini Realtime API with Audio Input/Output | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/gemini_realtime_with_audio",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:23.120008539-03:00",
      "description": "Requires LiteLLM Proxy v1.70.1+",
      "summary": "This document provides a Python implementation for streaming audio data to a realtime WebSocket API and processing the returned audio responses.",
      "tags": [
        "python",
        "websockets",
        "realtime-api",
        "audio-streaming",
        "multimodal-llm",
        "openai-realtime-api"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-gemini-realtime-with-audio.md"
    },
    {
      "file_path": "429-docs-tutorials-github-copilot-integration.md",
      "title": "GitHub Copilot | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/github_copilot_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:23.783572435-03:00",
      "description": "This tutorial shows you how to integrate GitHub Copilot with LiteLLM Proxy, allowing you to route requests through LiteLLM's unified interface.",
      "summary": "This tutorial explains how to route GitHub Copilot requests through LiteLLM Proxy to access various LLM providers, manage costs, and implement load balancing.",
      "tags": [
        "github-copilot",
        "litellm-proxy",
        "llm-gateway",
        "model-routing",
        "vscode-configuration",
        "multi-model"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-github-copilot-integration.md"
    },
    {
      "file_path": "430-docs-tutorials-google-adk.md",
      "title": "Google ADK with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/google_adk",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:25.12248147-03:00",
      "description": "<Image",
      "summary": "This tutorial explains how to build intelligent agents using the Google Agent Development Kit (ADK) and LiteLLM to support multiple large language model providers like OpenAI and Anthropic.",
      "tags": [
        "google-adk",
        "litellm",
        "llm-agents",
        "python-sdk",
        "multi-model-support"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-google-adk.md"
    },
    {
      "file_path": "431-docs-tutorials-gradio-integration.md",
      "title": "Gradio Chatbot + LiteLLM Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/gradio_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:27.439693235-03:00",
      "description": "Simple tutorial for integrating LiteLLM completion calls with streaming Gradio chatbot demos",
      "summary": "This document provides a brief tutorial on integrating LiteLLM streaming completion calls into a Gradio-based chatbot interface.",
      "tags": [
        "litellm",
        "gradio",
        "chatbot",
        "streaming",
        "python",
        "llm-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-gradio-integration.md"
    },
    {
      "file_path": "432-docs-tutorials-huggingface-codellama.md",
      "title": "CodeLlama - Code Infilling | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/huggingface_codellama",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:28.008405717-03:00",
      "description": "This tutorial shows how you can call CodeLlama (hosted on Huggingface PRO Inference Endpoints), to fill code.",
      "summary": "This tutorial explains how to use LiteLLM to perform code infilling tasks with CodeLlama models hosted on Hugging Face Inference Endpoints.",
      "tags": [
        "code-infilling",
        "litellm",
        "codellama",
        "hugging-face-inference",
        "python",
        "code-generation"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-huggingface-codellama.md"
    },
    {
      "file_path": "433-docs-tutorials-huggingface-tutorial.md",
      "title": "Llama2 - Huggingface Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/huggingface_tutorial",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:28.077433179-03:00",
      "description": "Huggingface is an open source platform to deploy machine-learnings models.",
      "summary": "This document provides instructions on how to use LiteLLM to call models via Hugging Face inference endpoints, covering default, public, and private endpoint configurations.",
      "tags": [
        "huggingface",
        "litellm",
        "inference-endpoints",
        "llama2",
        "api-integration",
        "python-client"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-huggingface-tutorial.md"
    },
    {
      "file_path": "434-docs-tutorials-instructor.md",
      "title": "Instructor | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/instructor",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:30.83929352-03:00",
      "description": "Combine LiteLLM with jxnl's instructor library for more robust structured outputs. Outputs are automatically validated into Pydantic types and validation errors are provided back to the model to increase the chance of a successful response in the retries.",
      "summary": "This document explains how to integrate LiteLLM with the instructor library to generate validated structured outputs using Pydantic models with support for retries.",
      "tags": [
        "litellm",
        "instructor-library",
        "pydantic",
        "structured-outputs",
        "python",
        "validation",
        "error-handling"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-instructor.md"
    },
    {
      "file_path": "435-docs-tutorials-litellm-gemini-cli.md",
      "title": "Gemini CLI | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/litellm_gemini_cli",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:32.531049856-03:00",
      "description": "This tutorial shows you how to integrate the Gemini CLI with LiteLLM Proxy, allowing you to route requests through LiteLLM's unified interface.",
      "summary": "This tutorial explains how to integrate the Gemini CLI with LiteLLM Proxy to enable unified model access, centralized management, and model routing across various providers.",
      "tags": [
        "gemini-cli",
        "litellm-proxy",
        "model-routing",
        "api-gateway",
        "load-balancing",
        "llm-ops"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-litellm-gemini-cli.md"
    },
    {
      "file_path": "436-docs-tutorials-litellm-proxy-aporia.md",
      "title": "Aporia Guardrails with LiteLLM Gateway | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/litellm_proxy_aporia",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:34.369576942-03:00",
      "description": "In this tutorial we will use LiteLLM AI Gateway with Aporia to detect PII in requests and profanity in responses",
      "summary": "This tutorial explains how to integrate LiteLLM AI Gateway with Aporia guardrails to detect PII in prompts and profanity in model responses.",
      "tags": [
        "litellm",
        "aporia",
        "guardrails",
        "pii-detection",
        "ai-security",
        "llm-monitoring",
        "content-filtering"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-litellm-proxy-aporia.md"
    },
    {
      "file_path": "437-docs-tutorials-litellm-qwen-code-cli.md",
      "title": "Qwen Code CLI | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/litellm_qwen_code_cli",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:35.170576198-03:00",
      "description": "This tutorial shows you how to integrate the Qwen Code CLI with LiteLLM Proxy, allowing you to route requests through LiteLLM's unified interface.",
      "summary": "This tutorial provides instructions for integrating the Qwen Code CLI with LiteLLM Proxy to manage model access, routing, and costs through a unified interface.",
      "tags": [
        "qwen-code",
        "litellm",
        "proxy-integration",
        "cli",
        "model-routing",
        "load-balancing"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-litellm-qwen-code-cli.md"
    },
    {
      "file_path": "438-docs-tutorials-litellm-Test-Multiple-Providers.md",
      "title": "Reliability test Multiple LLM Providers with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/litellm_Test_Multiple_Providers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:36.266163078-03:00",
      "description": "*   Quality Testing",
      "summary": "This document demonstrates how to perform batch completion and load testing across multiple LLM providers to evaluate performance, latency, and reliability.",
      "tags": [
        "llm-benchmarking",
        "load-testing",
        "latency-analysis",
        "python",
        "performance-monitoring",
        "api-testing"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-litellm-Test-Multiple-Providers.md"
    },
    {
      "file_path": "439-docs-tutorials-lm-evaluation-harness.md",
      "title": "Benchmark LLMs - LM Harness, FastEval, Flask | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/lm_evaluation_harness",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:36.062741857-03:00",
      "description": "LM Harness Benchmarks",
      "summary": "This document provides step-by-step instructions for running various LLM benchmarks using the LiteLLM proxy server to enable compatibility with OpenAI-based evaluation tools.",
      "tags": [
        "llm-benchmarking",
        "litellm-proxy",
        "lm-eval-harness",
        "fasteval",
        "flask-evaluation",
        "model-evaluation"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-lm-evaluation-harness.md"
    },
    {
      "file_path": "440-docs-tutorials-mock-completion.md",
      "title": "Mock Completion Responses - Save Testing Costs | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/mock_completion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:37.089487275-03:00",
      "description": "Trying to test making LLM Completion calls without calling the LLM APIs ?",
      "summary": "This document explains how to use the mock_response parameter in LiteLLM to simulate API responses for testing purposes without making actual network calls or incurring costs.",
      "tags": [
        "litellm",
        "mock-response",
        "unit-testing",
        "api-mocking",
        "python",
        "completion-api"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-mock-completion.md"
    },
    {
      "file_path": "441-docs-tutorials-model-config-proxy.md",
      "title": "Customize Prompt Templates on OpenAI-Compatible server | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/model_config_proxy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:38.860479567-03:00",
      "description": "You will learn: How to set a custom prompt template on our OpenAI compatible server.",
      "summary": "This tutorial explains how to define and implement custom prompt templates for specific models within a LiteLLM OpenAI-compatible server. It covers creating a configuration file to specify start and end tokens for system, user, and assistant messages to ensure correct model formatting.",
      "tags": [
        "litellm",
        "prompt-templates",
        "llm-configuration",
        "openai-proxy",
        "codellama",
        "huggingface-tgi"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-model-config-proxy.md"
    },
    {
      "file_path": "442-docs-tutorials-model-fallbacks.md",
      "title": "Model Fallbacks w/ LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/model_fallbacks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:39.517674956-03:00",
      "description": "Here's how you can implement model fallbacks across 3 LLM providers (OpenAI, Anthropic, Azure) using LiteLLM.",
      "summary": "This document demonstrates how to implement model fallbacks and handle context window exceptions across multiple LLM providers using the LiteLLM library.",
      "tags": [
        "litellm",
        "model-fallback",
        "error-handling",
        "context-window",
        "llm-integration"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-model-fallbacks.md"
    },
    {
      "file_path": "443-docs-tutorials-msft-sso.md",
      "title": "Microsoft SSO: Sync Groups, Members with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/msft_sso",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:40.877339466-03:00",
      "description": "Sync Microsoft SSO Groups, Members with LiteLLM Teams.",
      "summary": "This document provides instructions for synchronizing Microsoft Entra ID groups and memberships with LiteLLM teams to automate user provisioning and role-based access control. It covers auto-creating teams, syncing members during SSO sign-in, and configuring default parameters for new teams.",
      "tags": [
        "microsoft-entra-id",
        "azure-ad",
        "sso-integration",
        "user-provisioning",
        "team-sync",
        "role-based-access-control"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-msft-sso.md"
    },
    {
      "file_path": "444-docs-tutorials-oobabooga.md",
      "title": "Oobabooga Text Web API Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/oobabooga",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:42.521082145-03:00",
      "description": "Install + Import LiteLLM",
      "summary": "This document explains how to integrate LiteLLM with a local Oobabooga model server to perform text completion tasks. It covers installation, basic configuration, and API usage for calling local LLMs.",
      "tags": [
        "litellm",
        "oobabooga",
        "local-llm",
        "python-api",
        "text-generation",
        "llm-inference"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-oobabooga.md"
    },
    {
      "file_path": "445-docs-tutorials-openai-codex.md",
      "title": "OpenAI Codex | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/openai_codex",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:43.34062639-03:00",
      "description": "This guide walks you through connecting OpenAI Codex to LiteLLM. Using LiteLLM with Codex allows teams to:",
      "summary": "This guide provides step-by-step instructions for integrating OpenAI Codex with LiteLLM, allowing users to route Codex requests to various LLM providers and track usage through a central proxy.",
      "tags": [
        "litellm",
        "openai-codex",
        "llm-proxy",
        "model-routing",
        "devops",
        "api-configuration",
        "multi-llm"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-openai-codex.md"
    },
    {
      "file_path": "446-docs-tutorials-openweb-ui.md",
      "title": "Open WebUI | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/openweb_ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:44.111118672-03:00",
      "description": "This guide walks you through connecting Open WebUI to LiteLLM. Using LiteLLM with Open WebUI allows teams to",
      "summary": "This guide explains how to integrate Open WebUI with LiteLLM to manage access to multiple LLMs, track usage and costs, and configure advanced model features like reasoning output.",
      "tags": [
        "open-webui",
        "litellm",
        "llm-proxy",
        "usage-tracking",
        "api-key-management",
        "user-management",
        "cost-tracking"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-openweb-ui.md"
    },
    {
      "file_path": "447-docs-tutorials-presidio-pii-masking.md",
      "title": "Presidio PII Masking with LiteLLM - Complete Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/presidio_pii_masking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:45.047566365-03:00",
      "description": "This tutorial will guide you through setting up PII (Personally Identifiable Information) masking with Microsoft Presidio and LiteLLM Gateway. By the end of this tutorial, you'll have a production-ready setup that automatically detects and masks sensitive information in your LLM requests.",
      "summary": "This tutorial explains how to integrate Microsoft Presidio with LiteLLM Gateway to automatically detect, mask, or block personally identifiable information (PII) in language model interactions. It covers Docker deployment, YAML configuration for redaction strategies, and advanced features like output parsing for unmasking sensitive data.",
      "tags": [
        "pii-masking",
        "litellm-gateway",
        "microsoft-presidio",
        "data-privacy",
        "llm-security",
        "guardrails",
        "content-redaction"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-presidio-pii-masking.md"
    },
    {
      "file_path": "448-docs-tutorials-prompt-caching.md",
      "title": "Auto-Inject Prompt Caching Checkpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/prompt_caching",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:47.791022964-03:00",
      "description": "Reduce costs by up to 90% by using LiteLLM to auto-inject prompt caching checkpoints.",
      "summary": "This document explains how to use LiteLLM's auto-injection feature to automatically insert prompt caching checkpoints into LLM requests for cost reduction. It details configuration methods via the Python SDK and proxy settings to target specific message roles or indices.",
      "tags": [
        "litellm",
        "prompt-caching",
        "cost-optimization",
        "anthropic-claude",
        "llm-proxy",
        "caching-directives",
        "performance-tuning"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-prompt-caching.md"
    },
    {
      "file_path": "449-docs-tutorials-provider-specific-params.md",
      "title": "provider_specific_params | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/provider_specific_params",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:49.472723835-03:00",
      "description": "Setting provider-specific Params",
      "summary": "This document explains how LiteLLM automatically maps and translates model parameters like max_tokens across different providers using its completion function and provider-specific configurations.",
      "tags": [
        "litellm",
        "parameter-mapping",
        "max-tokens",
        "llm-integration",
        "python-library",
        "model-configuration"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-provider-specific-params.md"
    },
    {
      "file_path": "450-docs-tutorials-scim-litellm.md",
      "title": "SCIM with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/scim_litellm",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:50.213308549-03:00",
      "description": "âœ¨ Enterprise: SCIM support requires a premium license.",
      "summary": "This tutorial provides instructions for connecting identity providers to LiteLLM SCIM endpoints to automate user and team provisioning. It explains how to generate credentials, configure the identity provider, and verify the connection through SSO sign-in.",
      "tags": [
        "scim",
        "sso",
        "identity-management",
        "user-provisioning",
        "enterprise-features",
        "litellm"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-scim-litellm.md"
    },
    {
      "file_path": "451-docs-tutorials-tag-management.md",
      "title": "[Beta] Routing based on request metadata | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/tag_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:50.376327816-03:00",
      "description": "Create routing rules based on request metadata.",
      "summary": "This document explains how to configure and use tag-based routing in the LiteLLM proxy to control which models are available based on request metadata. It covers the setup process in the configuration file, tag management via the UI, and testing routing rules using the OpenAI SDK.",
      "tags": [
        "litellm-proxy",
        "tag-routing",
        "access-control",
        "model-routing",
        "configuration-guide",
        "request-metadata"
      ],
      "category": "guide",
      "original_file_path": "docs-tutorials-tag-management.md"
    },
    {
      "file_path": "452-docs-tutorials-text-completion.md",
      "title": "Using Text Completion Format - with Completion() | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/text_completion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:52.706748357-03:00",
      "description": "If your prefer interfacing with the OpenAI Text Completion format this tutorial covers how to use LiteLLM in this format",
      "summary": "This tutorial explains how to use LiteLLM to interface with various language models using the OpenAI Text Completion format. It provides implementation examples for models like GPT-3.5 and Llama-2 using the text_completion function.",
      "tags": [
        "litellm",
        "text-completion",
        "openai-api-format",
        "python-library",
        "llm-integration",
        "gpt-3-5",
        "llama-2"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-text-completion.md"
    },
    {
      "file_path": "453-docs-tutorials-TogetherAI-liteLLM.md",
      "title": "Llama2 Together AI Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/docs/tutorials/TogetherAI_liteLLM",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:52.930128435-03:00",
      "description": "https://together.ai/",
      "summary": "This document explains how to use LiteLLM to interact with Together AI models, covering basic completion calls, streaming responses, and the configuration of custom prompt templates for specific Llama2 variants.",
      "tags": [
        "litellm",
        "together-ai",
        "llama-2",
        "prompt-templates",
        "streaming",
        "python"
      ],
      "category": "tutorial",
      "original_file_path": "docs-tutorials-TogetherAI-liteLLM.md"
    },
    {
      "file_path": "454-docs-vertex-batch-passthrough.md",
      "title": "/batchPredictionJobs | liteLLM",
      "url": "https://docs.litellm.ai/docs/vertex_batch_passthrough",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:56.972885514-03:00",
      "description": "LiteLLM supports Vertex AI batch prediction jobs through passthrough endpoints, allowing you to create and manage batch jobs directly through the proxy server.",
      "summary": "This document explains how to configure and use LiteLLM to manage Vertex AI batch prediction jobs, including job creation, status monitoring, and automated cost tracking.",
      "tags": [
        "vertex-ai",
        "batch-prediction",
        "litellm",
        "cost-tracking",
        "gemini",
        "api-passthrough"
      ],
      "category": "guide",
      "original_file_path": "docs-vertex-batch-passthrough.md"
    },
    {
      "file_path": "455-docs-videos.md",
      "title": "/videos | liteLLM",
      "url": "https://docs.litellm.ai/docs/videos",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:58.200476492-03:00",
      "description": "| Feature | Supported |",
      "summary": "This document provides instructions and code examples for using LiteLLM to generate and manage videos using its Python SDK and Proxy server across multiple AI providers.",
      "tags": [
        "litellm",
        "video-generation",
        "python-sdk",
        "proxy-server",
        "ai-video-api",
        "azure-openai",
        "sora-2"
      ],
      "category": "guide",
      "original_file_path": "docs-videos.md"
    },
    {
      "file_path": "456-observability-callbacks.md",
      "title": "Callbacks | liteLLM",
      "url": "https://docs.litellm.ai/observability/callbacks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:09.351590259-03:00",
      "description": "Use Callbacks to send Output Data to Posthog, Sentry etc",
      "summary": "This document explains how to configure success and failure callbacks in liteLLM to automatically send completion data and error reports to external monitoring and logging providers.",
      "tags": [
        "litellm",
        "callbacks",
        "monitoring",
        "logging",
        "observability",
        "error-tracking"
      ],
      "category": "guide",
      "original_file_path": "observability-callbacks.md"
    },
    {
      "file_path": "457-observability-helicone-integration.md",
      "title": "Helicone Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/observability/helicone_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:10.795486736-03:00",
      "description": "Helicone is an open source observability platform that proxies your OpenAI traffic and provides you key insights into your spend, latency and usage.",
      "summary": "This document explains how to integrate LiteLLM with Helicone to monitor and log LLM requests across various providers using success callbacks or proxy configurations.",
      "tags": [
        "helicone",
        "litellm",
        "observability",
        "llm-logging",
        "callbacks",
        "proxy-configuration"
      ],
      "category": "guide",
      "original_file_path": "observability-helicone-integration.md"
    },
    {
      "file_path": "458-observability-supabase-integration.md",
      "title": "Supabase Tutorial | liteLLM",
      "url": "https://docs.litellm.ai/observability/supabase_integration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:11.94402682-03:00",
      "description": "Supabase is an open source Firebase alternative.",
      "summary": "This document explains how to integrate Supabase with liteLLM to log request data and track costs across multiple LLM providers using success and failure callbacks. It provides instructions for database schema setup, environment configuration, and advanced logging controls.",
      "tags": [
        "supabase",
        "litellm",
        "request-logging",
        "cost-tracking",
        "llm-monitoring",
        "python",
        "database-integration"
      ],
      "category": "guide",
      "original_file_path": "observability-supabase-integration.md"
    },
    {
      "file_path": "459-release-notes-tags-deepgram.md",
      "title": "One post tagged with \"deepgram\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/deepgram",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:34.830964141-03:00",
      "summary": "This document provides instructions on performing audio transcription with Deepgram using LiteLLM and explains the document inlining feature for Fireworks AI models.",
      "tags": [
        "litellm",
        "audio-transcription",
        "deepgram",
        "fireworks-ai",
        "document-parsing"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-deepgram.md"
    },
    {
      "file_path": "460-release-notes-tags-dependency-upgrades.md",
      "title": "One post tagged with \"dependency upgrades\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/dependency-upgrades",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:35.548652577-03:00",
      "summary": "This document demonstrates how to perform audio transcription using LiteLLM with Deepgram and explains the implementation of document inlining for Fireworks AI models.",
      "tags": [
        "litellm",
        "audio-transcription",
        "deepgram",
        "fireworks-ai",
        "document-inlining",
        "python"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-dependency-upgrades.md"
    },
    {
      "file_path": "461-release-notes-tags-docker-image.md",
      "title": "One post tagged with \"docker image\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/docker-image",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:36.014144707-03:00",
      "summary": "This document explains the update of the LiteLLM Docker base image to a Chainguard image to eliminate security vulnerabilities and provides migration steps for users to switch from apt-get to apk.",
      "tags": [
        "docker-image",
        "security",
        "vulnerability-management",
        "litellm",
        "migration-guide",
        "package-management"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-docker-image.md"
    },
    {
      "file_path": "462-release-notes-tags-fireworks-ai.md",
      "title": "One post tagged with \"fireworks ai\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/fireworks-ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:40.924291984-03:00",
      "summary": "This document explains how to perform audio transcription using LiteLLM with Deepgram and describes document inlining support for Fireworks AI models.",
      "tags": [
        "litellm",
        "transcription",
        "deepgram",
        "fireworks-ai",
        "document-inlining",
        "audio-processing"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-fireworks-ai.md"
    },
    {
      "file_path": "463-release-notes-tags-virtual-key-management.md",
      "title": "One post tagged with \"virtual key management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/virtual-key-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:18.14384962-03:00",
      "summary": "This document explains how to monitor guardrail performance, list available guardrails, and test guardrail configurations using mock responses within the LiteLLM proxy.",
      "tags": [
        "litellm",
        "guardrails",
        "api-monitoring",
        "mock-responses",
        "llm-proxy",
        "request-validation"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-virtual-key-management.md"
    },
    {
      "file_path": "464-release-notes-tags-vision.md",
      "title": "One post tagged with \"vision\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/vision",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:22.102178596-03:00",
      "summary": "This document demonstrates how to use LiteLLM for audio transcription via Deepgram and explains the implementation of document inlining for Fireworks AI models.",
      "tags": [
        "litellm",
        "audio-transcription",
        "deepgram",
        "fireworks-ai",
        "document-inlining",
        "python-sdk"
      ],
      "category": "tutorial",
      "original_file_path": "release-notes-tags-vision.md"
    },
    {
      "file_path": "465-release-notes-tags-vulnerability.md",
      "title": "One post tagged with \"vulnerability\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/vulnerability",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:22.697224811-03:00",
      "summary": "This document explains the transition of the LiteLLM base Docker image to a Chainguard-based image to eliminate security vulnerabilities and provides instructions for migrating custom Dockerfiles from apt-get to apk.",
      "tags": [
        "docker-image",
        "security",
        "vulnerability-management",
        "litellm",
        "migration-guide"
      ],
      "category": "guide",
      "original_file_path": "release-notes-tags-vulnerability.md"
    },
    {
      "file_path": "466-release-notes-v1.57.3.md",
      "title": "v1.57.3 - New Base Docker Image",
      "url": "https://docs.litellm.ai/release_notes/v1.57.3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:15.382877348-03:00",
      "description": "docker image, security, vulnerability",
      "summary": "This document announces the migration of the LiteLLM base Docker image to a Chainguard Python image to ensure zero critical vulnerabilities and provides instructions for updating custom Dockerfiles.",
      "tags": [
        "docker-image",
        "security",
        "vulnerability-management",
        "litellm",
        "migration-guide",
        "chainguard"
      ],
      "category": "guide",
      "original_file_path": "release-notes-v1.57.3.md"
    },
    {
      "file_path": "467-stream.md",
      "title": "Streaming Responses & Async Completion | liteLLM",
      "url": "https://docs.litellm.ai/stream",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:41.760331138-03:00",
      "description": "- Streaming Responses",
      "summary": "This document explains how to implement streaming responses, asynchronous completion, and token usage tracking within the LiteLLM library.",
      "tags": [
        "litellm",
        "streaming",
        "async-completion",
        "token-usage",
        "python-sdk",
        "llm-api"
      ],
      "category": "guide",
      "original_file_path": "stream.md"
    },
    {
      "file_path": "468-troubleshoot.md",
      "title": "Troubleshooting | liteLLM",
      "url": "https://docs.litellm.ai/troubleshoot",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:47.864391543-03:00",
      "description": "Stable Version",
      "summary": "This document provides instructions for installing a specific stable version of the litellm library to resolve potential installation or usage issues.",
      "tags": [
        "litellm",
        "installation",
        "stable-version",
        "pip-install",
        "troubleshooting",
        "package-management"
      ],
      "category": "guide",
      "original_file_path": "troubleshoot.md"
    },
    {
      "file_path": "469-docs-projects-Agent-Lightning.md",
      "title": "Agent Lightning | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Agent%20Lightning",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:04.866680016-03:00",
      "description": "Agent Lightning is Microsoft's open-source framework for training and optimizing AI agents with Reinforcement Learning, Automatic Prompt Optimization, and Supervised Fine-tuning â€” with almost zero code changes.",
      "summary": "Agent Lightning is an open-source Microsoft framework for training and optimizing AI agents using techniques like reinforcement learning and automatic prompt optimization across various existing agent platforms.",
      "tags": [
        "ai-agents",
        "reinforcement-learning",
        "prompt-optimization",
        "microsoft",
        "fine-tuning",
        "agent-optimization"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-Agent-Lightning.md"
    },
    {
      "file_path": "470-docs-projects-GraphRAG.md",
      "title": "Microsoft GraphRAG | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/GraphRAG",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:18.554165871-03:00",
      "description": "GraphRAG is a data pipeline and transformation suite that extracts meaningful, structured data from unstructured text using the power of LLMs. It uses a graph-based approach to RAG (Retrieval-Augmented Generation) that leverages knowledge graphs to improve reasoning over private datasets.",
      "summary": "This document introduces GraphRAG, a data pipeline and transformation suite that uses LLMs and knowledge graphs to extract structured data from unstructured text for improved retrieval-augmented generation.",
      "tags": [
        "graphrag",
        "knowledge-graph",
        "retrieval-augmented-generation",
        "llm",
        "data-pipeline",
        "information-extraction"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-GraphRAG.md"
    },
    {
      "file_path": "471-docs-projects-HolmesGPT.md",
      "title": "HolmesGPT | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/HolmesGPT",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:19.407512453-03:00",
      "description": "HolmesGPT is an AI-powered observability tool designed to enhance incident response and troubleshooting processes. It's like your 24/7 on-call assistant, helps you solve alerts faster with Automatic Correlations, Investigations, and More.",
      "summary": "This document introduces HolmesGPT, an AI-powered observability assistant for incident response that integrates with LiteLLM to support multiple large language model providers.",
      "tags": [
        "observability",
        "incident-response",
        "ai-assistant",
        "holmesgpt",
        "litellm",
        "troubleshooting"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-HolmesGPT.md"
    },
    {
      "file_path": "472-docs-projects-OpenInterpreter.md",
      "title": "OpenInterpreter | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/OpenInterpreter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:29.817596027-03:00",
      "description": "Open Interpreter lets LLMs run code on your computer to complete tasks.",
      "summary": "This document introduces Open Interpreter, an open-source tool that enables large language models to execute code locally to complete various tasks.",
      "tags": [
        "open-interpreter",
        "llm",
        "code-execution",
        "automation",
        "python",
        "ai-tools"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-OpenInterpreter.md"
    },
    {
      "file_path": "473-docs-projects-Otter.md",
      "title": "Otter | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Otter",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:30.590332052-03:00",
      "description": "ðŸ¦¦ Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following and in-context learning ability.",
      "summary": "This document introduces Otter, a multi-modal artificial intelligence model derived from OpenFlamingo that emphasizes improved instruction-following and in-context learning performance.",
      "tags": [
        "multi-modal-model",
        "open-flamingo",
        "instruction-following",
        "in-context-learning",
        "mimic-it",
        "machine-learning"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-Otter.md"
    },
    {
      "file_path": "474-docs-projects-PDL.md",
      "title": "PDL | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/PDL",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:30.626199164-03:00",
      "description": "PDL - A YAML-based approach to prompt programming",
      "summary": "This document introduces the Prompt Declaration Language (PDL), a declarative YAML-based approach for prompt programming that supports model chaining and tool integration.",
      "tags": [
        "prompt-programming",
        "yaml",
        "declarative-language",
        "model-chaining",
        "llm-tool-use",
        "prompt-engineering"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-PDL.md"
    },
    {
      "file_path": "475-docs-projects-YiVal.md",
      "title": "YiVal | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/YiVal",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:40.294670338-03:00",
      "description": "ðŸš€ Evaluate and Evolve.ðŸš€ YiVal is an open source GenAI-Ops framework that allows you to manually or automatically tune and evaluate your AIGC prompts, retrieval configs and fine-tune the model params all at once with your preferred choices of test dataset generation, evaluation algorithms and improvement strategies.",
      "summary": "YiVal is an open-source GenAI-Ops framework designed to automate the tuning and evaluation of AIGC prompts, retrieval configurations, and model parameters.",
      "tags": [
        "genai-ops",
        "prompt-engineering",
        "model-evaluation",
        "aigc",
        "open-source",
        "retrieval-config"
      ],
      "category": "concept",
      "original_file_path": "docs-projects-YiVal.md"
    },
    {
      "file_path": "476-docs-proxy-access-control.md",
      "title": "Role-based Access Controls (RBAC) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/access_control",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:07.222560169-03:00",
      "description": "Role-based access control (RBAC) is based on Organizations, Teams and Internal User Roles",
      "summary": "This document explains the hierarchical role-based access control (RBAC) system in LiteLLM, defining the relationships between organizations, teams, and users. It details the specific permissions for global and organization-specific roles as well as the different types of virtual keys available for API authentication.",
      "tags": [
        "rbac",
        "litellm",
        "access-control",
        "user-roles",
        "virtual-keys",
        "team-management",
        "identity-management"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-access-control.md"
    },
    {
      "file_path": "477-docs-proxy-architecture.md",
      "title": "Life of a Request | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/architecture",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:13.829660994-03:00",
      "description": "High Level architecture",
      "summary": "This document outlines the high-level architecture and request flow of the LiteLLM Proxy Server, detailing the sequence of authentication, rate limiting, routing, and asynchronous post-request processing.",
      "tags": [
        "litellm",
        "proxy-server",
        "architecture",
        "request-flow",
        "rate-limiting",
        "llm-routing",
        "authentication"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-architecture.md"
    },
    {
      "file_path": "478-docs-proxy-image-handling.md",
      "title": "Image URL Handling | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/image_handling",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:42.907580261-03:00",
      "description": "Some LLM API's don't support url's for images, but do support base-64 strings.",
      "summary": "Explains how LiteLLM automatically converts image URLs to base64 strings for incompatible LLM APIs and manages this process using in-memory caching.",
      "tags": [
        "litellm",
        "image-processing",
        "base64-conversion",
        "caching",
        "multimodal-api"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-image-handling.md"
    },
    {
      "file_path": "479-docs-proxy-multi-tenant-architecture.md",
      "title": "Multi-Tenant Architecture with LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/multi_tenant_architecture",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:05.648992145-03:00",
      "description": "Overview",
      "summary": "This document explains LiteLLM's hierarchical multi-tenant architecture for managing LLM access, cost attribution, and isolation across organizations, teams, and users.",
      "tags": [
        "multi-tenancy",
        "litellm",
        "access-control",
        "rbac",
        "cost-tracking",
        "organizational-hierarchy"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-multi-tenant-architecture.md"
    },
    {
      "file_path": "480-docs-proxy-user-management-heirarchy.md",
      "title": "User Management Hierarchy | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/user_management_heirarchy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:02.360586533-03:00",
      "description": "LiteLLM supports a hierarchy of users, teams, organizations, and budgets.",
      "summary": "This document describes the hierarchical relationship between organizations, teams, users, and budgets in LiteLLM, including links to their respective management APIs.",
      "tags": [
        "litellm",
        "user-hierarchy",
        "organization-management",
        "team-management",
        "budget-tracking",
        "access-control"
      ],
      "category": "concept",
      "original_file_path": "docs-proxy-user-management-heirarchy.md"
    },
    {
      "file_path": "481-docs-router-architecture.md",
      "title": "Router Architecture (Fallbacks / Retries) | liteLLM",
      "url": "https://docs.litellm.ai/docs/router_architecture",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:16.140961426-03:00",
      "description": "High Level architecture",
      "summary": "This document describes the high-level architecture and internal request flow of the LiteLLM Router, detailing how requests are processed through fallback and retry mechanisms.",
      "tags": [
        "litellm",
        "router-architecture",
        "request-flow",
        "error-handling",
        "load-balancing",
        "llm-infrastructure"
      ],
      "category": "concept",
      "original_file_path": "docs-router-architecture.md"
    },
    {
      "file_path": "482-docs-completion-drop-params.md",
      "title": "Drop Unsupported Params | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/drop_params",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:22.942890689-03:00",
      "description": "Drop unsupported OpenAI params by your LLM Provider.",
      "summary": "This document explains how to manage unsupported OpenAI parameters in LiteLLM by dropping them automatically, manually specifying fields for removal, or force-allowing specific parameters for cross-provider compatibility.",
      "tags": [
        "litellm",
        "parameter-management",
        "openai-compatibility",
        "llm-proxy",
        "python-sdk",
        "error-handling"
      ],
      "category": "configuration",
      "original_file_path": "docs-completion-drop-params.md"
    },
    {
      "file_path": "483-docs-completion-http-handler-config.md",
      "title": "Custom HTTP Handler | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/http_handler_config",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:26.40334982-03:00",
      "description": "Configure custom aiohttp sessions for better performance and control in LiteLLM completions.",
      "summary": "This document explains how to configure and inject custom aiohttp sessions into LiteLLM to optimize performance, manage connection pooling, and handle corporate proxy settings.",
      "tags": [
        "litellm",
        "aiohttp",
        "python",
        "performance-optimization",
        "async-io",
        "api-configuration",
        "connection-pooling"
      ],
      "category": "configuration",
      "original_file_path": "docs-completion-http-handler-config.md"
    },
    {
      "file_path": "484-docs-completion-model-alias.md",
      "title": "Model Alias | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/model_alias",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:35.403810071-03:00",
      "description": "The model name you show an end-user might be different from the one you pass to LiteLLM - e.g. Displaying GPT-3.5 while calling gpt-3.5-turbo-16k on the backend.",
      "summary": "This document explains how to use LiteLLM's model alias mapping feature to link user-friendly display names to specific backend model identifiers. It allows developers to simplify model invocation and manage model versioning through a centralized mapping.",
      "tags": [
        "litellm",
        "model-aliasing",
        "python",
        "llm-api",
        "configuration-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-completion-model-alias.md"
    },
    {
      "file_path": "485-docs-completion-usage.md",
      "title": "Usage | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:47.125713965-03:00",
      "description": "LiteLLM returns the OpenAI compatible usage object across all providers.",
      "summary": "This document explains how to retrieve token usage statistics in LiteLLM when using streaming responses by enabling the include_usage option in stream_options.",
      "tags": [
        "litellm",
        "token-usage",
        "streaming",
        "openai-compatibility",
        "usage-statistics"
      ],
      "category": "configuration",
      "original_file_path": "docs-completion-usage.md"
    },
    {
      "file_path": "486-docs-mcp-control.md",
      "title": "MCP Permission Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp_control",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:40.597649004-03:00",
      "description": "Control which MCP servers and tools can be accessed by specific keys, teams, or organizations in LiteLLM. When a client attempts to list or call tools, LiteLLM enforces access controls based on configured permissions.",
      "summary": "This document explains how to configure and manage access controls for MCP servers in LiteLLM, covering tool-level filtering, parameter restrictions, and entity-based permission management.",
      "tags": [
        "mcp-server",
        "access-control",
        "permissions",
        "security",
        "tool-filtering",
        "litellm-config",
        "parameter-control"
      ],
      "category": "configuration",
      "original_file_path": "docs-mcp-control.md"
    },
    {
      "file_path": "487-docs-observability-callbacks.md",
      "title": "Callbacks | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/callbacks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:57.657870397-03:00",
      "description": "Use Callbacks to send Output Data to Posthog, Sentry etc",
      "summary": "This document explains how to configure input, success, and failure callbacks in liteLLM to integrate with various monitoring and logging providers.",
      "tags": [
        "litellm",
        "callbacks",
        "logging",
        "monitoring",
        "integrations",
        "error-handling"
      ],
      "category": "configuration",
      "original_file_path": "docs-observability-callbacks.md"
    },
    {
      "file_path": "488-docs-observability-generic-api.md",
      "title": "Generic API Callback (Webhook) | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/generic_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:07.179757717-03:00",
      "description": "Send LiteLLM logs to any HTTP endpoint.",
      "summary": "This document explains how to configure and use LiteLLM's generic HTTP callbacks to send logs to external endpoints with support for various log formats and batching settings.",
      "tags": [
        "litellm",
        "http-callbacks",
        "logging",
        "api-integration",
        "log-formatting",
        "batching",
        "ndjson"
      ],
      "category": "configuration",
      "original_file_path": "docs-observability-generic-api.md"
    },
    {
      "file_path": "489-docs-old-guardrails.md",
      "title": "ðŸ›¡ï¸ [Beta] Guardrails | liteLLM",
      "url": "https://docs.litellm.ai/docs/old_guardrails",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:44.251572138-03:00",
      "description": "Setup Prompt Injection Detection, Secret Detection on LiteLLM Proxy",
      "summary": "This document provides instructions for configuring security guardrails on LiteLLM Proxy, including prompt injection detection, PII masking, and secret detection through YAML configuration and API key permissions.",
      "tags": [
        "litellm",
        "security-guardrails",
        "prompt-injection",
        "pii-masking",
        "secret-detection",
        "llm-proxy",
        "configuration-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-old-guardrails.md"
    },
    {
      "file_path": "490-docs-provider-registration-add-model-pricing.md",
      "title": "Add Model Pricing & Context Window | liteLLM",
      "url": "https://docs.litellm.ai/docs/provider_registration/add_model_pricing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:42.090473922-03:00",
      "description": "To add pricing or context window information for a model, simply make a PR to this file:",
      "summary": "This document provides the schema and instructions for contributing model metadata, including pricing, token limits, and supported features, to a model configuration file.",
      "tags": [
        "model-metadata",
        "pricing-configuration",
        "token-limits",
        "litellm",
        "json-schema"
      ],
      "category": "configuration",
      "original_file_path": "docs-provider-registration-add-model-pricing.md"
    },
    {
      "file_path": "491-docs-providers-anthropic-effort.md",
      "title": "Anthropic Effort Parameter | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anthropic_effort",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:52.150069331-03:00",
      "description": "Control how many tokens Claude uses when responding with the effort parameter, trading off between response thoroughness and token efficiency.",
      "summary": "This document explains how to use the effort parameter with Claude Opus 4.5 to balance response thoroughness against token efficiency, speed, and cost.",
      "tags": [
        "claude-opus-4-5",
        "token-optimization",
        "reasoning-effort",
        "litellm",
        "cost-management",
        "anthropic-api"
      ],
      "category": "configuration",
      "original_file_path": "docs-providers-anthropic-effort.md"
    },
    {
      "file_path": "492-docs-proxy-admin-ui-sso.md",
      "title": "âœ¨ SSO for Admin UI | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/admin_ui_sso",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:08.994230115-03:00",
      "description": "From v1.76.0, SSO is now Free for up to 5 users.",
      "summary": "This document provides instructions for configuring Single Sign-On (SSO) with providers like Okta, Google, and Microsoft, including environment variable setup, UI customization, and troubleshooting.",
      "tags": [
        "sso-configuration",
        "okta-setup",
        "authentication",
        "litellm-proxy",
        "ui-customization",
        "identity-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-admin-ui-sso.md"
    },
    {
      "file_path": "493-docs-proxy-budget-reset-and-tz.md",
      "title": "budget_reset_and_tz | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/budget_reset_and_tz",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:17.988833011-03:00",
      "description": "Budget Reset Times and Timezones",
      "summary": "This document explains how LiteLLM handles automatic budget reset schedules and provides instructions for configuring the local timezone for these resets.",
      "tags": [
        "litellm",
        "budget-management",
        "timezone-configuration",
        "usage-limits",
        "billing-cycles"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-budget-reset-and-tz.md"
    },
    {
      "file_path": "494-docs-proxy-caching.md",
      "title": "Caching | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/caching",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:18.229019026-03:00",
      "description": "For OpenAI/Anthropic Prompt Caching, go here",
      "summary": "This document explains how to configure and use LiteLLM's caching systems to store and reuse LLM responses, reducing latency and operational costs. It details setup procedures for various backends like Redis and S3, alongside dynamic per-request cache controls and environment variable configurations.",
      "tags": [
        "litellm",
        "caching",
        "redis-config",
        "semantic-cache",
        "latency-optimization",
        "response-caching",
        "llm-performance"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-caching.md"
    },
    {
      "file_path": "495-docs-proxy-clientside-auth.md",
      "title": "Clientside LLM Credentials | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/clientside_auth",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:22.275923083-03:00",
      "description": "Pass User LLM API Keys, Fallbacks",
      "summary": "This document demonstrates how to define and pass a custom configuration object to an LLM proxy to manage model routing, rate limits, and failover logic dynamically.",
      "tags": [
        "litellm",
        "model-routing",
        "fallback-logic",
        "openai-api",
        "configuration-management",
        "rate-limiting"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-clientside-auth.md"
    },
    {
      "file_path": "496-docs-proxy-config-management.md",
      "title": "File Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/config_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:25.477180712-03:00",
      "description": "include external YAML files in a config.yaml",
      "summary": "This document explains how to use the include directive in LiteLLM configuration files to import and merge external YAML files. It demonstrates how to modularize settings by splitting model lists and parameters into multiple separate files.",
      "tags": [
        "litellm",
        "yaml-configuration",
        "include-directive",
        "proxy-server",
        "modular-config",
        "config-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-config-management.md"
    },
    {
      "file_path": "497-docs-proxy-config-settings.md",
      "title": "All settings | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/config_settings",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:26.906567916-03:00",
      "description": "litellm_settings - Reference",
      "summary": "This document provides a comprehensive reference list of environment variables and configuration settings for various cloud providers, AI services, and application integrations. It details variables for authentication, connection limits, and service-specific parameters for platforms including AWS, Azure, and Anthropic.",
      "tags": [
        "environment-variables",
        "configuration-settings",
        "cloud-integrations",
        "api-credentials",
        "authentication",
        "infrastructure-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-config-settings.md"
    },
    {
      "file_path": "498-docs-proxy-custom-pricing.md",
      "title": "Custom LLM Pricing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/custom_pricing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:33.58495171-03:00",
      "description": "Overview",
      "summary": "This document explains how to configure and customize cost tracking in LiteLLM, including overriding default pricing, managing provider-specific margins, and mapping base models for accurate billing.",
      "tags": [
        "litellm",
        "cost-tracking",
        "pricing-configuration",
        "llm-proxy",
        "token-usage",
        "billing-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-custom-pricing.md"
    },
    {
      "file_path": "499-docs-proxy-custom-root-ui.md",
      "title": "UI - Custom Root Path | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/custom_root_ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:35.06719753-03:00",
      "description": "ðŸ’¥ Use this when you want to serve LiteLLM on a custom base url path like https4000/api/v1",
      "summary": "This guide explains how to configure and run the LiteLLM proxy on a custom base URL path using the SERVER_ROOT_PATH environment variable.",
      "tags": [
        "litellm",
        "proxy-server",
        "custom-root-path",
        "environment-variables",
        "server-configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-custom-root-ui.md"
    },
    {
      "file_path": "500-docs-proxy-db-info.md",
      "title": "What is stored in the DB | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/db_info",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:42.879208697-03:00",
      "description": "The LiteLLM Proxy uses a PostgreSQL database to store various information. Here's are the main features the DB is used for:",
      "summary": "This document explains how LiteLLM Proxy utilizes a PostgreSQL database for logging and provides instructions for configuring log settings and performing database migrations.",
      "tags": [
        "litellm-proxy",
        "postgresql",
        "logging",
        "database-migration",
        "configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-db-info.md"
    },
    {
      "file_path": "501-docs-proxy-forward-client-headers.md",
      "title": "Forward Client Headers to LLM API | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/forward_client_headers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:00.569525643-03:00",
      "description": "Control which model groups can forward client headers to the underlying LLM provider APIs.",
      "summary": "This document explains how to configure LiteLLM to selectively forward client request headers to underlying LLM provider APIs. It covers global and model-specific configurations, supported header patterns, and security best practices for handling sensitive metadata.",
      "tags": [
        "litellm",
        "header-forwarding",
        "proxy-configuration",
        "request-tracing",
        "model-groups",
        "api-security"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-forward-client-headers.md"
    },
    {
      "file_path": "502-docs-proxy-guardrails-azure-content-guardrail.md",
      "title": "Azure Content Safety Guardrail | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/azure_content_guardrail",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:05.479394998-03:00",
      "description": "LiteLLM supports Azure Content Safety guardrails via the Azure Content Safety API.",
      "summary": "This document explains how to integrate and configure Azure Content Safety guardrails within LiteLLM to provide prompt shielding and text moderation for LLM inputs and outputs.",
      "tags": [
        "litellm",
        "azure-content-safety",
        "guardrails",
        "text-moderation",
        "prompt-shield",
        "content-filtering"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-guardrails-azure-content-guardrail.md"
    },
    {
      "file_path": "503-docs-proxy-guardrails-lakera-ai.md",
      "title": "Lakera AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/lakera_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:18.321567098-03:00",
      "description": "Quick Start",
      "summary": "This document provides instructions on configuring and implementing Lakera guardrails within LiteLLM to monitor or block LLM inputs and outputs for content safety.",
      "tags": [
        "litellm",
        "lakera",
        "guardrails",
        "content-safety",
        "configuration",
        "llm-security"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-guardrails-lakera-ai.md"
    },
    {
      "file_path": "504-docs-proxy-guardrails-model-armor.md",
      "title": "Google Cloud Model Armor | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/model_armor",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:21.720728686-03:00",
      "description": "LiteLLM supports Google Cloud Model Armor guardrails via the Model Armor API.",
      "summary": "This document explains how to integrate and configure Google Cloud Model Armor guardrails within LiteLLM for content sanitization and security filtering.",
      "tags": [
        "litellm",
        "google-cloud",
        "model-armor",
        "guardrails",
        "security",
        "llm-proxy",
        "content-filtering"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-guardrails-model-armor.md"
    },
    {
      "file_path": "505-docs-proxy-guardrails-qualifire.md",
      "title": "Qualifire | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/qualifire",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:31.966302423-03:00",
      "description": "Use Qualifire to evaluate LLM outputs for quality, safety, and reliability. Detect prompt injections, hallucinations, PII, harmful content, and validate that your AI follows instructions.",
      "summary": "This document provides instructions for integrating Qualifire guardrails with LiteLLM to monitor and evaluate AI model outputs for safety, reliability, and security. It explains how to configure various checks such as prompt injection detection, hallucination checks, and PII detection within the LiteLLM gateway.",
      "tags": [
        "litellm",
        "qualifire",
        "llm-guardrails",
        "prompt-injection",
        "hallucination-detection",
        "pii-detection",
        "content-moderation"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-guardrails-qualifire.md"
    },
    {
      "file_path": "506-docs-proxy-ip-address.md",
      "title": "IP Address Filtering | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/ip_address",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:44.646071187-03:00",
      "description": "You need a LiteLLM License to unlock this feature. Grab time, to get one today!",
      "summary": "This document explains how to restrict access to LiteLLM proxy endpoints by configuring a list of allowed IP addresses.",
      "tags": [
        "litellm",
        "ip-restriction",
        "access-control",
        "security",
        "proxy-configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-ip-address.md"
    },
    {
      "file_path": "507-docs-proxy-jwt-auth-arch.md",
      "title": "Control Model Access with OIDC (Azure AD/Keycloak/etc.) | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/jwt_auth_arch",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:44.812160423-03:00",
      "description": "âœ¨ JWT Auth is on LiteLLM Enterprise",
      "summary": "This document explains how to configure JWT authentication and role-based access control (RBAC) for LiteLLM Proxy using external identity providers like Azure AD and Keycloak.",
      "tags": [
        "litellm-proxy",
        "jwt-authentication",
        "rbac",
        "azure-ad",
        "keycloak",
        "identity-provider",
        "access-control"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-jwt-auth-arch.md"
    },
    {
      "file_path": "508-docs-proxy-load-balancing.md",
      "title": "Proxy - Load Balancing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/load_balancing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:48.277024344-03:00",
      "description": "Load balance multiple instances of the same model",
      "summary": "This document explains how to implement load balancing for multiple LLM deployments using the LiteLLM Proxy's built-in router and various routing strategies.",
      "tags": [
        "litellm-proxy",
        "load-balancing",
        "model-routing",
        "redis-integration",
        "high-availability",
        "rate-limiting",
        "yaml-configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-load-balancing.md"
    },
    {
      "file_path": "509-docs-proxy-pagerduty.md",
      "title": "PagerDuty Alerting | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/pagerduty",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:09.54715251-03:00",
      "description": "âœ¨ PagerDuty Alerting is on LiteLLM Enterprise",
      "summary": "This document provides instructions for configuring and testing PagerDuty alerts within LiteLLM to monitor high API failure rates and hanging requests.",
      "tags": [
        "litellm",
        "pagerduty",
        "monitoring",
        "alerting",
        "configuration",
        "error-handling"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-pagerduty.md"
    },
    {
      "file_path": "510-docs-proxy-pass-through.md",
      "title": "Create Pass Through Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/pass_through",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:10.949131361-03:00",
      "description": "Route requests from your LiteLLM proxy to any external API. Perfect for custom models, image generation APIs, or any service you want to proxy through LiteLLM.",
      "summary": "This document explains how to configure pass-through endpoints in LiteLLM Proxy to route requests to external APIs while maintaining centralized authentication and cost tracking.",
      "tags": [
        "litellm-proxy",
        "api-gateway",
        "pass-through-endpoints",
        "request-routing",
        "cost-management",
        "custom-adapters"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-pass-through.md"
    },
    {
      "file_path": "511-docs-proxy-prod.md",
      "title": "âš¡ Best Practices for Production | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/prod",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:15.130708424-03:00",
      "description": "1. Use this config.yaml",
      "summary": "This document provides comprehensive configuration best practices and deployment recommendations for running LiteLLM in production environments. It covers resource specifications, Kubernetes optimization, Redis performance tuning, and database reliability settings.",
      "tags": [
        "litellm-production",
        "performance-optimization",
        "kubernetes-deployment",
        "redis-configuration",
        "database-reliability",
        "environment-variables"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-prod.md"
    },
    {
      "file_path": "512-docs-proxy-prompt-management.md",
      "title": "Prompt Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/prompt_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:16.845516271-03:00",
      "description": "Run experiments or change the specific model (e.g. from gpt-4o to gpt4o-mini finetune) from your prompt management tool (e.g. Langfuse) instead of making changes in the application.",
      "summary": "This document explains how to integrate and manage prompts in LiteLLM by defining them within a configuration file and connecting to external tools like Langfuse and DotPrompt. It covers setup procedures for loading prompts at proxy startup and using them via API endpoints with dynamic variables.",
      "tags": [
        "prompt-management",
        "litellm-proxy",
        "config-yaml",
        "langfuse",
        "dotprompt",
        "prompt-integration"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-prompt-management.md"
    },
    {
      "file_path": "513-docs-proxy-provider-budget-routing.md",
      "title": "Budget Routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/provider_budget_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:17.311521537-03:00",
      "description": "LiteLLM Supports setting the following budgets:",
      "summary": "This document explains how to configure and manage usage budgets in LiteLLM across providers, specific models, and metadata tags. It details the setup process for spend tracking, routing logic, and multi-instance synchronization using Redis.",
      "tags": [
        "litellm",
        "budget-management",
        "cost-control",
        "proxy-configuration",
        "redis",
        "usage-limits"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-provider-budget-routing.md"
    },
    {
      "file_path": "514-docs-proxy-provider-discounts.md",
      "title": "Provider Discounts | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/provider_discounts",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:18.839966875-03:00",
      "description": "Apply percentage-based discounts to specific providers. This is useful for negotiated enterprise pricing with providers.",
      "summary": "This document explains how to configure and apply percentage-based cost discounts for specific LLM providers within the LiteLLM Proxy Server.",
      "tags": [
        "litellm",
        "cost-management",
        "discount-configuration",
        "enterprise-pricing",
        "llm-proxy",
        "billing"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-provider-discounts.md"
    },
    {
      "file_path": "515-docs-proxy-public-routes.md",
      "title": "Control Public & Private Routes | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/public_routes",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:23.044100354-03:00",
      "description": "Requires a LiteLLM Enterprise License. Get a free trial.",
      "summary": "This document explains how to configure route access controls for the LiteLLM proxy, including setting public, admin-only, and allowed routes using direct paths or wildcard patterns.",
      "tags": [
        "litellm-proxy",
        "route-management",
        "authentication",
        "access-control",
        "security-configuration",
        "wildcard-patterns"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-public-routes.md"
    },
    {
      "file_path": "516-docs-proxy-reject-clientside-metadata-tags.md",
      "title": "Reject Client-Side Metadata Tags | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/reject_clientside_metadata_tags",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:28.414175748-03:00",
      "description": "Overview",
      "summary": "This document explains the reject_clientside_metadata_tags setting, which prevents users from overriding request tags to ensure consistent budget tracking and routing. It covers configuration steps, usage scenarios, and the error behavior of the API when this security feature is enabled.",
      "tags": [
        "api-security",
        "configuration",
        "metadata-tags",
        "litellm",
        "access-control",
        "budget-tracking"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-reject-clientside-metadata-tags.md"
    },
    {
      "file_path": "517-docs-proxy-spend-logs-deletion.md",
      "title": "âœ¨ Maximum Retention Period for Spend Logs | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/spend_logs_deletion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:39.7627774-03:00",
      "description": "This walks through how to set the maximum retention period for spend logs. This helps manage database size by deleting old logs automatically.",
      "summary": "This document explains how to configure and automate the deletion of old spend logs to manage database storage and performance. It provides details on setting retention periods, cleanup schedules using cron syntax, and managing batch deletions in single or multi-instance environments.",
      "tags": [
        "log-retention",
        "database-management",
        "spend-logs",
        "proxy-configuration",
        "automated-cleanup",
        "distributed-locking"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-spend-logs-deletion.md"
    },
    {
      "file_path": "518-docs-proxy-timeout.md",
      "title": "Timeouts | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/timeout",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:52.326432347-03:00",
      "description": "The timeout set in router is for the entire length of the call, and is passed down to the completion() call level as well.",
      "summary": "This document explains how to configure global, per-model, and per-request timeouts for synchronous and streaming calls using the LiteLLM Router, including methods for testing timeout handling.",
      "tags": [
        "litellm",
        "router",
        "timeout-configuration",
        "stream-timeout",
        "error-handling",
        "python-sdk"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-timeout.md"
    },
    {
      "file_path": "519-docs-proxy-users.md",
      "title": "Budgets, Rate Limits | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/users",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:03.781724741-03:00",
      "description": "Personal budgets: Create virtual keys without team_id for individual spending limits",
      "summary": "This document outlines how to configure and manage spending limits for LiteLLM Proxy across global, team, and individual user levels. It provides instructions for setting budget durations and maximum spend limits using both configuration files and API endpoints.",
      "tags": [
        "litellm",
        "proxy-server",
        "budget-management",
        "cost-control",
        "team-management",
        "api-configuration",
        "rate-limiting"
      ],
      "category": "configuration",
      "original_file_path": "docs-proxy-users.md"
    },
    {
      "file_path": "520-docs-realtime.md",
      "title": "/realtime | liteLLM",
      "url": "https://docs.litellm.ai/docs/realtime",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:09.903514476-03:00",
      "description": "Use this to loadbalance across Azure + OpenAI.",
      "summary": "This document explains how to implement load balancing for realtime audio requests across Azure and OpenAI using LiteLLM and WebSockets.",
      "tags": [
        "azure-openai",
        "litellm",
        "load-balancing",
        "websocket",
        "realtime-api",
        "gpt-4o"
      ],
      "category": "configuration",
      "original_file_path": "docs-realtime.md"
    },
    {
      "file_path": "521-docs-routing-load-balancing.md",
      "title": "Routing, Loadbalancing & Fallbacks | liteLLM",
      "url": "https://docs.litellm.ai/docs/routing-load-balancing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:20.13600739-03:00",
      "description": "Learn how to load balance, route, and set fallbacks for your LLM requests",
      "summary": "Explains how timeout settings configured in the router apply to the entire duration of a call and propagate down to the completion call level.",
      "tags": [
        "timeout-configuration",
        "router-settings",
        "api-completion",
        "litellm-proxy"
      ],
      "category": "configuration",
      "original_file_path": "docs-routing-load-balancing.md"
    },
    {
      "file_path": "522-docs-secret-managers-aws-kms.md",
      "title": "AWS Key Management V1 | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/aws_kms",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:38.941354608-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document explains how to use AWS Key Management Service (KMS) to store and decrypt a hashed copy of the Proxy Master Key for secure key management.",
      "tags": [
        "aws-kms",
        "key-management",
        "security",
        "enterprise-feature",
        "environment-variables"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-aws-kms.md"
    },
    {
      "file_path": "523-docs-secret-managers-aws-secret-manager.md",
      "title": "AWS Secret Manager | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/aws_secret_manager",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:39.193951357-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document provides instructions for configuring LiteLLM to use AWS Secret Manager for storing and retrieving API keys, virtual keys, and other secrets using various IAM authentication methods.",
      "tags": [
        "aws-secret-manager",
        "key-management",
        "iam-roles",
        "security",
        "litellm-proxy",
        "aws-iam"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-aws-secret-manager.md"
    },
    {
      "file_path": "524-docs-secret-managers-azure-key-vault.md",
      "title": "Azure Key Vault | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/azure_key_vault",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:40.366005749-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document provides instructions for configuring the LiteLLM Proxy Server to use Azure Key Vault for managing API keys and secrets. It covers installation, environment setup, and the necessary configuration file parameters.",
      "tags": [
        "litellm",
        "proxy-server",
        "azure-key-vault",
        "secrets-management",
        "configuration"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-azure-key-vault.md"
    },
    {
      "file_path": "525-docs-secret-managers-google-secret-manager.md",
      "title": "Google Secret Manager | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/google_secret_manager",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:45.388547132-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document explains how to configure and integrate Google Secret Manager as a key management system for the LiteLLM proxy.",
      "tags": [
        "google-secret-manager",
        "litellm-proxy",
        "key-management",
        "security-configuration",
        "environment-variables"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-google-secret-manager.md"
    },
    {
      "file_path": "526-docs-secret-managers-hashicorp-vault.md",
      "title": "Hashicorp Vault | liteLLM",
      "url": "https://docs.litellm.ai/docs/secret_managers/hashicorp_vault",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:45.581096133-03:00",
      "description": "âœ¨ This is an Enterprise Feature",
      "summary": "This document explains how to configure and use Hashicorp Vault with LiteLLM for managing, reading, and writing secrets through various authentication methods.",
      "tags": [
        "hashicorp-vault",
        "secret-management",
        "litellm-proxy",
        "authentication",
        "approle",
        "key-management"
      ],
      "category": "configuration",
      "original_file_path": "docs-secret-managers-hashicorp-vault.md"
    },
    {
      "file_path": "527-docs-set-keys.md",
      "title": "Setting API Keys, Base, Version | liteLLM",
      "url": "https://docs.litellm.ai/docs/set_keys",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:49.068425745-03:00",
      "description": "LiteLLM allows you to specify the following:",
      "summary": "This document explains how to configure LiteLLM API settings such as keys, bases, and versions using environment variables, package-level variables, or direct function arguments. It also describes helper functions for validating keys and identifying supported models across various providers.",
      "tags": [
        "litellm",
        "api-configuration",
        "environment-variables",
        "authentication",
        "python-sdk",
        "llm-providers"
      ],
      "category": "configuration",
      "original_file_path": "docs-set-keys.md"
    },
    {
      "file_path": "528-docs-wildcard-routing.md",
      "title": "Provider specific Wildcard routing | liteLLM",
      "url": "https://docs.litellm.ai/docs/wildcard_routing",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:59.052755392-03:00",
      "description": "Proxy all models from a provider",
      "summary": "This document demonstrates how to configure the LiteLLM Router using wildcard patterns to dynamically route requests to different model providers based on naming conventions.",
      "tags": [
        "litellm",
        "router-configuration",
        "wildcard-routing",
        "model-deployment",
        "python-sdk"
      ],
      "category": "configuration",
      "original_file_path": "docs-wildcard-routing.md"
    },
    {
      "file_path": "529-release-notes-tags-budgets-rate-limits.md",
      "title": "One post tagged with \"budgets/rate limits\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/budgets-rate-limits",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:22.810540428-03:00",
      "summary": "This document outlines how to configure usage budget tiers for rate limiting and details updates regarding logging improvements and custom guardrail parameters.",
      "tags": [
        "litellm",
        "budget-management",
        "rate-limiting",
        "logging",
        "guardrails",
        "api-updates"
      ],
      "category": "configuration",
      "original_file_path": "release-notes-tags-budgets-rate-limits.md"
    },
    {
      "file_path": "530-release-notes-tags-key-management.md",
      "title": "One post tagged with \"key management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/key-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:44.786889605-03:00",
      "summary": "This document provides instructions on defining usage tiers with rate limits and highlights updates regarding logging fixes, fine-tuning observability, and customizable guardrail parameters.",
      "tags": [
        "rate-limiting",
        "budget-management",
        "api-logging",
        "guardrails",
        "observability",
        "litellm-updates"
      ],
      "category": "configuration",
      "original_file_path": "release-notes-tags-key-management.md"
    },
    {
      "file_path": "531-release-notes-v1.59.0.md",
      "title": "v1.59.0",
      "url": "https://docs.litellm.ai/release_notes/v1.59.0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:18.465727271-03:00",
      "description": "Get a 7 day free trial for LiteLLM Enterprise here.",
      "summary": "This document explains how to enable and view message and response logs in the LiteLLM Admin UI by modifying the proxy configuration and database settings.",
      "tags": [
        "litellm",
        "admin-ui",
        "logging",
        "proxy-configuration",
        "database-schema",
        "enterprise-features"
      ],
      "category": "configuration",
      "original_file_path": "release-notes-v1.59.0.md"
    },
    {
      "file_path": "532-docs-adding-provider-generic-guardrail-api.md",
      "title": "[BETA] Generic Guardrail API - Integrate Without a PR | liteLLM",
      "url": "https://docs.litellm.ai/docs/adding_provider/generic_guardrail_api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:55.713280185-03:00",
      "description": "The Problem",
      "summary": "This document explains how to integrate custom guardrail providers with LiteLLM using a standardized Generic Guardrail API contract. It covers the required request and response formats, supported endpoints, and parameter details for real-time content filtering and modification.",
      "tags": [
        "litellm",
        "guardrails",
        "api-integration",
        "llm-security",
        "content-moderation",
        "middleware"
      ],
      "category": "api",
      "original_file_path": "docs-adding-provider-generic-guardrail-api.md"
    },
    {
      "file_path": "533-docs-providers-abliteration.md",
      "title": "Abliteration | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/abliteration",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:45.938199319-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Abliteration with LiteLLM using its OpenAI-compatible chat completions endpoint, including environment setup and proxy configuration.",
      "tags": [
        "abliteration",
        "litellm",
        "openai-compatible",
        "chat-completions",
        "api-integration",
        "streaming"
      ],
      "category": "api",
      "original_file_path": "docs-providers-abliteration.md"
    },
    {
      "file_path": "534-docs-providers-aleph-alpha.md",
      "title": "Aleph Alpha | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/aleph_alpha",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:49.729827085-03:00",
      "description": "LiteLLM supports all models from Aleph Alpha.",
      "summary": "This document provides instructions for using Aleph Alpha models with LiteLLM, including API key configuration and a list of supported model identifiers.",
      "tags": [
        "litellm",
        "aleph-alpha",
        "llm-integration",
        "model-support"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-aleph-alpha.md"
    },
    {
      "file_path": "535-docs-providers-anthropic.md",
      "title": "Anthropic | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/anthropic",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:51.375666059-03:00",
      "description": "LiteLLM supports all anthropic models.",
      "summary": "This document provides technical details and instructions for integrating Anthropic's Claude models with LiteLLM, covering model versions, parameter mapping, and structured output support.",
      "tags": [
        "litellm",
        "anthropic",
        "claude",
        "api-integration",
        "structured-outputs",
        "azure-foundry"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-anthropic.md"
    },
    {
      "file_path": "536-docs-providers-apertis.md",
      "title": "Apertis AI (Stima API) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/apertis",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:01.029813253-03:00",
      "description": "Overview",
      "summary": "This document provides an overview and integration guide for Apertis AI, detailing how to access over 430 AI models using the LiteLLM Python SDK and proxy server.",
      "tags": [
        "apertis-ai",
        "stima-api",
        "litellm",
        "api-integration",
        "llm-models",
        "python-usage"
      ],
      "category": "api",
      "original_file_path": "docs-providers-apertis.md"
    },
    {
      "file_path": "537-docs-providers-azure-azure-responses.md",
      "title": "Azure Responses API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/azure_responses",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:18.638224557-03:00",
      "description": "| Property | Details |",
      "summary": "This document explains how to integrate and use the Azure OpenAI Responses API with LiteLLM, including instructions for streaming, cost tracking, and handling specific model types like Azure Codex.",
      "tags": [
        "azure-openai",
        "litellm",
        "responses-api",
        "python-sdk",
        "streaming",
        "cost-tracking",
        "codex-models"
      ],
      "category": "api",
      "original_file_path": "docs-providers-azure-azure-responses.md"
    },
    {
      "file_path": "538-docs-providers-azure-azure-speech.md",
      "title": "Azure Text to Speech (tts) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/azure/azure_speech",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:21.490865746-03:00",
      "description": "Overview",
      "summary": "This document provides a code implementation for converting text to speech using an Azure-hosted model, detailing parameters for voice selection, speed, and output format.",
      "tags": [
        "text-to-speech",
        "azure-openai",
        "speech-synthesis",
        "api-usage",
        "audio-generation"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-azure-azure-speech.md"
    },
    {
      "file_path": "539-docs-providers-bedrock-embedding.md",
      "title": "Bedrock Embedding | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:27.878200912-03:00",
      "description": "Supported Embedding Models",
      "summary": "This document provides a technical reference for using LiteLLM with AWS Bedrock embedding models, including support for synchronous and asynchronous multimodal processing.",
      "tags": [
        "aws-bedrock",
        "litellm",
        "embedding-models",
        "async-invoke",
        "amazon-nova",
        "multimodal-embeddings"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-bedrock-embedding.md"
    },
    {
      "file_path": "540-docs-providers-bedrock-rerank.md",
      "title": "AWS Bedrock - Rerank API | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/bedrock_rerank",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:31.257380315-03:00",
      "description": "Use Bedrock's Rerank API in the Cohere /rerank format.",
      "summary": "This document explains how to integrate and use the Bedrock Rerank API through LiteLLM using a Cohere-style interface. It details supported parameters, authentication procedures, and cost tracking while providing a practical Python code sample.",
      "tags": [
        "bedrock",
        "rerank-api",
        "litellm",
        "aws-bedrock",
        "python-sdk",
        "search-ranking"
      ],
      "category": "api",
      "original_file_path": "docs-providers-bedrock-rerank.md"
    },
    {
      "file_path": "541-docs-providers-codestral.md",
      "title": "Codestral API [Mistral AI] | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/codestral",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:43.734949477-03:00",
      "description": "Codestral is available in select code-completion plugins but can also be queried directly. See the documentation for more details.",
      "summary": "This document provides instructions and code examples for integrating Codestral models via LiteLLM for both code completions and chat interactions.",
      "tags": [
        "codestral",
        "litellm",
        "text-completion",
        "chat-completion",
        "python",
        "api-integration"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-codestral.md"
    },
    {
      "file_path": "542-docs-providers-compactifai.md",
      "title": "CompactifAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/compactifai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:48:47.010615886-03:00",
      "description": "https://docs.compactif.ai/",
      "summary": "This document provides a technical overview and integration guide for CompactifAI's OpenAI-compatible API, which offers compressed language models designed for low-latency and cost-effective inference. It covers API configuration, supported parameters, and implementation examples using the LiteLLM SDK.",
      "tags": [
        "compactifai",
        "openai-compatibility",
        "model-compression",
        "litellm-integration",
        "llm-inference",
        "api-reference",
        "cost-optimization"
      ],
      "category": "api",
      "original_file_path": "docs-providers-compactifai.md"
    },
    {
      "file_path": "543-docs-providers-gigachat.md",
      "title": "GigaChat | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/gigachat",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:11.205770941-03:00",
      "description": "https://developers.sber.ru/docs/ru/gigachat/api/overview",
      "summary": "This document provides technical instructions for integrating Sber AI's GigaChat models with the LiteLLM library, covering authentication, SSL configuration, and specific API features.",
      "tags": [
        "gigachat",
        "litellm",
        "sber-ai",
        "python-sdk",
        "llm-api",
        "embeddings",
        "vision-api"
      ],
      "category": "api",
      "original_file_path": "docs-providers-gigachat.md"
    },
    {
      "file_path": "544-docs-providers-lambda-ai.md",
      "title": "Lambda AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/lambda_ai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:29.070618125-03:00",
      "description": "Overview",
      "summary": "This document provides instructions for integrating Lambda AI with LiteLLM, covering model support, environment configuration, and implementation of features like streaming, vision, and function calling.",
      "tags": [
        "lambda-ai",
        "litellm",
        "api-integration",
        "llm-inference",
        "python-sdk",
        "cloud-gpu",
        "openai-compatible"
      ],
      "category": "api",
      "original_file_path": "docs-providers-lambda-ai.md"
    },
    {
      "file_path": "545-docs-providers-llamagate.md",
      "title": "LlamaGate | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/llamagate",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:35.779154773-03:00",
      "description": "Overview",
      "summary": "This document provides a technical overview and integration guide for LlamaGate, an OpenAI-compatible API gateway that facilitates access to various open-source large language models through LiteLLM. It details supported model types, configuration requirements, and implementation examples for chat, vision, and embedding tasks.",
      "tags": [
        "llamagate",
        "api-gateway",
        "litellm",
        "open-source-llm",
        "openai-compatible",
        "text-embeddings",
        "multimodal-ai"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-llamagate.md"
    },
    {
      "file_path": "546-docs-providers-milvus-vector-stores.md",
      "title": "Milvus - Vector Store | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/milvus_vector_stores",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:40.953225745-03:00",
      "description": "Use Milvus as a vector store for RAG.",
      "summary": "This document provides a Python implementation of a client for interacting with the Milvus REST API v2, facilitating collection management, data ingestion, and vector search operations.",
      "tags": [
        "milvus-vector-db",
        "rest-api-v2",
        "python-client",
        "vector-search",
        "api-wrapper",
        "data-management"
      ],
      "category": "api",
      "original_file_path": "docs-providers-milvus-vector-stores.md"
    },
    {
      "file_path": "547-docs-providers-nvidia-nim.md",
      "title": "Nvidia NIM | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/nvidia_nim",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:49:54.081999374-03:00",
      "description": "https://docs.api.nvidia.com/nim/reference/",
      "summary": "This document provides instructions and code examples for integrating Nvidia NIM models with LiteLLM, including chat completions, streaming, embeddings, and proxy server configuration.",
      "tags": [
        "nvidia-nim",
        "litellm",
        "api-integration",
        "chat-completions",
        "embeddings",
        "python-sdk",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-nvidia-nim.md"
    },
    {
      "file_path": "548-docs-providers-perplexity.md",
      "title": "Perplexity AI (pplx-api) | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/perplexity",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:06.33173901-03:00",
      "description": "https://www.perplexity.ai",
      "summary": "This document provides instructions and code examples for integrating Perplexity AI models with the LiteLLM library, covering API key configuration, streaming, and reasoning parameters.",
      "tags": [
        "litellm",
        "perplexity-ai",
        "api-integration",
        "python-sdk",
        "streaming",
        "reasoning-effort",
        "llm-models"
      ],
      "category": "api",
      "original_file_path": "docs-providers-perplexity.md"
    },
    {
      "file_path": "549-docs-providers-togetherai.md",
      "title": "Together AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/togetherai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:30.38234247-03:00",
      "description": "LiteLLM supports all models on Together AI.",
      "summary": "This document provides instructions and code examples for integrating Together AI models with LiteLLM, covering authentication, model lists, and custom prompt template configuration.",
      "tags": [
        "together-ai",
        "litellm",
        "python-sdk",
        "llm-integration",
        "prompt-templates",
        "api-configuration"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-togetherai.md"
    },
    {
      "file_path": "550-docs-providers-topaz.md",
      "title": "Topaz | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/topaz",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:31.644726091-03:00",
      "description": "| Property | Details |",
      "summary": "This document provides a code example for generating image variations using the LiteLLM library and the Topaz API.",
      "tags": [
        "litellm",
        "image-variation",
        "topaz-api",
        "python",
        "image-generation"
      ],
      "category": "api",
      "original_file_path": "docs-providers-topaz.md"
    },
    {
      "file_path": "551-docs-providers-vercel-ai-gateway.md",
      "title": "Vercel AI Gateway | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vercel_ai_gateway",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:36.222528276-03:00",
      "description": "Overview",
      "summary": "This document explains how to integrate LiteLLM with Vercel AI Gateway to access multiple AI providers through a unified interface. It covers authentication setup, environment variable configuration, and implementation via both the Python SDK and the LiteLLM Proxy.",
      "tags": [
        "vercel-ai-gateway",
        "litellm",
        "api-integration",
        "python-sdk",
        "llm-proxy",
        "environment-variables"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-vercel-ai-gateway.md"
    },
    {
      "file_path": "552-docs-providers-vertex-embedding.md",
      "title": "Vertex AI Embedding | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:43.918774564-03:00",
      "description": "Usage - Embedding",
      "summary": "This document provides a technical reference for using LiteLLM to interface with Vertex AI embedding models, including text, BGE, and multi-modal types. It details supported model IDs, parameter mappings for OpenAI compatibility, and configuration steps for both the SDK and LiteLLM Proxy.",
      "tags": [
        "litellm",
        "vertex-ai",
        "embeddings",
        "multi-modal",
        "google-cloud",
        "python-sdk",
        "bge-embeddings"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-vertex-embedding.md"
    },
    {
      "file_path": "553-docs-providers-vertex-partner.md",
      "title": "Vertex AI - Anthropic, DeepSeek, Model Garden | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/vertex_partner",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:46.87904539-03:00",
      "description": "Supported Partner Providers",
      "summary": "This document provides configuration details and code examples for accessing partner models like Claude, Llama, and Mistral via Google Vertex AI using LiteLLM. It outlines the specific routing prefixes and environment variables required for successful integration.",
      "tags": [
        "vertex-ai",
        "litellm",
        "model-routing",
        "anthropic-claude",
        "mistral-ai",
        "meta-llama",
        "python-sdk"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-vertex-partner.md"
    },
    {
      "file_path": "554-docs-providers-voyage.md",
      "title": "Voyage AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/voyage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:55.66409915-03:00",
      "description": "https://docs.voyageai.com/embeddings/",
      "summary": "This document provides technical specifications and implementation examples for using VoyageAI's embedding and reranking models via the LiteLLM library. It covers API configuration, supported model parameters, and specialized features like contextual embeddings for long documents.",
      "tags": [
        "voyage-ai",
        "embeddings",
        "reranking",
        "litellm",
        "api-reference",
        "vector-search",
        "python-sdk"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-voyage.md"
    },
    {
      "file_path": "555-docs-providers-xai.md",
      "title": "xAI | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers/xai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:50:58.648216377-03:00",
      "description": "https://docs.x.ai/docs",
      "summary": "This document provides technical specifications and implementation guides for using xAI's Grok models through the LiteLLM library, covering model versions, feature support, and proxy configuration.",
      "tags": [
        "xai",
        "grok",
        "litellm",
        "api-integration",
        "python-sdk",
        "llm-models",
        "reasoning-models"
      ],
      "category": "reference",
      "original_file_path": "docs-providers-xai.md"
    },
    {
      "file_path": "556-docs-providers.md",
      "title": "Providers | liteLLM",
      "url": "https://docs.litellm.ai/docs/providers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:44.391305908-03:00",
      "description": "Learn how to deploy + call models from different providers on LiteLLM",
      "summary": "This document serves as a comprehensive index of all model providers supported by LiteLLM, detailing how to integrate new OpenAI-compatible providers and configure model pricing. It provides a central directory for users to find technical documentation and setup guides for dozens of third-party AI services.",
      "tags": [
        "model-providers",
        "llm-integration",
        "openai-compatible",
        "provider-registration",
        "multi-model-api",
        "litellm-support"
      ],
      "category": "reference",
      "original_file_path": "docs-providers.md"
    },
    {
      "file_path": "557-release-notes-v1-81-0.md",
      "title": "v1.81.0 - Claude Code - Web Search Across All Providers",
      "url": "https://docs.litellm.ai/release_notes/v1-81-0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:07.240753388-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates and new features in LiteLLM version 1.81.0, including cross-provider web search support, image download size restrictions, and CPU performance optimizations.",
      "tags": [
        "release-notes",
        "litellm-proxy",
        "image-processing",
        "performance-optimization",
        "claude-code",
        "audit-logs",
        "llm-gateway"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-81-0.md"
    },
    {
      "file_path": "558-blog-authors.md",
      "title": "Authors | liteLLM",
      "url": "https://docs.litellm.ai/blog/authors",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:45.949664317-03:00",
      "summary": "This document identifies the core leadership and team members of the LiteLLM project, including its CEO and CTO.",
      "tags": [
        "litellm",
        "team-profiles",
        "leadership",
        "core-team",
        "company-structure"
      ],
      "category": "other",
      "original_file_path": "blog-authors.md"
    },
    {
      "file_path": "559-docs-data-security.md",
      "title": "Data Privacy and Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/data_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:01.231069888-03:00",
      "description": "At LiteLLM, safeguarding your data privacy and security is our top priority. We recognize the critical importance of the data you share with us and handle it with the highest level of diligence.",
      "summary": "This document outlines the security infrastructure, privacy policies, and compliance certifications for LiteLLM Cloud and self-hosted deployments. It provides detailed information on data encryption, regional storage, SOC 2 and ISO certifications, and vulnerability reporting procedures.",
      "tags": [
        "security-compliance",
        "data-privacy",
        "soc2",
        "iso-27001",
        "vulnerability-scanning",
        "cloud-security",
        "data-protection"
      ],
      "category": "reference",
      "original_file_path": "docs-data-security.md"
    },
    {
      "file_path": "560-docs-proxy-guardrails-lasso-security.md",
      "title": "Lasso Security | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/lasso_security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:19.258385506-03:00",
      "description": "Use Lasso Security to protect your LLM applications from prompt injection attacks, harmful content generation, and other security threats through comprehensive input and output validation.",
      "summary": "This document demonstrates the API response behavior when a chat completion request is blocked by a Lasso guardrail policy due to a detected jailbreak attempt.",
      "tags": [
        "api-response",
        "guardrails",
        "security-filtering",
        "jailbreak-detection",
        "error-handling",
        "lasso-policy"
      ],
      "category": "api",
      "original_file_path": "docs-proxy-guardrails-lasso-security.md"
    },
    {
      "file_path": "561-docs-proxy-security-encryption-faq.md",
      "title": "LiteLLM Self-Hosted Security & Encryption FAQ | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/security_encryption_faq",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:35.598557473-03:00",
      "description": "Data in Transit Encryption",
      "summary": "This document outlines the security protocols for LiteLLM, detailing how data in transit and at rest are secured through TLS/SSL and specific database encryption practices. It provides technical configuration steps for SSL certificates and identifies which data types, such as API keys, are encrypted versus stored in plaintext.",
      "tags": [
        "litellm",
        "encryption",
        "security-configuration",
        "tls-ssl",
        "data-security",
        "ssl-certificates"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-security-encryption-faq.md"
    },
    {
      "file_path": "562-release-notes-authors.md",
      "title": "Authors | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/authors",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:16.148522126-03:00",
      "summary": "This document provides professional profiles and contact links for the leadership team of LiteLLM, including the CEO and CTO.",
      "tags": [
        "litellm",
        "leadership",
        "executive-team",
        "corporate-info",
        "profiles"
      ],
      "category": "other",
      "original_file_path": "release-notes-authors.md"
    },
    {
      "file_path": "563-release-notes-tags-custom-auth.md",
      "title": "One post tagged with \"custom auth\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/custom-auth",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:29.130677272-03:00",
      "summary": "This document outlines several new and updated enterprise features for LiteLLM, including batch API cost tracking, team-based model management, and custom authentication guardrails.",
      "tags": [
        "litellm-enterprise",
        "batches-api",
        "cost-tracking",
        "guardrails",
        "team-management",
        "custom-auth"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-custom-auth.md"
    },
    {
      "file_path": "564-release-notes-tags-security.md",
      "title": "4 posts tagged with \"security\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/security",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:03.888673047-03:00",
      "summary": "This document details the release notes for LiteLLM version 1.67.4, highlighting new features such as enhanced user management, load balancing for the Responses API, and UI session logs. It provides an overview of model-specific updates, bug fixes, and management dashboard improvements.",
      "tags": [
        "release-notes",
        "litellm-proxy",
        "load-balancing",
        "user-management",
        "session-logs",
        "model-updates",
        "api-gateway"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-security.md"
    },
    {
      "file_path": "565-release-notes-v1-72-6-stable.md",
      "title": "v1.72.6-stable - MCP Gateway Permission Management",
      "url": "https://docs.litellm.ai/release_notes/v1-72-6-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:26.299972941-03:00",
      "description": "Deploy this version",
      "summary": "Detailed release notes for LiteLLM version 1.72.6 outlining new features like MCP permissions management, Codex-mini support on Claude Code, and various model and pricing updates.",
      "tags": [
        "litellm",
        "release-notes",
        "mcp-server",
        "model-updates",
        "api-integration",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-72-6-stable.md"
    },
    {
      "file_path": "566-release-notes-v1-73-0-stable.md",
      "title": "v1.73.0-stable - Set default team for new users",
      "url": "https://docs.litellm.ai/release_notes/v1-73-0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:27.536961302-03:00",
      "description": "Known Issues",
      "summary": "This document outlines the updates in LiteLLM version 1.73.0, detailing new features like default team assignments, enhanced passthrough endpoints, and a model health check dashboard. It also provides a comprehensive list of pricing and configuration updates for various LLM providers including Gemini, Azure, and Vertex AI.",
      "tags": [
        "litellm-release",
        "user-management",
        "passthrough-endpoints",
        "health-monitoring",
        "model-updates",
        "api-gateway"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-73-0-stable.md"
    },
    {
      "file_path": "567-release-notes-v1-77-5.md",
      "title": "v1.77.5-stable - MCP OAuth 2.0 Support",
      "url": "https://docs.litellm.ai/release_notes/v1-77-5",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:49.369070653-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM version 1.77.5-stable, detailing significant performance optimizations, new model support for Gemini and DeepSeek, and enhanced security features.",
      "tags": [
        "litellm",
        "release-notes",
        "performance-benchmarks",
        "model-support",
        "llm-gateway",
        "oauth-authentication",
        "gemini-flash"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-77-5.md"
    },
    {
      "file_path": "568-release-notes-v1-78-0.md",
      "title": "v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key",
      "url": "https://docs.litellm.ai/release_notes/v1-78-0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:50.105131459-03:00",
      "description": "Deploy this version",
      "summary": "This document provides the release notes for LiteLLM version 1.78.0-stable, detailing performance enhancements, new model integrations, and administrative controls for tool access.",
      "tags": [
        "litellm-release",
        "ai-gateway",
        "performance-optimization",
        "model-deployment",
        "latency-reduction",
        "mcp-control"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-78-0.md"
    },
    {
      "file_path": "569-release-notes-v1.67.4-stable.md",
      "title": "v1.67.4-stable - Improved User Management",
      "url": "https://docs.litellm.ai/release_notes/v1.67.4-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:34.541020679-03:00",
      "description": "Deploy this version",
      "summary": "This document details the features and improvements in LiteLLM version 1.67.4-stable, including enhanced user management, API load balancing with session continuity, and updated model support.",
      "tags": [
        "litellm",
        "release-notes",
        "load-balancing",
        "user-management",
        "llm-proxy",
        "model-integration"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.67.4-stable.md"
    },
    {
      "file_path": "570-blog-tags.md",
      "title": "Tags | liteLLM",
      "url": "https://docs.litellm.ai/blog/tags",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:51.292775326-03:00",
      "summary": "This document provides an alphabetical index of metadata tags used to categorize blog posts and articles on the LiteLLM platform.",
      "tags": [
        "blog-index",
        "litellm-tags",
        "content-navigation",
        "metadata",
        "topic-index"
      ],
      "category": "reference",
      "original_file_path": "blog-tags.md"
    },
    {
      "file_path": "571-completion-input.md",
      "title": "Completion Function - completion() | liteLLM",
      "url": "https://docs.litellm.ai/completion/input",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:03.307932476-03:00",
      "description": "The Input params are exactly the same as the",
      "summary": "This document outlines the request body parameters for the liteLLM chat completion API, detailing required and optional fields for cross-provider model compatibility.",
      "tags": [
        "litellm",
        "chat-completion",
        "api-parameters",
        "openai-compatibility",
        "request-body",
        "llm-integration"
      ],
      "category": "reference",
      "original_file_path": "completion-input.md"
    },
    {
      "file_path": "572-completion-output.md",
      "title": "Completion Function - completion() | liteLLM",
      "url": "https://docs.litellm.ai/completion/output",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:03.630469643-03:00",
      "description": "Here's the exact json output you can expect from a litellm completion call:",
      "summary": "This document illustrates the standardized JSON response structure returned by a LiteLLM completion call, including fields for choices, model usage, and metadata.",
      "tags": [
        "litellm",
        "api-response",
        "json-structure",
        "completion-api",
        "llm-output"
      ],
      "category": "reference",
      "original_file_path": "completion-output.md"
    },
    {
      "file_path": "573-completion-supported.md",
      "title": "Generation/Completion/Chat Completion Models | liteLLM",
      "url": "https://docs.litellm.ai/completion/supported",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:05.113898652-03:00",
      "description": "OpenAI Chat Completion Models",
      "summary": "This document outlines the supported AI models and providers for the liteLLM library, detailing the specific function calls and environment variables required for integration.",
      "tags": [
        "llm-providers",
        "litellm",
        "api-configuration",
        "openai",
        "azure-openai",
        "anthropic",
        "environment-variables"
      ],
      "category": "reference",
      "original_file_path": "completion-supported.md"
    },
    {
      "file_path": "574-docs-aiohttp-benchmarks.md",
      "title": "LiteLLM v1.71.1 Benchmarks | liteLLM",
      "url": "https://docs.litellm.ai/docs/aiohttp_benchmarks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:00.25769849-03:00",
      "description": "Overview",
      "summary": "This document provides performance benchmark results for LiteLLM v1.71.1, comparing the efficiency of the new aiohttp transport layer against the legacy httpx implementation.",
      "tags": [
        "litellm",
        "performance-benchmarks",
        "aiohttp",
        "load-testing",
        "latency-reduction",
        "httpx"
      ],
      "category": "reference",
      "original_file_path": "docs-aiohttp-benchmarks.md"
    },
    {
      "file_path": "575-docs-anthropic-count-tokens.md",
      "title": "/v1/messages/count_tokens | liteLLM",
      "url": "https://docs.litellm.ai/docs/anthropic_count_tokens",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:01.211969585-03:00",
      "description": "Overview",
      "summary": "This document explains how to use LiteLLM's Anthropic-compatible endpoint to count tokens across multiple providers including Anthropic, Vertex AI, and Bedrock.",
      "tags": [
        "litellm",
        "token-counting",
        "anthropic-api",
        "claude",
        "vertex-ai",
        "aws-bedrock",
        "llm-proxy"
      ],
      "category": "api",
      "original_file_path": "docs-anthropic-count-tokens.md"
    },
    {
      "file_path": "576-docs-anthropic-unified.md",
      "title": "/v1/messages | liteLLM",
      "url": "https://docs.litellm.ai/docs/anthropic_unified",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:02.120047761-03:00",
      "description": "Use LiteLLM to call all your LLM APIs in the Anthropic v1/messages format.",
      "summary": "This document explains how to use LiteLLM to call various LLM APIs using the Anthropic messages format, including implementation details for the Python SDK and proxy server.",
      "tags": [
        "litellm",
        "anthropic-api",
        "messages-format",
        "llm-proxy",
        "python-sdk",
        "api-integration"
      ],
      "category": "api",
      "original_file_path": "docs-anthropic-unified.md"
    },
    {
      "file_path": "577-docs-apply-guardrail.md",
      "title": "/guardrails/apply_guardrail | liteLLM",
      "url": "https://docs.litellm.ai/docs/apply_guardrail",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:03.094804563-03:00",
      "description": "Use this endpoint to directly call a guardrail configured on your LiteLLM instance. This is useful when you have services that need to directly call a guardrail.",
      "summary": "This document explains how to use the LiteLLM apply_guardrail endpoint to directly invoke safety and moderation guardrails such as Presidio and Bedrock. It provides configuration instructions, request and response schemas, and details on supported guardrail types.",
      "tags": [
        "litellm",
        "api-endpoint",
        "guardrails",
        "pii-masking",
        "content-moderation",
        "bedrock",
        "presidio",
        "ai-safety"
      ],
      "category": "api",
      "original_file_path": "docs-apply-guardrail.md"
    },
    {
      "file_path": "578-docs-assistants.md",
      "title": "/assistants | liteLLM",
      "url": "https://docs.litellm.ai/docs/assistants",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:03.799252406-03:00",
      "description": "OpenAI has deprecated the Assistants API. It will shut down on August 26, 2026.",
      "summary": "This document explains how to use the OpenAI Assistants API via LiteLLM, covering thread management, message handling, and assistant execution while detailing its upcoming deprecation.",
      "tags": [
        "litellm",
        "assistants-api",
        "openai",
        "azure-openai",
        "thread-management",
        "streaming",
        "python-sdk"
      ],
      "category": "api",
      "original_file_path": "docs-assistants.md"
    },
    {
      "file_path": "579-docs-benchmarks.md",
      "title": "Benchmarks | liteLLM",
      "url": "https://docs.litellm.ai/docs/benchmarks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:10.973725075-03:00",
      "description": "Benchmarks for LiteLLM Gateway (Proxy Server) tested against a fake OpenAI endpoint.",
      "summary": "This document provides performance benchmarks for the LiteLLM Proxy Server, offering infrastructure scaling recommendations and comparisons against other tools like Portkey. It details latency metrics, hardware requirements for PostgreSQL and Redis, and methods for measuring proxy overhead during load testing.",
      "tags": [
        "litellm-proxy",
        "benchmarks",
        "performance-metrics",
        "infrastructure-scaling",
        "api-gateway",
        "load-testing",
        "redis-optimization"
      ],
      "category": "reference",
      "original_file_path": "docs-benchmarks.md"
    },
    {
      "file_path": "580-docs-completion-input.md",
      "title": "Input Params | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/input",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:28.023892794-03:00",
      "description": "Common Params",
      "summary": "This document provides a comprehensive reference for LiteLLM's completion parameters, detailing how OpenAI-style inputs are translated across various model providers. It covers function signatures, required fields like model and messages, and specific mapping for optional parameters such as temperature and streaming.",
      "tags": [
        "litellm",
        "openai-compatibility",
        "api-reference",
        "chat-completion",
        "parameter-mapping",
        "multi-provider"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-input.md"
    },
    {
      "file_path": "581-docs-completion-message-trimming.md",
      "title": "Trimming Input Messages | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/message_trimming",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:30.919235102-03:00",
      "description": "Use litellm.trim_messages() to ensure messages does not exceed a model's token limit or specified max_tokens",
      "summary": "This document explains how to use the litellm.trim_messages() utility function to ensure message lists stay within model token limits or a specified maximum token count.",
      "tags": [
        "litellm",
        "token-management",
        "message-trimming",
        "context-window",
        "python-sdk"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-message-trimming.md"
    },
    {
      "file_path": "582-docs-completion-output.md",
      "title": "Output | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/output",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:36.009725467-03:00",
      "description": "Format",
      "summary": "This document defines the standardized JSON response structure and data types for LiteLLM completion calls across different models. It outlines available fields such as message content, token usage, and latency, while noting compatibility with dictionary and class-based access.",
      "tags": [
        "litellm",
        "api-response",
        "json-schema",
        "llm-integration",
        "completion-endpoint",
        "metadata"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-output.md"
    },
    {
      "file_path": "583-docs-completion-prefix.md",
      "title": "Pre-fix Assistant Messages | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/prefix",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:38.596358581-03:00",
      "description": "Supported by:",
      "summary": "This document illustrates the chat completion response structure and provides instructions on how to verify model feature support using the litellm library.",
      "tags": [
        "litellm",
        "chat-completion",
        "api-response",
        "model-metadata",
        "prefix-support"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-prefix.md"
    },
    {
      "file_path": "584-docs-completion-stream.md",
      "title": "Streaming + Async | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/stream",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:45.736660176-03:00",
      "description": "| Feature | LiteLLM SDK | LiteLLM Proxy |",
      "summary": "This document provides a technical example of how LiteLLM handles repeated streaming chunks and the logic for triggering error states when predefined limits are exceeded.",
      "tags": [
        "litellm",
        "streaming",
        "error-handling",
        "api-limits",
        "python-sdk"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-stream.md"
    },
    {
      "file_path": "585-docs-completion-token-usage.md",
      "title": "Completion Token Usage & Cost | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion/token_usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:46.198345294-03:00",
      "description": "By default LiteLLM returns token usage in all completion requests (See here)",
      "summary": "This document outlines LiteLLM's helper functions for tokenization, token counting, and cost estimation across various LLM providers.",
      "tags": [
        "litellm",
        "tokenization",
        "cost-tracking",
        "token-counter",
        "llm-api",
        "pricing-management"
      ],
      "category": "reference",
      "original_file_path": "docs-completion-token-usage.md"
    },
    {
      "file_path": "586-docs-completion.md",
      "title": "Chat Completions | liteLLM",
      "url": "https://docs.litellm.ai/docs/completion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:18.133314333-03:00",
      "description": "Details on the completion() function",
      "summary": "This document provides an overview of the LiteLLM completion API, detailing input parameters, output formats, standardized usage tracking, and custom HTTP handler configurations.",
      "tags": [
        "litellm",
        "completion-api",
        "openai-compatibility",
        "api-usage",
        "http-configuration"
      ],
      "category": "api",
      "original_file_path": "docs-completion.md"
    },
    {
      "file_path": "587-docs-container-files.md",
      "title": "/containers/files | liteLLM",
      "url": "https://docs.litellm.ai/docs/container_files",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:53.100991827-03:00",
      "description": "Manage files within Code Interpreter containers. Files are created automatically when code interpreter generates outputs (charts, CSVs, images, etc.).",
      "summary": "This document provides a comprehensive guide and API reference for managing files within Code Interpreter containers using the LiteLLM SDK and Proxy. It details endpoints and methods for uploading, listing, retrieving, and deleting files associated with container sessions.",
      "tags": [
        "litellm",
        "code-interpreter",
        "file-management",
        "api-reference",
        "python-sdk",
        "container-files",
        "openai-compatibility"
      ],
      "category": "reference",
      "original_file_path": "docs-container-files.md"
    },
    {
      "file_path": "588-docs-data-retention.md",
      "title": "Data Retention Policy | liteLLM",
      "url": "https://docs.litellm.ai/docs/data_retention",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:59.298498604-03:00",
      "description": "LiteLLM Cloud",
      "summary": "This document outlines LiteLLM Cloud's policy regarding the retention, deletion, and protection of customer data across active, suspended, and closed accounts. It details specific timelines for data removal and provides options for enterprise customers to configure custom retention periods.",
      "tags": [
        "data-retention",
        "privacy-policy",
        "litellm-cloud",
        "data-protection",
        "account-management",
        "compliance"
      ],
      "category": "reference",
      "original_file_path": "docs-data-retention.md"
    },
    {
      "file_path": "589-docs-embedding-async-embedding.md",
      "title": "litellm.aembedding() | liteLLM",
      "url": "https://docs.litellm.ai/docs/embedding/async_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:05.113597275-03:00",
      "description": "LiteLLM provides an asynchronous version of the embedding function called aembedding",
      "summary": "This document explains how to perform asynchronous text embedding calls using LiteLLM's aembedding function.",
      "tags": [
        "litellm",
        "asynchronous",
        "embeddings",
        "python",
        "aembedding"
      ],
      "category": "api",
      "original_file_path": "docs-embedding-async-embedding.md"
    },
    {
      "file_path": "590-docs-embedding-moderation.md",
      "title": "litellm.moderation() | liteLLM",
      "url": "https://docs.litellm.ai/docs/embedding/moderation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:07.311869689-03:00",
      "description": "LiteLLM supports the moderation endpoint for OpenAI",
      "summary": "This document explains how to use LiteLLM to interact with the OpenAI moderation endpoint to check content for violations.",
      "tags": [
        "litellm",
        "openai",
        "moderation-api",
        "content-filtering",
        "python"
      ],
      "category": "reference",
      "original_file_path": "docs-embedding-moderation.md"
    },
    {
      "file_path": "591-docs-embedding-supported-embedding.md",
      "title": "/embeddings | liteLLM",
      "url": "https://docs.litellm.ai/docs/embedding/supported_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:08.082991732-03:00",
      "description": "Quick Start",
      "summary": "Technical documentation for LiteLLM's embedding functions, detailing synchronous and asynchronous usage, provider-specific configurations, and parameter definitions.",
      "tags": [
        "litellm",
        "embeddings",
        "python-sdk",
        "async-await",
        "api-proxy",
        "vector-search",
        "llm-integration"
      ],
      "category": "reference",
      "original_file_path": "docs-embedding-supported-embedding.md"
    },
    {
      "file_path": "592-docs-exception-mapping.md",
      "title": "Exception Mapping | liteLLM",
      "url": "https://docs.litellm.ai/docs/exception_mapping",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:10.078565273-03:00",
      "description": "LiteLLM maps exceptions across all providers to their OpenAI counterparts.",
      "summary": "This document details how LiteLLM maps exceptions from multiple LLM providers to standardized OpenAI-compatible error types for consistent error handling.",
      "tags": [
        "litellm",
        "exception-handling",
        "error-mapping",
        "openai-compatibility",
        "python-sdk",
        "api-errors"
      ],
      "category": "reference",
      "original_file_path": "docs-exception-mapping.md"
    },
    {
      "file_path": "593-docs-image-generation.md",
      "title": "Image Generations | liteLLM",
      "url": "https://docs.litellm.ai/docs/image_generation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:26.239267592-03:00",
      "description": "Overview",
      "summary": "This document provides documentation and usage instructions for the LiteLLM image_generation function and proxy server across multiple AI providers. It details available features, required parameters, and configuration steps for integrating image generation models into applications.",
      "tags": [
        "litellm",
        "image-generation",
        "api-integration",
        "openai",
        "azure-openai",
        "ai-proxy",
        "python-sdk"
      ],
      "category": "api",
      "original_file_path": "docs-image-generation.md"
    },
    {
      "file_path": "594-docs-image-variations.md",
      "title": "[BETA] Image Variations | liteLLM",
      "url": "https://docs.litellm.ai/docs/image_variations",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:26.763580417-03:00",
      "description": "OpenAI's /image/variations endpoint is now supported.",
      "summary": "This document demonstrates how to use the LiteLLM library to generate image variations using OpenAI's DALL-E and Topaz AI models.",
      "tags": [
        "litellm",
        "image-variation",
        "openai",
        "topaz",
        "dall-e-2",
        "python-sdk"
      ],
      "category": "api",
      "original_file_path": "docs-image-variations.md"
    },
    {
      "file_path": "595-docs-integrations.md",
      "title": "Integrations | liteLLM",
      "url": "https://docs.litellm.ai/docs/integrations/",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:28.423752153-03:00",
      "description": "This section covers integrations with various tools and services that can be used with LiteLLM (either Proxy or SDK).",
      "summary": "This document lists various third-party tool and service integrations compatible with LiteLLM, covering AI agent frameworks, development interfaces, and observability platforms.",
      "tags": [
        "litellm",
        "integrations",
        "observability",
        "monitoring",
        "ai-agents",
        "developer-tools"
      ],
      "category": "reference",
      "original_file_path": "docs-integrations.md"
    },
    {
      "file_path": "596-docs-interactions.md",
      "title": "/interactions | liteLLM",
      "url": "https://docs.litellm.ai/docs/interactions",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:32.643340008-03:00",
      "description": "| Feature | Supported | Notes |",
      "summary": "This document provides technical specifications and implementation guides for the LiteLLM Interactions API, covering Python SDK usage, proxy configuration, and multi-provider integration.",
      "tags": [
        "litellm",
        "interactions-api",
        "python-sdk",
        "ai-gateway",
        "llm-integration",
        "api-reference",
        "streaming"
      ],
      "category": "api",
      "original_file_path": "docs-interactions.md"
    },
    {
      "file_path": "597-docs-mcp-usage.md",
      "title": "Using your MCP | liteLLM",
      "url": "https://docs.litellm.ai/docs/mcp_usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:45.391200421-03:00",
      "description": "This document covers how to use LiteLLM as an MCP Gateway. You can see how to use it with Responses API, Cursor IDE, and OpenAI SDK.",
      "summary": "This document provides a curl command example for making a streaming request to an API endpoint that utilizes Model Context Protocol (MCP) tools.",
      "tags": [
        "api-request",
        "mcp-tools",
        "litellm-proxy",
        "curl-command",
        "streaming-api",
        "tool-calling"
      ],
      "category": "reference",
      "original_file_path": "docs-mcp-usage.md"
    },
    {
      "file_path": "598-docs-moderation.md",
      "title": "/moderations | liteLLM",
      "url": "https://docs.litellm.ai/docs/moderation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:47.654579569-03:00",
      "description": "Usage",
      "summary": "This document provides a sample JSON response format from a text moderation API, illustrating how content safety flags and confidence scores are structured.",
      "tags": [
        "moderation-api",
        "content-safety",
        "json-schema",
        "api-response",
        "text-classification",
        "safety-filters"
      ],
      "category": "reference",
      "original_file_path": "docs-moderation.md"
    },
    {
      "file_path": "599-docs-observability-telemetry.md",
      "title": "Telemetry | liteLLM",
      "url": "https://docs.litellm.ai/docs/observability/telemetry",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:37.800914971-03:00",
      "description": "There is no Telemetry on LiteLLM - no data is stored by us",
      "summary": "This document clarifies LiteLLM's policy regarding telemetry and data collection, stating that no user data is logged or sent to their servers.",
      "tags": [
        "litellm",
        "telemetry",
        "data-privacy",
        "logging-policy",
        "security"
      ],
      "category": "reference",
      "original_file_path": "docs-observability-telemetry.md"
    },
    {
      "file_path": "600-docs-ocr.md",
      "title": "/ocr | liteLLM",
      "url": "https://docs.litellm.ai/docs/ocr",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:46:43.14031781-03:00",
      "description": "| Feature | Supported |",
      "summary": "This document provides technical documentation for implementing Optical Character Recognition (OCR) using LiteLLM, covering Python SDK integration, proxy configuration, and detailed API request and response specifications.",
      "tags": [
        "litellm",
        "ocr",
        "python-sdk",
        "api-reference",
        "mistral-ai",
        "document-processing"
      ],
      "category": "api",
      "original_file_path": "docs-ocr.md"
    },
    {
      "file_path": "601-docs-project.md",
      "title": "Projects built on LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/project",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:04.17640031-03:00",
      "description": "Learn how to deploy + call models from different providers on LiteLLM",
      "summary": "This document provides a directory of open-source projects, SDKs, and frameworks that integrate with LiteLLM for building AI agents, RAG pipelines, and LLM applications.",
      "tags": [
        "ai-agents",
        "llm-integrations",
        "open-source-projects",
        "agent-frameworks",
        "developer-ecosystem",
        "rag-tools"
      ],
      "category": "reference",
      "original_file_path": "docs-project.md"
    },
    {
      "file_path": "602-docs-projects-LiteLLM-Proxy.md",
      "title": "LiteLLM Proxy | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/LiteLLM%20Proxy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:21.716586857-03:00",
      "description": "LiteLLM Proxy",
      "summary": "This document introduces the LiteLLM Proxy server, which provides a unified interface for over 50 LLM models with built-in features for error handling and caching.",
      "tags": [
        "litellm-proxy",
        "llm-gateway",
        "error-handling",
        "caching",
        "proxy-server"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-LiteLLM-Proxy.md"
    },
    {
      "file_path": "603-docs-projects-mini-swe-agent.md",
      "title": "mini-swe-agent | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/mini-swe-agent",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:24.514510195-03:00",
      "description": "mini-swe-agent The 100 line AI agent that solves GitHub issues & more.",
      "summary": "A lightweight 100-line Python AI agent designed to automate solving GitHub issues using bash commands and LiteLLM integration.",
      "tags": [
        "ai-agent",
        "python-automation",
        "github-issues",
        "litellm",
        "bash-scripting",
        "software-engineering"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-mini-swe-agent.md"
    },
    {
      "file_path": "604-docs-projects-pgai.md",
      "title": "pgai | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/pgai",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:32.314980452-03:00",
      "description": "pgai is a suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL.",
      "summary": "This document introduces pgai, a PostgreSQL toolset for building AI applications, and provides links to specific documentation for integrating litellm for model calling and embeddings.",
      "tags": [
        "pgai",
        "postgresql",
        "litellm",
        "rag",
        "embeddings",
        "semantic-search",
        "ai-development"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-pgai.md"
    },
    {
      "file_path": "605-docs-projects-PROMPTMETHEUS.md",
      "title": "PROMPTMETHEUS | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/PROMPTMETHEUS",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:34.838428892-03:00",
      "description": "ðŸ”¥ PROMPTMETHEUS â€“ Prompt Engineering IDE",
      "summary": "PROMPTMETHEUS is a Prompt Engineering IDE designed for composing, testing, optimizing, and deploying reliable prompts for various large language models.",
      "tags": [
        "prompt-engineering",
        "ide",
        "llm-development",
        "litellm",
        "workflow-automation",
        "prompt-optimization"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-PROMPTMETHEUS.md"
    },
    {
      "file_path": "606-docs-projects-Railtracks.md",
      "title": "Railtracks | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Railtracks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:37.489067755-03:00",
      "description": "Railtracks is an open-source agentic framework that helps developers build resilient agentic systems offering local and remote monitoring tools.",
      "summary": "This document introduces Railtracks, an open-source framework designed for building resilient agentic systems with integrated local and remote monitoring capabilities.",
      "tags": [
        "agentic-framework",
        "ai-agents",
        "open-source",
        "monitoring-tools",
        "system-resilience"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-Railtracks.md"
    },
    {
      "file_path": "607-docs-projects-SalesGPT.md",
      "title": "SalesGPT | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/SalesGPT",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:38.124885673-03:00",
      "description": "ðŸ¤– SalesGPT - Your Context-Aware AI Sales Assistant",
      "summary": "This document provides information about SalesGPT, an open-source, context-aware AI sales assistant project available on GitHub.",
      "tags": [
        "sales-gpt",
        "ai-assistant",
        "sales-automation",
        "llm",
        "open-source"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-SalesGPT.md"
    },
    {
      "file_path": "608-docs-projects-smolagents.md",
      "title": "ðŸ¤— Smolagents | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/smolagents",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:39.507947541-03:00",
      "description": "smolagents is a barebones library for agents. Agents write python code to call tools and orchestrate other agents.",
      "summary": "This document introduces smolagents, a lightweight library designed for building AI agents that write Python code to call tools and orchestrate tasks.",
      "tags": [
        "smolagents",
        "hugging-face",
        "python-agents",
        "llm-orchestration",
        "tool-calling"
      ],
      "category": "reference",
      "original_file_path": "docs-projects-smolagents.md"
    },
    {
      "file_path": "609-docs-projects.md",
      "title": "Projects Built on LiteLLM | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:04.619546178-03:00",
      "description": "EntoAI",
      "summary": "This document provides a directory of AI-powered tools and models designed for data interaction, codebase migration, and multi-modal instruction following.",
      "tags": [
        "ai-tools",
        "open-source",
        "multi-modal-models",
        "codebase-migration",
        "data-analysis",
        "machine-learning"
      ],
      "category": "reference",
      "original_file_path": "docs-projects.md"
    },
    {
      "file_path": "610-docs-proxy-cli.md",
      "title": "CLI Arguments | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/cli",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:21.342963584-03:00",
      "description": "Cli arguments,  --host, --port, --num_workers",
      "summary": "This document provides a comprehensive list of command-line interface arguments and environment variables available for configuring the litellm server and model parameters.",
      "tags": [
        "litellm",
        "cli-arguments",
        "server-configuration",
        "command-line-interface",
        "environment-variables"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-cli.md"
    },
    {
      "file_path": "611-docs-proxy-fallback-management.md",
      "title": "[New] Fallback Management Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/fallback_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:51:57.082198447-03:00",
      "description": "Dedicated endpoints for managing model fallbacks separately from the general configuration.",
      "summary": "This document describes the dedicated API endpoints for configuring and managing model fallbacks, enabling specialized error handling for context window limits and content policy violations. It provides technical details on request parameters, validation logic, and the advantages of these endpoints over general configuration updates.",
      "tags": [
        "model-fallbacks",
        "api-endpoints",
        "proxy-configuration",
        "error-handling",
        "database-storage",
        "content-policy",
        "request-validation"
      ],
      "category": "api",
      "original_file_path": "docs-proxy-fallback-management.md"
    },
    {
      "file_path": "612-docs-proxy-guardrails-pangea.md",
      "title": "Pangea | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/guardrails/pangea",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:25.725776625-03:00",
      "description": "The Pangea guardrail uses configurable detection policies (called recipes) from its AI Guard service to identify and mitigate risks in AI application traffic, including:",
      "summary": "This document provides an example of an API error response triggered by a Pangea guardrail violation during a prompt injection attempt.",
      "tags": [
        "pangea-ai-guard",
        "prompt-injection",
        "error-handling",
        "api-security",
        "security-policy"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-guardrails-pangea.md"
    },
    {
      "file_path": "613-docs-proxy-health.md",
      "title": "Health Checks | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/health",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:40.989796792-03:00",
      "description": "Use this to health check all LLMs defined in your config.yaml",
      "summary": "This document explains how to utilize and configure health check endpoints for the LiteLLM proxy to monitor model status, container liveness, and service readiness. It includes specific configurations for different model types and instructions for enabling background or shared health check states.",
      "tags": [
        "litellm",
        "health-check",
        "monitoring",
        "api-proxy",
        "system-health",
        "deployment"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-health.md"
    },
    {
      "file_path": "614-docs-proxy-logging-spec.md",
      "title": "StandardLoggingPayload Specification | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/logging_spec",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:51.153159198-03:00",
      "description": "Found under kwargs[\"standardloggingobject\"]. This is a standard payload, logged for every successful and failed response.",
      "summary": "This document defines the structure and fields of the StandardLoggingPayload used for tracking LLM requests, including cost breakdowns, token usage, and metadata for both successful and failed responses.",
      "tags": [
        "logging",
        "observability",
        "llm-monitoring",
        "cost-tracking",
        "api-schema",
        "telemetry"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-logging-spec.md"
    },
    {
      "file_path": "615-docs-proxy-metrics.md",
      "title": "ðŸ’¸ GET Daily Spend, Usage Metrics | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/metrics",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:52:56.412869342-03:00",
      "description": "Request Format",
      "summary": "This document illustrates the data structure used for reporting daily and total expenditures across various AI models and individual API keys.",
      "tags": [
        "billing-data",
        "usage-tracking",
        "cost-analysis",
        "api-metrics",
        "model-usage"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-metrics.md"
    },
    {
      "file_path": "616-docs-proxy-model-management.md",
      "title": "Model Management | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/model_management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:04.800365197-03:00",
      "description": "Add new models + Get model info without restarting proxy.",
      "summary": "This document explains how to use specific API endpoints to dynamically add new models and retrieve detailed model metadata without restarting the proxy server.",
      "tags": [
        "litellm",
        "model-management",
        "api-endpoints",
        "dynamic-configuration",
        "proxy-management"
      ],
      "category": "api",
      "original_file_path": "docs-proxy-model-management.md"
    },
    {
      "file_path": "617-docs-proxy-perf.md",
      "title": "LiteLLM Proxy Performance | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/perf",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:12.748804421-03:00",
      "description": "Throughput - 30% Increase",
      "summary": "This document provides performance benchmarks for the LiteLLM proxy, detailing its impact on throughput and latency compared to the raw OpenAI API.",
      "tags": [
        "litellm-proxy",
        "benchmarks",
        "throughput",
        "latency",
        "load-balancing",
        "performance-metrics"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-perf.md"
    },
    {
      "file_path": "618-docs-proxy-release-cycle.md",
      "title": "Release Cycle | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/release_cycle",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:29.850184445-03:00",
      "description": "Litellm Proxy has the following release cycle:",
      "summary": "This document outlines the release cycle and versioning strategy for LiteLLM Proxy, defining the criteria for nightly, release candidate, and stable versions.",
      "tags": [
        "release-cycle",
        "versioning",
        "litellm-proxy",
        "software-maintenance",
        "release-policy",
        "stable-releases"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-release-cycle.md"
    },
    {
      "file_path": "619-docs-proxy-request-headers.md",
      "title": "Request Headers | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/request_headers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:30.915169708-03:00",
      "description": "Special headers that are supported by LiteLLM.",
      "summary": "This document outlines the specific HTTP headers supported by LiteLLM for managing request timeouts, retries, spend tracking, and provider-specific configurations.",
      "tags": [
        "litellm",
        "http-headers",
        "header-forwarding",
        "request-management",
        "spend-tracking",
        "api-configuration"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-request-headers.md"
    },
    {
      "file_path": "620-docs-proxy-response-headers.md",
      "title": "Response Headers | liteLLM",
      "url": "https://docs.litellm.ai/docs/proxy/response_headers",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:53:32.569350377-03:00",
      "description": "When you make a request to the proxy, the proxy will return the following headers:",
      "summary": "This document outlines the standard and custom HTTP response headers returned by the LiteLLM proxy, detailing information on rate limits, latency, cost tracking, and retries.",
      "tags": [
        "litellm",
        "response-headers",
        "rate-limiting",
        "api-latency",
        "cost-tracking",
        "error-handling",
        "proxy-metadata"
      ],
      "category": "reference",
      "original_file_path": "docs-proxy-response-headers.md"
    },
    {
      "file_path": "621-docs-rag-ingest.md",
      "title": "/rag/ingest | liteLLM",
      "url": "https://docs.litellm.ai/docs/rag_ingest",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:07.195221763-03:00",
      "description": "All-in-one document ingestion pipeline: Upload â†’ Chunk â†’ Embed â†’ Vector Store",
      "summary": "This document details the LiteLLM RAG ingestion pipeline, providing API specifications and configuration options for uploading, chunking, and embedding documents across vector store providers like OpenAI, AWS Bedrock, and Vertex AI.",
      "tags": [
        "rag-ingestion",
        "vector-stores",
        "openai",
        "aws-bedrock",
        "vertex-ai",
        "embeddings",
        "api-reference"
      ],
      "category": "api",
      "original_file_path": "docs-rag-ingest.md"
    },
    {
      "file_path": "622-docs-rag-query.md",
      "title": "/rag/query | liteLLM",
      "url": "https://docs.litellm.ai/docs/rag_query",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:07.247989548-03:00",
      "description": "RAG Query endpoint: Search Vector Store â†’ (Rerank) â†’ LLM Completion",
      "summary": "This document provides technical specifications and implementation examples for the RAG Query endpoint, which integrates vector store searching, reranking, and LLM generation. It details request parameters, streaming options, and multi-provider support for building retrieval-augmented generation workflows.",
      "tags": [
        "rag",
        "vector-store",
        "reranking",
        "llm-api",
        "semantic-search",
        "retrieval-augmented-generation"
      ],
      "category": "api",
      "original_file_path": "docs-rag-query.md"
    },
    {
      "file_path": "623-docs-response-api-compact.md",
      "title": "/responses/compact | liteLLM",
      "url": "https://docs.litellm.ai/docs/response_api_compact",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:14.835769093-03:00",
      "description": "Compress conversation history using OpenAI's /responses/compact endpoint.",
      "summary": "This document explains how to use the OpenAI /responses/compact endpoint to compress conversation history and optimize token usage.",
      "tags": [
        "openai-api",
        "conversation-history",
        "message-compaction",
        "token-optimization",
        "data-compression"
      ],
      "category": "api",
      "original_file_path": "docs-response-api-compact.md"
    },
    {
      "file_path": "624-docs-search-linkup.md",
      "title": "Linkup Search | liteLLM",
      "url": "https://docs.litellm.ai/docs/search/linkup",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:31.018067532-03:00",
      "description": "Get API Key//linkup.so",
      "summary": "This code snippet demonstrates how to perform a search using the Linkup provider within the LiteLLM library, specifically detailing various configuration parameters like search depth, date filtering, and domain restrictions.",
      "tags": [
        "litellm",
        "linkup",
        "search-api",
        "python-sdk",
        "api-integration"
      ],
      "category": "api",
      "original_file_path": "docs-search-linkup.md"
    },
    {
      "file_path": "625-docs-simple-proxy.md",
      "title": "LiteLLM AI Gateway (LLM Proxy) | liteLLM",
      "url": "https://docs.litellm.ai/docs/simple_proxy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:49.899116733-03:00",
      "description": "OpenAI Proxy Server (LLM Gateway) to call 100+ LLMs in a unified interface & track spend, set budgets per virtual key/user",
      "summary": "This document serves as a comprehensive directory and index for LiteLLM Proxy documentation, providing navigation to setup guides, configuration references, and core features.",
      "tags": [
        "litellm-proxy",
        "llm-gateway",
        "api-management",
        "proxy-configuration",
        "deployment",
        "model-routing"
      ],
      "category": "reference",
      "original_file_path": "docs-simple-proxy.md"
    },
    {
      "file_path": "626-docs-supported-endpoints.md",
      "title": "Supported Endpoints | liteLLM",
      "url": "https://docs.litellm.ai/docs/supported_endpoints",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:51.798667612-03:00",
      "description": "Learn how to deploy + call models from different providers on LiteLLM",
      "summary": "This document describes how to manage files within Code Interpreter containers, covering automatically generated outputs like charts and datasets.",
      "tags": [
        "code-interpreter",
        "file-management",
        "containers",
        "litellm"
      ],
      "category": "api",
      "original_file_path": "docs-supported-endpoints.md"
    },
    {
      "file_path": "627-docs-text-completion.md",
      "title": "/completions | liteLLM",
      "url": "https://docs.litellm.ai/docs/text_completion",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:54:52.91637695-03:00",
      "description": "Overview",
      "summary": "This document illustrates the structure and schema of a response object from a text completion API, including metadata, generation results, and token usage statistics.",
      "tags": [
        "api-response",
        "text-completion",
        "json-structure",
        "openai-api",
        "token-usage"
      ],
      "category": "reference",
      "original_file_path": "docs-text-completion.md"
    },
    {
      "file_path": "628-docs-vector-store-files.md",
      "title": "/vector_stores/\\{vector_store_id\\}/files | liteLLM",
      "url": "https://docs.litellm.ai/docs/vector_store_files",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:53.678184345-03:00",
      "description": "Vector store files represent the individual files that live inside a vector store.",
      "summary": "This document outlines the API endpoints and client methods for managing vector store files, detailing operations such as creating, listing, retrieving, and deleting files through the LiteLLM proxy and OpenAI.",
      "tags": [
        "vector-stores",
        "litellm-proxy",
        "openai-api",
        "file-management",
        "rag-systems",
        "api-endpoints"
      ],
      "category": "api",
      "original_file_path": "docs-vector-store-files.md"
    },
    {
      "file_path": "629-docs-vector-stores-create.md",
      "title": "/vector_stores - Create Vector Store | liteLLM",
      "url": "https://docs.litellm.ai/docs/vector_stores/create",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:54.943458005-03:00",
      "description": "Create a vector store which can be used to store and search document chunks for retrieval-augmented generation (RAG) use cases.",
      "summary": "This document explains how to create vector stores for document chunk storage and retrieval in RAG systems, following the OpenAI API format and utilizing tools like litellm for testing.",
      "tags": [
        "vector-stores",
        "rag",
        "openai-api",
        "litellm",
        "document-retrieval",
        "chunking-strategy"
      ],
      "category": "api",
      "original_file_path": "docs-vector-stores-create.md"
    },
    {
      "file_path": "630-docs-vector-stores-search.md",
      "title": "/vector_stores/search - Search Vector Store | liteLLM",
      "url": "https://docs.litellm.ai/docs/vector_stores/search",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:55:55.229309523-03:00",
      "description": "Search a vector store for relevant chunks based on a query and file attributes filter. This is useful for retrieval-augmented generation (RAG) use cases.",
      "summary": "This document explains how to perform vector store searches using the LiteLLM Python SDK and Proxy Server to support retrieval-augmented generation (RAG) across multiple providers.",
      "tags": [
        "litellm",
        "vector-store",
        "rag",
        "search-api",
        "python-sdk",
        "proxy-server",
        "llm-integration"
      ],
      "category": "api",
      "original_file_path": "docs-vector-stores-search.md"
    },
    {
      "file_path": "631-embedding-supported-embedding.md",
      "title": "Embedding Models | liteLLM",
      "url": "https://docs.litellm.ai/embedding/supported_embedding",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:08.671941108-03:00",
      "description": "| Model Name           | Function Call                               | Required OS Variables                |",
      "summary": "This document provides the specific function call syntax and environment variable requirements for implementing the text-embedding-ada-002 model.",
      "tags": [
        "openai-api",
        "text-embeddings",
        "environment-variables",
        "model-configuration",
        "api-integration"
      ],
      "category": "reference",
      "original_file_path": "embedding-supported-embedding.md"
    },
    {
      "file_path": "632-release-notes-archive.md",
      "title": "Archive | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/archive",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:15.214809951-03:00",
      "description": "Archive",
      "summary": "This document provides a chronological index of LiteLLM release notes and version updates from 2024 to 2026. It tracks the introduction of new features, API support, performance improvements, and security integrations across the software's history.",
      "tags": [
        "release-notes",
        "changelog",
        "version-history",
        "litellm-updates",
        "api-evolution",
        "software-releases"
      ],
      "category": "reference",
      "original_file_path": "release-notes-archive.md"
    },
    {
      "file_path": "633-release-notes-tags-admin-ui.md",
      "title": "3 posts tagged with \"admin ui\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/admin-ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:17.158752364-03:00",
      "summary": "This document provides a comprehensive list of updates and new features for LiteLLM, covering model integrations, spend tracking improvements, and management UI enhancements.",
      "tags": [
        "litellm-updates",
        "changelog",
        "llm-integration",
        "spend-tracking",
        "api-translation",
        "model-support",
        "proxy-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-admin-ui.md"
    },
    {
      "file_path": "634-release-notes-tags-alerting.md",
      "title": "One post tagged with \"alerting\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/alerting",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:20.402321231-03:00",
      "summary": "This document outlines the updates and improvements for LiteLLM version 1.57.8, including new model support, integration enhancements, and management features for the LLM proxy.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "api-integration",
        "monitoring",
        "alerting",
        "secret-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-alerting.md"
    },
    {
      "file_path": "635-release-notes-tags-azure-storage.md",
      "title": "One post tagged with \"azure_storage\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/azure-storage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:20.46696793-03:00",
      "summary": "This document highlights key updates in the LiteLLM v1.55.8 stable release, including new features for remote prompt management via Langfuse and usage data logging to Azure Data Lake.",
      "tags": [
        "litellm",
        "stable-release",
        "azure-data-lake",
        "prompt-management",
        "logging",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-azure-storage.md"
    },
    {
      "file_path": "636-release-notes-tags-batch.md",
      "title": "One post tagged with \"batch\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/batch",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:21.404704558-03:00",
      "summary": "This document outlines the release notes and feature updates for LiteLLM versions 1.56.3 through 1.57.8, covering new model support, integration improvements, and enhanced proxy capabilities.",
      "tags": [
        "litellm-release",
        "model-updates",
        "llm-proxy",
        "api-integration",
        "secret-management",
        "alerting-integration",
        "observability"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-batch.md"
    },
    {
      "file_path": "637-release-notes-tags-cost-tracking.md",
      "title": "2 posts tagged with \"cost_tracking\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/cost-tracking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:26.665796348-03:00",
      "summary": "This document outlines the latest updates to LiteLLM, including SCIM integration for identity providers, enhanced team and tag-based usage tracking, and a unified Responses API for multi-provider support. It provides a technical overview of new model additions, spend monitoring improvements, and management UI enhancements.",
      "tags": [
        "litellm",
        "scim-integration",
        "usage-tracking",
        "unified-responses-api",
        "llm-monitoring",
        "identity-provisioning"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-cost-tracking.md"
    },
    {
      "file_path": "638-release-notes-tags-custom-prompt-management.md",
      "title": "One post tagged with \"custom_prompt_management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/custom-prompt-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:30.687458316-03:00",
      "summary": "This document defines the schema for an API usage report, detailing metrics such as token consumption, spend, and requests broken down by model and provider.",
      "tags": [
        "api-response",
        "usage-metrics",
        "token-consumption",
        "billing-data",
        "analytics-report",
        "model-monitoring"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-custom-prompt-management.md"
    },
    {
      "file_path": "639-release-notes-tags-db-schema.md",
      "title": "2 posts tagged with \"db schema\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/db-schema",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:32.109242704-03:00",
      "summary": "This document provides a detailed list of recent updates, bug fixes, and new feature releases for LiteLLM, covering model integrations, cost management, and security enhancements.",
      "tags": [
        "litellm-updates",
        "model-support",
        "spend-tracking",
        "api-translation",
        "guardrails",
        "release-notes"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-db-schema.md"
    },
    {
      "file_path": "640-release-notes-tags-finetuning.md",
      "title": "One post tagged with \"finetuning\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/finetuning",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:39.899736836-03:00",
      "summary": "This document provides release notes for LiteLLM, detailing new model support, API improvements, and integrations for alerting and secret management. It serves as a record of changes and new features introduced in versions v1.56.3 through v1.57.8.",
      "tags": [
        "release-notes",
        "litellm",
        "model-integration",
        "alerting",
        "proxy-configuration",
        "secret-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-finetuning.md"
    },
    {
      "file_path": "641-release-notes-tags-logging.md",
      "title": "4 posts tagged with \"logging\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/logging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:49.223461934-03:00",
      "summary": "This document details recent updates and new features for LiteLLM, including support for new models, improvements to spend tracking, and enhanced management UI capabilities.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "spend-tracking",
        "model-integration",
        "cloud-logging",
        "api-updates"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-logging.md"
    },
    {
      "file_path": "642-release-notes-tags-management-endpoints.md",
      "title": "3 posts tagged with \"management endpoints\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/management-endpoints",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:50.939456073-03:00",
      "summary": "This document outlines the feature updates and model integrations for LiteLLM, including new alerting integrations, prompt management tools, and enhanced secret management capabilities.",
      "tags": [
        "litellm-updates",
        "model-integration",
        "proxy-management",
        "alerting",
        "secret-management",
        "api-enhancements",
        "prompt-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-management-endpoints.md"
    },
    {
      "file_path": "643-release-notes-tags-mcp.md",
      "title": "One post tagged with \"mcp\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/mcp",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:51.322547761-03:00",
      "summary": "This document defines the data structure for tracking API usage metrics, including token consumption and cost breakdowns by model and provider.",
      "tags": [
        "api-usage",
        "billing-metrics",
        "token-tracking",
        "cost-monitoring",
        "data-schema"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-mcp.md"
    },
    {
      "file_path": "644-release-notes-tags-new-models.md",
      "title": "2 posts tagged with \"new models\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/new-models",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:51.975589095-03:00",
      "summary": "This document outlines new features, bug fixes, and model updates for LiteLLM, focusing on enhanced guardrails, observability integrations, and expanded provider support.",
      "tags": [
        "litellm",
        "release-notes",
        "guardrails",
        "llm-proxy",
        "logging",
        "observability",
        "prompt-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-new-models.md"
    },
    {
      "file_path": "645-release-notes-tags-prometheus.md",
      "title": "2 posts tagged with \"prometheus\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/prometheus",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:52.525695237-03:00",
      "summary": "This document details the feature updates and release notes for LiteLLM, highlighting new model support, proxy server enhancements, and integrations for alerting and secret management.",
      "tags": [
        "release-notes",
        "litellm",
        "model-updates",
        "proxy-server",
        "api-integration",
        "monitoring",
        "changelog"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-prometheus.md"
    },
    {
      "file_path": "646-release-notes-tags-reasoning-content.md",
      "title": "3 posts tagged with \"reasoning_content\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/reasoning-content",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:56.75654174-03:00",
      "summary": "This update for version 1.63.0 modifies the Anthropic thinking response structure to correctly return the signature block and align field naming with the official Anthropic API.",
      "tags": [
        "anthropic",
        "api-update",
        "extended-thinking",
        "response-format",
        "streaming",
        "bug-fix"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-reasoning-content.md"
    },
    {
      "file_path": "647-release-notes-tags-responses-api.md",
      "title": "3 posts tagged with \"responses_api\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/responses-api",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:00.203515855-03:00",
      "summary": "This document details the release notes for LiteLLM version 1.67.4-stable, featuring improvements in user management, Responses API load balancing, and UI session logging.",
      "tags": [
        "litellm",
        "release-notes",
        "load-balancing",
        "user-management",
        "session-logs",
        "model-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-responses-api.md"
    },
    {
      "file_path": "648-release-notes-tags-secret-management.md",
      "title": "2 posts tagged with \"secret management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/secret-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:00.119563284-03:00",
      "summary": "This document outlines the release notes and feature updates for LiteLLM between versions v1.56.3 and v1.57.8, covering model additions, integration improvements, and new system capabilities.",
      "tags": [
        "litellm",
        "release-notes",
        "model-updates",
        "proxy-server",
        "api-gateway",
        "secret-management",
        "alerting"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-secret-management.md"
    },
    {
      "file_path": "649-release-notes-tags-sso.md",
      "title": "2 posts tagged with \"sso\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/sso",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:06.959253179-03:00",
      "summary": "This document outlines the latest release updates for LiteLLM, featuring SCIM integration for automated identity management and enhanced team-based usage and spend tracking. It also details the new unified Responses API and various model-specific updates for providers like OpenAI, Azure, and Google.",
      "tags": [
        "litellm",
        "scim-integration",
        "usage-tracking",
        "api-updates",
        "spend-management",
        "identity-management",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-sso.md"
    },
    {
      "file_path": "650-release-notes-tags-team-management.md",
      "title": "One post tagged with \"team management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/team-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:07.416362981-03:00",
      "summary": "This document outlines several new enterprise-level features for LiteLLM including batch cost tracking, guardrail management endpoints, and expanded team administration capabilities.",
      "tags": [
        "litellm-proxy",
        "batches-api",
        "guardrails",
        "team-management",
        "custom-auth",
        "enterprise-features",
        "cost-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-team-management.md"
    },
    {
      "file_path": "651-release-notes-tags-team-models.md",
      "title": "One post tagged with \"team models\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/team-models",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:10.164959701-03:00",
      "summary": "This document outlines updates to LiteLLM v1.65.0 that restrict model creation to admins and expand management capabilities for team-specific models via the API.",
      "tags": [
        "litellm",
        "api-updates",
        "access-control",
        "model-management",
        "team-administration",
        "v1-65-0"
      ],
      "category": "api",
      "original_file_path": "release-notes-tags-team-models.md"
    },
    {
      "file_path": "652-release-notes-tags-thinking.md",
      "title": "3 posts tagged with \"thinking\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/thinking",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:10.53703329-03:00",
      "summary": "This document explains a fix in LiteLLM v1.63.0 that aligns the Anthropic thinking response structure with the official API by renaming the signature_delta field to signature.",
      "tags": [
        "litellm",
        "anthropic",
        "api-update",
        "streaming",
        "thinking-blocks"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-thinking.md"
    },
    {
      "file_path": "653-release-notes-tags-ui.md",
      "title": "4 posts tagged with \"ui\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/ui",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:16.076236504-03:00",
      "summary": "This document outlines the updates and new features for LiteLLM versions v1.56.3 through v1.57.8, covering model pricing updates, proxy enhancements, and new integrations.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "model-integration",
        "alerting",
        "secret-management",
        "logging"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-ui.md"
    },
    {
      "file_path": "654-release-notes-tags-unified-file-id.md",
      "title": "2 posts tagged with \"unified_file_id\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/unified-file-id",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:17.929655615-03:00",
      "summary": "This document outlines new features and updates for LiteLLM, including SCIM integration for automated provisioning, enhanced usage tracking by teams and tags, and a unified Responses API for multiple model providers.",
      "tags": [
        "litellm",
        "scim",
        "usage-tracking",
        "api-updates",
        "identity-management",
        "spend-monitoring",
        "llm-gateway"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags-unified-file-id.md"
    },
    {
      "file_path": "655-release-notes-tags.md",
      "title": "Tags | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:16.447285222-03:00",
      "summary": "This document provides an alphabetical index of tags used to categorize and navigate LiteLLM release notes across various features and services.",
      "tags": [
        "release-notes",
        "litellm",
        "tag-index",
        "documentation-navigation",
        "feature-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-tags.md"
    },
    {
      "file_path": "656-release-notes-v1-72-0-stable.md",
      "title": "v1.72.0-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-72-0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:23.564358751-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM version 1.72.0-stable, highlighting key updates such as vector store permissions, default aiohttp transport, and expanded support for various LLM providers and models.",
      "tags": [
        "litellm",
        "release-notes",
        "vector-store",
        "rate-limiting",
        "aiohttp",
        "api-gateway",
        "llm-integration"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-72-0-stable.md"
    },
    {
      "file_path": "657-release-notes-v1-72-2-stable.md",
      "title": "v1.72.2-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-72-2-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:26.118196354-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates in LiteLLM version 1.72.2, detailing performance optimizations for the messages API, enhanced multi-instance rate limiting, and new model support across various providers.",
      "tags": [
        "litellm",
        "release-notes",
        "api-performance",
        "rate-limiting",
        "model-integration",
        "audit-logs",
        "deployment"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-72-2-stable.md"
    },
    {
      "file_path": "658-release-notes-v1-73-6-stable.md",
      "title": "v1.73.6-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-73-6-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:28.295178055-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM v1.73.6, detailing new features such as gemini-cli support, batch API cost tracking, and various model pricing updates.",
      "tags": [
        "litellm",
        "release-notes",
        "gemini-cli",
        "batch-api",
        "cost-tracking",
        "model-updates",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-73-6-stable.md"
    },
    {
      "file_path": "659-release-notes-v1-74-0-stable.md",
      "title": "v1.74.0-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-74-0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:30.142128695-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates in LiteLLM version 1.74.0, detailing new features such as MCP Gateway namespacing, Azure Content Safety guardrails, and significant Python SDK performance improvements.",
      "tags": [
        "litellm",
        "release-notes",
        "mcp-gateway",
        "azure-content-safety",
        "python-sdk",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-74-0-stable.md"
    },
    {
      "file_path": "660-release-notes-v1-74-15.md",
      "title": "v1.74.15-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-74-15",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:32.132602093-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates in LiteLLM version 1.74.15-stable, highlighting new features such as user agent activity tracking, MCP gateway guardrails, and expanded model support for Google AI Studio and OpenRouter.",
      "tags": [
        "litellm",
        "release-notes",
        "model-deployment",
        "usage-tracking",
        "mcp-gateway",
        "prompt-management",
        "ai-infrastructure"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-74-15.md"
    },
    {
      "file_path": "661-release-notes-v1-74-3-stable.md",
      "title": "v1.74.3-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-74-3-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:32.327895331-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the features and improvements introduced in LiteLLM v1.74.3-stable, focusing on enhanced Model Context Protocol (MCP) management, cost tracking, and the new Model Hub v2. It provides technical details on new model provider integrations and administrative controls for managing LLM proxy access.",
      "tags": [
        "litellm",
        "release-notes",
        "mcp-gateway",
        "model-hub",
        "cost-tracking",
        "api-integration",
        "llm-proxy"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-74-3-stable.md"
    },
    {
      "file_path": "662-release-notes-v1-74-7.md",
      "title": "v1.74.7-stable",
      "url": "https://docs.litellm.ai/release_notes/v1-74-7",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:35.741913559-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates in LiteLLM version 1.74.7, featuring the introduction of a Vector Stores API, bulk user management tools, and enhanced health check reliability.",
      "tags": [
        "litellm",
        "release-notes",
        "vector-stores",
        "llm-providers",
        "health-check",
        "user-management",
        "api-updates"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-74-7.md"
    },
    {
      "file_path": "663-release-notes-v1-74-9.md",
      "title": "v1.74.9-stable - Auto-Router",
      "url": "https://docs.litellm.ai/release_notes/v1-74-9",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:35.842042532-03:00",
      "description": "Deploy this version",
      "summary": "This document details the updates in LiteLLM version 1.74.9-stable, focusing on new features like content-based auto-routing, model-level guardrails, and expanded LLM provider integrations. It also includes deployment instructions, pricing updates for various models, and bug fixes for the proxy and MCP gateway.",
      "tags": [
        "litellm",
        "release-notes",
        "auto-routing",
        "guardrails",
        "mcp-gateway",
        "llm-proxy",
        "model-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-74-9.md"
    },
    {
      "file_path": "664-release-notes-v1-75-5.md",
      "title": "v1.75.5-stable - Redis latency improvements",
      "url": "https://docs.litellm.ai/release_notes/v1-75-5",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:36.054198123-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM v1.75.5-stable, highlighting new provider support for Oracle Cloud and Digital Ocean and latency improvements through Redis in-memory caching. It provides critical update information regarding database migrations and a comprehensive list of newly supported models and their associated costs.",
      "tags": [
        "release-notes",
        "litellm",
        "llm-proxy",
        "redis-caching",
        "api-updates",
        "model-support"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-75-5.md"
    },
    {
      "file_path": "665-release-notes-v1-75-8.md",
      "title": "v1.75.8-stable - Team Member Rate Limits",
      "url": "https://docs.litellm.ai/release_notes/v1-75-8",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:40.055478132-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates and new features in LiteLLM version 1.75.8, including support for GPT-5 models, team member rate limits, and expanded provider integrations. It also details performance enhancements and improvements to the management UI and MCP gateway.",
      "tags": [
        "litellm-release",
        "model-support",
        "rate-limiting",
        "api-gateway",
        "llm-providers",
        "gpt-5"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-75-8.md"
    },
    {
      "file_path": "666-release-notes-v1-76-1.md",
      "title": "v1.76.1-stable - Gemini 2.5 Flash Image",
      "url": "https://docs.litellm.ai/release_notes/v1-76-1",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:41.843091318-03:00",
      "description": "Deploy this version",
      "summary": "Release notes for LiteLLM v1.76.1 detailing major performance optimizations, support for new models like Gemini 2.5 and GPT Realtime, and new provider integrations.",
      "tags": [
        "litellm",
        "release-notes",
        "performance-optimization",
        "model-support",
        "ai-proxy",
        "api-updates"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-76-1.md"
    },
    {
      "file_path": "667-release-notes-v1-77-2.md",
      "title": "v1.77.2-stable - Bedrock Batches API",
      "url": "https://docs.litellm.ai/release_notes/v1-77-2",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:43.751710816-03:00",
      "description": "Deploy this version",
      "summary": "This document provides the release notes for LiteLLM version 1.77.2, detailing new model support, Bedrock Batches API integration, and enhanced tiered pricing for Qwen models.",
      "tags": [
        "litellm-release",
        "changelog",
        "api-integration",
        "model-deployment",
        "cost-tracking",
        "performance-optimization"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-77-2.md"
    },
    {
      "file_path": "668-release-notes-v1-77-3.md",
      "title": "v1.77.3-stable - Priority Based Rate Limiting",
      "url": "https://docs.litellm.ai/release_notes/v1-77-3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:47.62802492-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM version 1.77.3, highlighting performance improvements, priority quota reservation, and new provider integrations. It also details specific bug fixes and feature updates across various LLM providers and API endpoints.",
      "tags": [
        "litellm",
        "release-notes",
        "performance-optimization",
        "rate-limiting",
        "model-support",
        "api-updates"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-77-3.md"
    },
    {
      "file_path": "669-release-notes-v1-78-5.md",
      "title": "v1.78.5-stable - Native OCR Support",
      "url": "https://docs.litellm.ai/release_notes/v1-78-5",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:50.21268974-03:00",
      "description": "Deploy this version",
      "summary": "Release notes for LiteLLM version 1.78.5-stable detailing new model support for Claude Haiku 4.5 and GPT-5-Codex, performance optimizations, and the introduction of native OCR endpoints.",
      "tags": [
        "litellm-release",
        "model-support",
        "ocr-api",
        "performance-optimization",
        "cost-tracking",
        "multi-provider-api"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-78-5.md"
    },
    {
      "file_path": "670-release-notes-v1-79-0.md",
      "title": "v1.79.0-stable - Search APIs",
      "url": "https://docs.litellm.ai/release_notes/v1-79-0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:52.594322852-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates and new features in LiteLLM version 1.79.0, including native search APIs, expanded guardrail support, and new model integrations for providers like Bedrock and Azure.",
      "tags": [
        "litellm",
        "release-notes",
        "model-integration",
        "api-updates",
        "llm-gateway",
        "docker-deployment"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-79-0.md"
    },
    {
      "file_path": "671-release-notes-v1-79-1.md",
      "title": "v1.79.1-stable - Guardrail Playground",
      "url": "https://docs.litellm.ai/release_notes/v1-79-1",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:54.902971324-03:00",
      "description": "Deploy this version",
      "summary": "This document details the release notes for LiteLLM v1.79.1, outlining new model support, provider updates, and critical bug fixes. It provides deployment instructions via Docker and highlights key enhancements such as Container API support and image generation capabilities.",
      "tags": [
        "litellm-release",
        "changelog",
        "model-support",
        "container-api",
        "deployment",
        "api-integration"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-79-1.md"
    },
    {
      "file_path": "672-release-notes-v1-80-10.md",
      "title": "[Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore",
      "url": "https://docs.litellm.ai/release_notes/v1-80-10",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:58.154486994-03:00",
      "description": "Deploy this version",
      "summary": "This document details the release notes for LiteLLM version 1.80.10.rc.1, focusing on the new Agent Gateway features, expanded provider integrations, and the addition of over 270 new language models.",
      "tags": [
        "litellm",
        "release-notes",
        "agent-gateway",
        "llm-providers",
        "model-updates",
        "api-endpoints",
        "a2a-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-10.md"
    },
    {
      "file_path": "673-release-notes-v1-80-11.md",
      "title": "v1.80.11-stable - Google Interactions API",
      "url": "https://docs.litellm.ai/release_notes/v1-80-11",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:01.337198928-03:00",
      "description": "Deploy this version",
      "summary": "Release notes for LiteLLM v1.80.11 detailing performance optimizations, extensive new provider support including Gemini and Stability AI, and significant memory reductions through component lazy loading.",
      "tags": [
        "litellm-release",
        "model-providers",
        "performance-optimization",
        "lazy-loading",
        "image-generation",
        "ai-guardrails",
        "api-reference",
        "cloud-integration"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-11.md"
    },
    {
      "file_path": "674-release-notes-v1-80-15.md",
      "title": "v1.80.15-stable - Manus API Support",
      "url": "https://docs.litellm.ai/release_notes/v1-80-15",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:02.233902752-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release highlights for LiteLLM version 1.80.15-stable.1, detailing performance improvements, new model provider integrations, and updated API endpoints.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "model-providers",
        "api-updates",
        "performance-optimization",
        "deployment"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-15.md"
    },
    {
      "file_path": "675-release-notes-v1-80-5.md",
      "title": "v1.80.5-stable - Gemini 3.0 Support",
      "url": "https://docs.litellm.ai/release_notes/v1-80-5",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:03.686220976-03:00",
      "description": "Deploy this version",
      "summary": "This document provides release notes for LiteLLM version 1.80.5, detailing new prompt management features, significant latency optimizations for real-time endpoints, and expanded model support.",
      "tags": [
        "litellm",
        "release-notes",
        "prompt-management",
        "latency-optimization",
        "model-comparison",
        "ai-gateway",
        "llm-deployment"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1-80-5.md"
    },
    {
      "file_path": "676-release-notes-v1.55.10.md",
      "title": "v1.55.10",
      "url": "https://docs.litellm.ai/release_notes/v1.55.10",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:07.484029346-03:00",
      "description": "batches, guardrails, team management, custom auth",
      "summary": "This document outlines recent updates to LiteLLM features, including batch API cost tracking, new guardrail endpoints, and enhanced team management capabilities for enterprise users.",
      "tags": [
        "litellm",
        "batches-api",
        "cost-tracking",
        "team-management",
        "custom-auth",
        "guardrails",
        "enterprise-features"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.55.10.md"
    },
    {
      "file_path": "677-release-notes-v1.56.1.md",
      "title": "v1.56.1",
      "url": "https://docs.litellm.ai/release_notes/v1.56.1",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:09.637432518-03:00",
      "description": "key management, budgets/rate limits, logging, guardrails",
      "summary": "This document outlines updates to LiteLLM including budget and rate limit tier definitions, logging enhancements for fine-tuning endpoints, and dynamic parameters for guardrails.",
      "tags": [
        "litellm",
        "rate-limiting",
        "budget-management",
        "logging",
        "guardrails",
        "fine-tuning",
        "observability"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.56.1.md"
    },
    {
      "file_path": "678-release-notes-v1.57.8-stable.md",
      "title": "v1.57.8-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.57.8-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:16.616281073-03:00",
      "description": "alerting, prometheus, secret management, management endpoints, ui, prompt management, finetuning, batch",
      "summary": "This document outlines the release notes and feature updates for LiteLLM, detailing new model support, proxy improvements, and integrations for alerting, secret management, and monitoring.",
      "tags": [
        "release-notes",
        "litellm-updates",
        "llm-proxy",
        "api-integration",
        "monitoring",
        "alerting",
        "secret-management"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.57.8-stable.md"
    },
    {
      "file_path": "679-release-notes-v1.59.8-stable.md",
      "title": "v1.59.8-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.59.8-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:20.677577236-03:00",
      "description": "Get a 7 day free trial for LiteLLM Enterprise here.",
      "summary": "This document outlines the latest feature updates and bug fixes for LiteLLM, covering new model integrations, spend tracking improvements, and proxy management enhancements.",
      "tags": [
        "litellm",
        "release-notes",
        "model-integration",
        "api-management",
        "cost-tracking",
        "security-updates"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.59.8-stable.md"
    },
    {
      "file_path": "680-release-notes-v1.63.0.md",
      "title": "v1.63.0 - Anthropic 'thinking' response update",
      "url": "https://docs.litellm.ai/release_notes/v1.63.0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:22.801314169-03:00",
      "description": "v1.63.0 fixes Anthropic 'thinking' response on streaming to return the signature block. Github Issue",
      "summary": "This document outlines an update to LiteLLM version 1.63.0 that aligns Anthropic streaming responses with official specifications by renaming the signature_delta field to signature. It ensures the signature block is correctly returned during extended thinking response streaming.",
      "tags": [
        "litellm",
        "anthropic-api",
        "streaming-responses",
        "extended-thinking",
        "api-update",
        "bug-fix"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.63.0.md"
    },
    {
      "file_path": "681-release-notes-v1.65.0-stable.md",
      "title": "v1.65.0-stable - Model Context Protocol",
      "url": "https://docs.litellm.ai/release_notes/v1.65.0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:30.00368969-03:00",
      "description": "v1.65.0-stable is live now. Here are the key highlights of this release:",
      "summary": "This document illustrates the structure of an API usage report, detailing metrics such as token consumption and financial spend broken down by date and model.",
      "tags": [
        "usage-metrics",
        "api-response",
        "billing-data",
        "token-tracking",
        "spend-analysis"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.65.0-stable.md"
    },
    {
      "file_path": "682-release-notes-v1.65.0.md",
      "title": "v1.65.0 - Team Model Add - update",
      "url": "https://docs.litellm.ai/release_notes/v1.65.0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:28.687966123-03:00",
      "description": "v1.65.0 updates the /model/new endpoint to prevent non-team admins from creating team models.",
      "summary": "This document details updates to model management endpoints in version 1.65.0, focusing on expanded permissions for team admins to create, update, and delete models via the API and UI.",
      "tags": [
        "litellm",
        "model-management",
        "access-control",
        "api-updates",
        "permissions",
        "team-administration"
      ],
      "category": "api",
      "original_file_path": "release-notes-v1.65.0.md"
    },
    {
      "file_path": "683-release-notes-v1.66.0-stable.md",
      "title": "v1.66.0-stable - Realtime API Cost Tracking",
      "url": "https://docs.litellm.ai/release_notes/v1.66.0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:31.820994098-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates in LiteLLM version v1.66.0-stable, including the introduction of Realtime API cost tracking, Microsoft SSO auto-sync, and security patches for known vulnerabilities. It also details new model support for xAI grok-3 and improvements to spend management and the proxy UI.",
      "tags": [
        "litellm-release",
        "realtime-api",
        "cost-tracking",
        "microsoft-sso",
        "grok-3",
        "security-fixes",
        "spend-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.66.0-stable.md"
    },
    {
      "file_path": "684-release-notes-v1.68.0-stable.md",
      "title": "v1.68.0-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.68.0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:37.170913026-03:00",
      "description": "Deploy this version",
      "summary": "This document provides the release notes for LiteLLM version 1.68.0-stable, detailing new features such as Bedrock Knowledge Base support and improved multi-instance rate limiting. It also covers updates to various model providers, API endpoints, and spend tracking functionality.",
      "tags": [
        "litellm-release",
        "bedrock-knowledge-base",
        "rate-limiting",
        "llm-proxy",
        "model-support",
        "api-updates",
        "vector-stores",
        "spend-tracking"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.68.0-stable.md"
    },
    {
      "file_path": "685-release-notes-v1.69.0-stable.md",
      "title": "v1.69.0-stable - Loadbalance Batch API Models",
      "url": "https://docs.litellm.ai/release_notes/v1.69.0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:39.413311833-03:00",
      "description": "Deploy this version",
      "summary": "This document provides the release notes for LiteLLM version 1.69.0-stable, detailing new features like Batch API load balancing and support for providers such as Nscale and updated Gemini models. It also highlights bug fixes and performance enhancements across the proxy's management UI and various LLM endpoints.",
      "tags": [
        "litellm",
        "release-notes",
        "batch-api",
        "model-updates",
        "llm-proxy",
        "api-integration"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.69.0-stable.md"
    },
    {
      "file_path": "686-release-notes-v1.70.1-stable.md",
      "title": "v1.70.1-stable - Gemini Realtime API Support",
      "url": "https://docs.litellm.ai/release_notes/v1.70.1-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:40.05772369-03:00",
      "description": "Deploy this version",
      "summary": "This document details the features and updates in LiteLLM v1.70.1-stable, including Gemini Realtime API support, spend log retention settings, and PII masking enhancements.",
      "tags": [
        "litellm",
        "release-notes",
        "gemini-realtime",
        "pii-masking",
        "spend-tracking",
        "model-integration"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.70.1-stable.md"
    },
    {
      "file_path": "687-release-notes-v1.71.1-stable.md",
      "title": "v1.71.1-stable - 2x Higher Requests Per Second (RPS)",
      "url": "https://docs.litellm.ai/release_notes/v1.71.1-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:40.973498843-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the release notes for LiteLLM v1.71.1-stable, detailing performance enhancements via aiohttp, new file permission features, and updated support for numerous LLM providers and models.",
      "tags": [
        "litellm-release",
        "performance-scaling",
        "aiohttp-transport",
        "file-permissions",
        "model-updates",
        "api-reference"
      ],
      "category": "reference",
      "original_file_path": "release-notes-v1.71.1-stable.md"
    },
    {
      "file_path": "688-release-notes.md",
      "title": "Release Notes | liteLLM",
      "url": "https://docs.litellm.ai/release_notes",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:13.959266166-03:00",
      "description": "Blog",
      "summary": "This document outlines the release notes and technical updates for LiteLLM version 1.81.0, focusing on performance optimizations, new model support, and administrative auditing features.",
      "tags": [
        "litellm-updates",
        "claude-code",
        "performance-optimization",
        "image-handling",
        "api-gateway",
        "model-support",
        "audit-logs"
      ],
      "category": "reference",
      "original_file_path": "release-notes.md"
    },
    {
      "file_path": "689-token-usage.md",
      "title": "Token Usage | liteLLM",
      "url": "https://docs.litellm.ai/token_usage",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:41.85945764-03:00",
      "description": "By default LiteLLM returns token usage in all completion requests (See here)",
      "summary": "This document details LiteLLM's helper functions for calculating token usage and the financial costs associated with LLM API calls across different providers.",
      "tags": [
        "litellm",
        "token-usage",
        "cost-calculation",
        "token-counter",
        "api-monitoring",
        "usage-tracking"
      ],
      "category": "reference",
      "original_file_path": "token-usage.md"
    },
    {
      "file_path": "690-release-notes-tags-credential-management.md",
      "title": "2 posts tagged with \"credential management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/credential-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:27.782403738-03:00",
      "summary": "This document outlines the updates in LiteLLM version 1.63.14-stable, covering new model support, performance optimizations for usage-based routing, and enhancements to spend tracking and logging.",
      "tags": [
        "release-notes",
        "litellm",
        "llm-proxy",
        "mcp-support",
        "performance-optimization",
        "bug-fixes",
        "azure-openai"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-credential-management.md"
    },
    {
      "file_path": "691-release-notes-tags-prompt-management.md",
      "title": "One post tagged with \"prompt management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/prompt-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:54.71224145-03:00",
      "summary": "This document outlines the release updates and new features for LiteLLM, covering model pricing changes, provider integrations, observability enhancements, and improvements to proxy management.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "model-updates",
        "observability",
        "secret-management",
        "alerting"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-prompt-management.md"
    },
    {
      "file_path": "692-release-notes-tags-session-management.md",
      "title": "One post tagged with \"session_management\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/session-management",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:04.079442715-03:00",
      "summary": "This document provides the release notes for LiteLLM version 1.67.4-stable, detailing new features like improved user management, Responses API load balancing, and UI session logging.",
      "tags": [
        "release-notes",
        "litellm",
        "api-updates",
        "load-balancing",
        "user-management",
        "model-integration"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-session-management.md"
    },
    {
      "file_path": "693-release-notes-tags-batches.md",
      "title": "One post tagged with \"batches\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/batches",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:21.598440499-03:00",
      "summary": "This document outlines new enterprise features and updates for LiteLLM, including cost tracking for the Batches API, team management capabilities, and enhanced custom authentication controls.",
      "tags": [
        "litellm",
        "enterprise-features",
        "batches-api",
        "team-management",
        "custom-auth",
        "cost-tracking"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-batches.md"
    },
    {
      "file_path": "694-release-notes-v1-76-3.md",
      "title": "v1.76.3-stable - Performance, Video Generation & CloudZero Integration",
      "url": "https://docs.litellm.ai/release_notes/v1-76-3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:43.234929447-03:00",
      "description": "This release has a known issue where startup is leading to Out of Memory errors when deploying on Kubernetes. We recommend waiting before upgrading to this version.",
      "summary": "Release notes for LiteLLM v1.76.3 detailing performance optimizations, video generation support, new model integrations, and critical security updates.",
      "tags": [
        "litellm",
        "release-notes",
        "performance-optimization",
        "video-generation",
        "model-support",
        "bug-fixes"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-76-3.md"
    },
    {
      "file_path": "695-blog-archive.md",
      "title": "Archive | liteLLM",
      "url": "https://docs.litellm.ai/blog/archive",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:40:46.019749993-03:00",
      "description": "Archive",
      "summary": "This document provides a chronological log of LiteLLM updates and blog posts documenting day-zero support for new AI models including Gemini 3 and Claude 4.5.",
      "tags": [
        "litellm",
        "model-support",
        "gemini-3",
        "claude-4-5",
        "release-notes",
        "ai-integration"
      ],
      "category": "other",
      "original_file_path": "blog-archive.md"
    },
    {
      "file_path": "696-release-notes-tags-claude-3-7-sonnet.md",
      "title": "3 posts tagged with \"claude-3-7-sonnet\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/claude-3-7-sonnet",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:26.562549225-03:00",
      "summary": "This update for LiteLLM v1.63.0 fixes Anthropic thinking responses during streaming and aligns the response structure by renaming signature_delta to signature.",
      "tags": [
        "litellm",
        "anthropic",
        "extended-thinking",
        "api-update",
        "streaming"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-claude-3-7-sonnet.md"
    },
    {
      "file_path": "697-release-notes-tags-fallbacks.md",
      "title": "One post tagged with \"fallbacks\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/fallbacks",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:37.003756561-03:00",
      "summary": "Summarizes key updates in the LiteLLM v1.55.8-stable release, including prompt management enhancements and Azure Data Lake integration for usage logging.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "azure-data-lake",
        "prompt-management",
        "docker"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-fallbacks.md"
    },
    {
      "file_path": "698-release-notes-tags-guardrails.md",
      "title": "3 posts tagged with \"guardrails\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/guardrails",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:42.290697791-03:00",
      "summary": "This document details a series of product updates and new features for LiteLLM, focusing on guardrail tracing, budget tiers, logging for batch and finetuning APIs, and enhanced team management capabilities.",
      "tags": [
        "litellm",
        "guardrails",
        "logging",
        "api-updates",
        "rate-limiting",
        "team-management",
        "proxy-server"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-guardrails.md"
    },
    {
      "file_path": "699-release-notes-tags-humanloop.md",
      "title": "One post tagged with \"humanloop\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/humanloop",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:42.490596609-03:00",
      "summary": "This document outlines the release notes and new features for LiteLLM, including model updates, provider integrations, and improvements to the proxy server's management and monitoring capabilities.",
      "tags": [
        "litellm",
        "changelog",
        "llm-proxy",
        "monitoring",
        "alerting",
        "secret-management",
        "model-integration"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-humanloop.md"
    },
    {
      "file_path": "700-release-notes-tags-langfuse.md",
      "title": "3 posts tagged with \"langfuse\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/langfuse",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:45.231108514-03:00",
      "summary": "This document highlights key updates in the LiteLLM v1.55.8 stable release, including Langfuse integration for prompt management and Azure Data Lake support for usage logging.",
      "tags": [
        "litellm",
        "release-notes",
        "langfuse",
        "azure-data-lake",
        "logging",
        "prompt-management"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-langfuse.md"
    },
    {
      "file_path": "701-release-notes-tags-llm-translation.md",
      "title": "3 posts tagged with \"llm translation\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/llm-translation",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:45.788643514-03:00",
      "summary": "LiteLLM version 1.63.0 updates the Anthropic extended thinking response format by renaming signature_delta to signature for consistency with the official API.",
      "tags": [
        "litellm",
        "anthropic",
        "api-integration",
        "extended-thinking",
        "changelog"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-llm-translation.md"
    },
    {
      "file_path": "702-release-notes-tags-rerank.md",
      "title": "One post tagged with \"rerank\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/rerank",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:00.182509039-03:00",
      "summary": "This document outlines the updates in LiteLLM version 1.61.20-stable, highlighting new model support for Claude 3.7 Sonnet and GPT-4.5, reasoning content translation, and various UI and performance enhancements.",
      "tags": [
        "litellm",
        "release-notes",
        "claude-3-7",
        "model-support",
        "llm-translation",
        "cost-tracking",
        "proxy-server"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-rerank.md"
    },
    {
      "file_path": "703-release-notes-tags-snowflake.md",
      "title": "2 posts tagged with \"snowflake\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/snowflake",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:06.691429101-03:00",
      "summary": "This document outlines the updates, new model integrations, and performance improvements introduced in the LiteLLM version 1.63.14-stable release.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "model-integration",
        "performance-optimization",
        "bug-fixes"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-snowflake.md"
    },
    {
      "file_path": "704-release-notes-tags-thinking-content.md",
      "title": "2 posts tagged with \"thinking content\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/thinking-content",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:15.25404899-03:00",
      "summary": "This document outlines the updates and bug fixes introduced in LiteLLM v1.63.14-stable, including new model support, performance optimizations for routing, and enhanced LLM translation features.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "api-integration",
        "performance-optimization",
        "bug-fixes"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-thinking-content.md"
    },
    {
      "file_path": "705-release-notes-tags-ui-improvements.md",
      "title": "One post tagged with \"ui_improvements\" | liteLLM",
      "url": "https://docs.litellm.ai/release_notes/tags/ui-improvements",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:16.10341835-03:00",
      "summary": "This document details the features and fixes in LiteLLM version 1.67.4, including enhanced user management tools and session-persistent load balancing. It also covers updated cost tracking for various AI providers and security improvements for the administrative dashboard.",
      "tags": [
        "litellm-update",
        "load-balancing",
        "user-management",
        "docker-deployment",
        "session-tracking",
        "api-integration"
      ],
      "category": "other",
      "original_file_path": "release-notes-tags-ui-improvements.md"
    },
    {
      "file_path": "706-release-notes-v1-76-0.md",
      "title": "v1.76.0-stable - RPS Improvements",
      "url": "https://docs.litellm.ai/release_notes/v1-76-0",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:41.17070227-03:00",
      "description": "LiteLLM is hiring a Founding Backend Engineer, in San Francisco.",
      "summary": "Release notes detailing recent bug fixes, new model integrations, and feature updates for the LiteLLM proxy and supported AI providers.",
      "tags": [
        "litellm",
        "changelog",
        "release-notes",
        "model-updates",
        "bug-fixes",
        "llm-proxy"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-76-0.md"
    },
    {
      "file_path": "707-release-notes-v1-77-7.md",
      "title": "v1.77.7-stable - 2.9x Lower Median Latency",
      "url": "https://docs.litellm.ai/release_notes/v1-77-7",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:49.772965476-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the updates and key highlights for LiteLLM version 1.77.7.rc.1, focusing on significant performance optimizations, the Dynamic Rate Limiter v3, and expanded model support for Claude Sonnet 4.5.",
      "tags": [
        "litellm",
        "release-notes",
        "performance-optimization",
        "claude-sonnet-4-5",
        "rate-limiting",
        "api-gateway",
        "mcp-server"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-77-7.md"
    },
    {
      "file_path": "708-release-notes-v1-79-3.md",
      "title": "v1.79.3-stable - Built-in Guardrails on AI Gateway",
      "url": "https://docs.litellm.ai/release_notes/v1-79-3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:42:57.08211622-03:00",
      "description": "Deploy this version",
      "summary": "This document details the release notes for LiteLLM version 1.79.3, highlighting new built-in guardrails, significant performance optimizations for the responses API, and expanded model support across multiple providers.",
      "tags": [
        "release-notes",
        "litellm",
        "ai-gateway",
        "performance-optimization",
        "guardrails",
        "model-updates",
        "changelog"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1-79-3.md"
    },
    {
      "file_path": "709-release-notes-v1.55.8-stable.md",
      "title": "v1.55.8-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.55.8-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:08.340267641-03:00",
      "description": "A new LiteLLM Stable release just went out. Here are 5 updates since v1.52.2-stable.",
      "summary": "This document summarizes five key updates in the LiteLLM v1.55.8-stable release, including prompt management, model fallbacks, and new provider support.",
      "tags": [
        "litellm",
        "release-notes",
        "prompt-management",
        "llm-proxy",
        "azure-data-lake",
        "docker"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.55.8-stable.md"
    },
    {
      "file_path": "710-release-notes-v1.56.3.md",
      "title": "v1.56.3",
      "url": "https://docs.litellm.ai/release_notes/v1.56.3",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:12.504949527-03:00",
      "description": "guardrails, logging, virtual key management, new models",
      "summary": "This document details recent updates and new features for LiteLLM, including guardrail tracing, API endpoints for guardrail listing, and support for new language models.",
      "tags": [
        "litellm",
        "guardrails",
        "api-updates",
        "model-support",
        "logging",
        "key-management"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.56.3.md"
    },
    {
      "file_path": "711-release-notes-v1.56.4.md",
      "title": "v1.56.4",
      "url": "https://docs.litellm.ai/release_notes/v1.56.4",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:15.250551297-03:00",
      "description": "deepgram, fireworks ai, vision, admin ui, dependency upgrades",
      "summary": "This document outlines recent updates to LiteLLM, including support for Deepgram speech-to-text and Fireworks AI vision models, alongside security fixes and Proxy Admin UI improvements.",
      "tags": [
        "litellm",
        "deepgram",
        "fireworks-ai",
        "speech-to-text",
        "vision-models",
        "dependency-upgrade",
        "admin-ui"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.56.4.md"
    },
    {
      "file_path": "712-release-notes-v1.57.7.md",
      "title": "v1.57.7",
      "url": "https://docs.litellm.ai/release_notes/v1.57.7",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:15.435152843-03:00",
      "description": "langfuse, management endpoints, ui, prometheus, secret management",
      "summary": "This document details recent updates to LiteLLM, including improvements to Langfuse prompt management, team and organization UI features, Hashicorp Vault support, and custom Prometheus metrics.",
      "tags": [
        "litellm",
        "langfuse",
        "team-management",
        "hashicorp-vault",
        "prometheus-metrics",
        "prompt-management",
        "secret-management"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.57.7.md"
    },
    {
      "file_path": "713-release-notes-v1.61.20-stable.md",
      "title": "v1.61.20-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.61.20-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:22.698209077-03:00",
      "description": "These are the changes since v1.61.13-stable.",
      "summary": "This document outlines the updates and new features in LiteLLM version 1.61.20, focusing on support for new models like Claude 3.7 Sonnet and GPT-4.5, reasoning content translation, and management UI enhancements.",
      "tags": [
        "litellm",
        "release-notes",
        "model-integration",
        "reasoning-content",
        "spend-tracking",
        "llm-proxy",
        "user-management"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.61.20-stable.md"
    },
    {
      "file_path": "714-release-notes-v1.63.11-stable.md",
      "title": "v1.63.11-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.63.11-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:23.738714413-03:00",
      "description": "These are the changes since v1.63.2-stable.",
      "summary": "This document details the release notes for LiteLLM version 1.63.11-stable, highlighting new support for Snowflake Cortex and Amazon Nova, the introduction of a Responses API, and significant UI and security enhancements. It provides a comprehensive list of bug fixes, model pricing updates, and improvements to credential management and spend tracking.",
      "tags": [
        "litellm-release",
        "responses-api",
        "snowflake-cortex",
        "amazon-nova",
        "credential-management",
        "security-updates",
        "spend-tracking"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.63.11-stable.md"
    },
    {
      "file_path": "715-release-notes-v1.63.14-stable.md",
      "title": "v1.63.14-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.63.14-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:23.959755167-03:00",
      "description": "These are the changes since v1.63.11-stable.",
      "summary": "This document outlines the updates and bug fixes for LiteLLM version 1.63.14, focusing on performance improvements, new model support, and enhanced LLM translation features.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "model-integration",
        "performance-optimization",
        "bedrock",
        "azure-openai",
        "api-updates"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.63.14-stable.md"
    },
    {
      "file_path": "716-release-notes-v1.63.2-stable.md",
      "title": "v1.63.2-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.63.2-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:27.477530213-03:00",
      "description": "These are the changes since v1.61.20-stable.",
      "summary": "This document details the release notes for LiteLLM version 1.63.2, highlighting updates to LLM translation, reasoning content handling, UI error logs, and expanded model support for various providers.",
      "tags": [
        "litellm",
        "release-notes",
        "llm-proxy",
        "bedrock",
        "anthropic",
        "openai-passthrough",
        "reasoning-content",
        "spend-tracking"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.63.2-stable.md"
    },
    {
      "file_path": "717-release-notes-v1.65.4-stable.md",
      "title": "v1.65.4-stable",
      "url": "https://docs.litellm.ai/release_notes/v1.65.4-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:31.324120868-03:00",
      "description": "Deploy this version",
      "summary": "This document outlines the v1.65.4-stable release updates for LiteLLM, focusing on database deadlock fixes, enhanced usage tracking features, and expanded model compatibility.",
      "tags": [
        "release-notes",
        "litellm",
        "changelog",
        "database-optimization",
        "spend-tracking",
        "llm-support"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.65.4-stable.md"
    },
    {
      "file_path": "718-release-notes-v1.67.0-stable.md",
      "title": "v1.67.0-stable - SCIM Integration",
      "url": "https://docs.litellm.ai/release_notes/v1.67.0-stable",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:43:34.208934399-03:00",
      "description": "Key Highlights",
      "summary": "This document outlines major updates to LiteLLM, including SCIM integration for identity providers, enhanced team and tag-based usage tracking, and a unified API for multi-model responses.",
      "tags": [
        "litellm",
        "scim-integration",
        "usage-tracking",
        "unified-responses-api",
        "identity-management",
        "spend-monitoring"
      ],
      "category": "other",
      "original_file_path": "release-notes-v1.67.0-stable.md"
    },
    {
      "file_path": "719-contact.md",
      "title": "Contact Us | liteLLM",
      "url": "https://docs.litellm.ai/contact",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:41:08.165252962-03:00",
      "description": "* Meet with us ðŸ‘‹",
      "summary": "This document provides contact information and support channels for LiteLLM, including links to scheduling meetings, community platforms, and direct email addresses.",
      "tags": [
        "litellm",
        "support",
        "contact-details",
        "community-links",
        "berriai"
      ],
      "category": "other",
      "original_file_path": "contact.md"
    },
    {
      "file_path": "720-docs-contact.md",
      "title": "Contact Us | liteLLM",
      "url": "https://docs.litellm.ai/docs/contact",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:44:53.698552489-03:00",
      "description": "* Community Slack ðŸ’­",
      "summary": "This document provides contact information and community support resources for LiteLLM, including links to Discord, Slack, and direct meeting scheduling.",
      "tags": [
        "community-support",
        "contact-information",
        "customer-service",
        "litellm-onboarding"
      ],
      "category": "other",
      "original_file_path": "docs-contact.md"
    },
    {
      "file_path": "721-docs-debugging-hosted-debugging.md",
      "title": "hosted_debugging | liteLLM",
      "url": "https://docs.litellm.ai/docs/debugging/hosted_debugging",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:45:02.873051587-03:00",
      "summary": "This document fragment represents a common accessibility skip-to-content link used in Docusaurus-based websites to facilitate navigation.",
      "tags": [
        "accessibility",
        "docusaurus",
        "navigation",
        "skip-link",
        "web-design"
      ],
      "category": "other",
      "original_file_path": "docs-debugging-hosted-debugging.md"
    },
    {
      "file_path": "722-docs-projects-Codium-PR-Agent.md",
      "title": "Codium PR Agent | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Codium%20PR%20Agent",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:06.165039765-03:00",
      "description": "An AI-Powered ðŸ¤– Tool for Automated Pull Request Analysis,",
      "summary": "This document introduces an AI-powered tool designed to automate pull request analysis, providing developers with automated feedback and suggestions.",
      "tags": [
        "ai-powered",
        "pull-request-analysis",
        "code-review-automation",
        "github-integration",
        "developer-tools"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Codium-PR-Agent.md"
    },
    {
      "file_path": "723-docs-projects-dbally.md",
      "title": "dbally | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/dbally",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:06.837560802-03:00",
      "description": "Efficient, consistent and secure library for querying structured data with natural language. Query any database with over 100 LLMs â¤ï¸ ðŸš….",
      "summary": "db-ally is a library that enables natural language querying of structured databases by integrating with over 100 large language models. It provides a secure and efficient framework for consistent data retrieval and interaction.",
      "tags": [
        "natural-language-querying",
        "structured-data",
        "llm-integration",
        "database-access",
        "data-retrieval"
      ],
      "category": "other",
      "original_file_path": "docs-projects-dbally.md"
    },
    {
      "file_path": "724-docs-projects-Docq.AI.md",
      "title": "Docq.AI | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Docq.AI",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:10.316556002-03:00",
      "description": "A private and secure ChatGPT alternative that knows your business.",
      "summary": "This document introduces an open-source, secure GenAI platform that allows businesses to interact with their confidential documents using private LLMs.",
      "tags": [
        "open-source",
        "genai",
        "document-querying",
        "private-ai",
        "self-hosted",
        "llm-integration"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Docq.AI.md"
    },
    {
      "file_path": "725-docs-projects-Elroy.md",
      "title": "ðŸ• Elroy | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Elroy",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:10.6134725-03:00",
      "description": "Elroy is a scriptable AI assistant that remembers and sets goals.",
      "summary": "Elroy is a scriptable AI assistant that features persistent memory and goal-tracking capabilities, allowing users to interact via the command line or extend functionality through Python.",
      "tags": [
        "ai-assistant",
        "scriptable-ai",
        "command-line",
        "memory-management",
        "python-sdk",
        "mcp-integration"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Elroy.md"
    },
    {
      "file_path": "726-docs-projects-FastREPL.md",
      "title": "FastREPL | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/FastREPL",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:13.382120642-03:00",
      "description": "âš¡Fast Run-Eval-Polish Loop for LLM Applications",
      "summary": "FastRepl provides a streamlined workflow for the continuous evaluation and improvement of applications built with large language models.",
      "tags": [
        "llm-ops",
        "evaluation",
        "fastrepl",
        "ai-development",
        "workflow"
      ],
      "category": "other",
      "original_file_path": "docs-projects-FastREPL.md"
    },
    {
      "file_path": "727-docs-projects-GPTLocalhost.md",
      "title": "GPTLocalhost | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/GPTLocalhost",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:16.884848392-03:00",
      "description": "GPTLocalhost - LiteLLM is supported by GPTLocalhost, a local Word Add-in for you to use models in LiteLLM within Microsoft Word. 100% Private.",
      "summary": "This document introduces GPTLocalhost, a private Microsoft Word add-in that enables the integration and local use of LiteLLM models within the word processor.",
      "tags": [
        "gptlocalhost",
        "litellm",
        "microsoft-word",
        "word-addin",
        "privacy",
        "local-ai"
      ],
      "category": "other",
      "original_file_path": "docs-projects-GPTLocalhost.md"
    },
    {
      "file_path": "728-docs-projects-Langstream.md",
      "title": "Langstream | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Langstream",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:20.367104551-03:00",
      "description": "Build robust LLM applications with true composability ðŸ”—",
      "summary": "Langstream is a framework designed for building robust and composable applications using Large Language Models.",
      "tags": [
        "llm",
        "composability",
        "orchestration",
        "ai-development",
        "langstream"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Langstream.md"
    },
    {
      "file_path": "729-docs-projects-llm-cord.md",
      "title": "llmcord.py | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/llm_cord",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:23.382375789-03:00",
      "description": "llmcord.py lets you and your friends chat with LLMs directly in your Discord server. It works with practically any LLM, remote or locally hosted.",
      "summary": "llmcord.py is a tool that enables users to interact with various local or remote large language models directly through a Discord interface.",
      "tags": [
        "discord-bot",
        "llm-integration",
        "ai-chatbot",
        "python",
        "self-hosting"
      ],
      "category": "other",
      "original_file_path": "docs-projects-llm-cord.md"
    },
    {
      "file_path": "730-docs-projects-Prompt2Model.md",
      "title": "Prompt2Model | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Prompt2Model",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:32.327787809-03:00",
      "description": "Prompt2Model - Generate Deployable Models from Instructions",
      "summary": "Prompt2Model is a system designed to transform natural language task descriptions into small, specialized, and deployable machine learning models.",
      "tags": [
        "machine-learning",
        "model-training",
        "natural-language-processing",
        "llm",
        "model-deployment"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Prompt2Model.md"
    },
    {
      "file_path": "731-docs-projects-Quivr.md",
      "title": "Quivr | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Quivr",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:35.736420634-03:00",
      "description": "ðŸ§  Your Second Brain supercharged by Generative AI ðŸ§  Dump all your files and chat with your personal assistant on your files & more using GPT 3.5/4, Private, Anthropic, VertexAI, LLMs...",
      "summary": "This document introduces an AI-powered personal knowledge management platform that allows users to store files and interact with them using various large language models.",
      "tags": [
        "generative-ai",
        "second-brain",
        "knowledge-management",
        "llm",
        "document-chat"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Quivr.md"
    },
    {
      "file_path": "732-docs-projects-Softgen.md",
      "title": "Softgen | liteLLM",
      "url": "https://docs.litellm.ai/docs/projects/Softgen",
      "source": "sitemap",
      "fetched_at": "2026-01-21T19:47:41.046676132-03:00",
      "description": "Softgen is an AI-powered platform that builds full-stack web apps from your plain instructions.",
      "summary": "This document introduces Softgen, an AI platform that leverages LiteLLM to enable users to create full-stack web applications using various large language models.",
      "tags": [
        "softgen",
        "litellm",
        "ai-app-builder",
        "full-stack-development",
        "web-apps"
      ],
      "category": "other",
      "original_file_path": "docs-projects-Softgen.md"
    }
  ],
  "organization": {
    "method": "sequential-numbering",
    "organized_at": "2026-01-22T15:44:12.323Z",
    "total_files": 732,
    "categories": [
      "Introduction & Overview",
      "Quick Start & Installation",
      "Tutorials & How-To",
      "Concepts & Fundamentals",
      "Configuration & Settings",
      "Integration & Connection",
      "Authentication & Security",
      "API & Reference",
      "Operations & Deployment",
      "Automation & Workflow",
      "Advanced Topics",
      "Changelog & Releases",
      "Meta & Resources"
    ]
  }
}