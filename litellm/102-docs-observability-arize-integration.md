---
title: Arize AI | liteLLM
url: https://docs.litellm.ai/docs/observability/arize_integration
source: sitemap
fetched_at: 2026-01-21T19:45:52.40740197-03:00
rendered_js: false
word_count: 101
summary: This document provides instructions for integrating Arize AI with LiteLLM to enable LLM observability, tracing, and logging through SDK callbacks and proxy configurations.
tags:
    - arize-ai
    - litellm
    - llm-observability
    - tracing
    - monitoring
    - callback-functions
    - proxy-configuration
category: guide
---

AI Observability and Evaluation Platform

## Pre-Requisites[‚Äã](#pre-requisites "Direct link to Pre-Requisites")

Make an account on [Arize AI](https://app.arize.com/auth/login)

## Quick Start[‚Äã](#quick-start "Direct link to Quick Start")

Use just 2 lines of code, to instantly log your responses **across all providers** with arize

You can also use the instrumentor option instead of the callback, which you can find [here](https://docs.arize.com/arize/llm-tracing/tracing-integrations-auto/litellm).

```
litellm.callbacks =["arize"]
```

```

import litellm
import os

os.environ["ARIZE_SPACE_KEY"]=""
os.environ["ARIZE_API_KEY"]=""

# LLM API Keys
os.environ['OPENAI_API_KEY']=""

# set arize as a callback, litellm will send the data to arize
litellm.callbacks =["arize"]

# openai call
response = litellm.completion(
  model="gpt-3.5-turbo",
  messages=[
{"role":"user","content":"Hi üëã - i'm openai"}
]
)
```

## Using with LiteLLM Proxy[‚Äã](#using-with-litellm-proxy "Direct link to Using with LiteLLM Proxy")

1. Setup config.yaml

```
model_list:
-model_name: gpt-4
litellm_params:
model: openai/fake
api_key: fake-key
api_base: https://exampleopenaiendpoint-production.up.railway.app/

litellm_settings:
callbacks:["arize"]

general_settings:
master_key:"sk-1234"# can also be set as an environment variable

environment_variables:
ARIZE_SPACE_ID:"d0*****"
ARIZE_API_KEY:"141a****"
ARIZE_ENDPOINT:"https://otlp.arize.com/v1"# OPTIONAL - your custom arize GRPC api endpoint
ARIZE_HTTP_ENDPOINT:"https://otlp.arize.com/v1"# OPTIONAL - your custom arize HTTP api endpoint. Set either this or ARIZE_ENDPOINT or Neither (defaults to https://otlp.arize.com/v1 on grpc)
ARIZE_PROJECT_NAME:"my-litellm-project"# OPTIONAL - sets the arize project name
```

2. Start the proxy

```
litellm --config config.yaml
```

3. Test it!

```
curl -X POST 'http://0.0.0.0:4000/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{ "model": "gpt-4", "messages": [{"role": "user", "content": "Hi üëã - i'm openai"}]}'
```

## Pass Arize Space/Key per-request[‚Äã](#pass-arize-spacekey-per-request "Direct link to Pass Arize Space/Key per-request")

Supported parameters:

- `arize_api_key`
- `arize_space_key` *(deprecated, use `arize_space_id` instead)*
- `arize_space_id`

<!--THE END-->

- SDK
- PROXY

```
import litellm
import os

# LLM API Keys
os.environ['OPENAI_API_KEY']=""

# set arize as a callback, litellm will send the data to arize
litellm.callbacks =["arize"]

# openai call
response = litellm.completion(
  model="gpt-3.5-turbo",
  messages=[
{"role":"user","content":"Hi üëã - i'm openai"}
],
  arize_api_key=os.getenv("ARIZE_API_KEY"),
  arize_space_id=os.getenv("ARIZE_SPACE_ID"),
)
```

## Support & Talk to Founders[‚Äã](#support--talk-to-founders "Direct link to Support & Talk to Founders")

- [Schedule Demo üëã](https://calendly.com/d/4mp-gd3-k5k/berriai-1-1-onboarding-litellm-hosted-version)
- [Community Discord üí≠](https://discord.gg/wuPM9dRgDw)
- Our numbers üìû +1 (770) 8783-106 / +1 (412) 618-6238
- Our emails ‚úâÔ∏è [ishaan@berri.ai](mailto:ishaan@berri.ai) / [krrish@berri.ai](mailto:krrish@berri.ai)