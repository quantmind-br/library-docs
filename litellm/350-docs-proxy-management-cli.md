---
title: LiteLLM Proxy CLI | liteLLM
url: https://docs.litellm.ai/docs/proxy/management_cli
source: sitemap
fetched_at: 2026-01-21T19:52:54.939560265-03:00
rendered_js: false
word_count: 458
summary: This document explains how to use the litellm-proxy CLI tool to manage models, credentials, and users on a LiteLLM proxy server. It provides instructions for installation, authentication via SSO, and executing common management commands.
tags:
    - litellm-proxy
    - cli
    - model-management
    - api-keys
    - user-management
    - authentication
    - llm-proxy
category: guide
---

The `litellm-proxy` CLI is a command-line tool for managing your LiteLLM proxy server. It provides commands for managing models, credentials, API keys, users, and more, as well as making chat and HTTP requests to the proxy server.

FeatureWhat you can doModels ManagementList, add, update, and delete modelsCredentials ManagementManage provider credentialsKeys ManagementGenerate, list, and delete API keysUser ManagementCreate, list, and delete usersChat CompletionsRun chat completionsHTTP RequestsMake custom HTTP requests to the proxy server

## Quick Start[​](#quick-start "Direct link to Quick Start")

1. **Install the CLI**
   
   If you have [uv](https://github.com/astral-sh/uv) installed, you can try this:
   
   ```
   uv tool install 'litellm[proxy]'
   ```
   
   If that works, you'll see something like this:
   
   ```
   ...
   Installed 2 executables: litellm, litellm-proxy
   ```
   
   and now you can use the tool by just typing `litellm-proxy` in your terminal:
2. **Set up environment variables**
   
   ```
   export LITELLM_PROXY_URL=http://localhost:4000
   export LITELLM_PROXY_API_KEY=sk-your-key
   ```
   
   *(Replace with your actual proxy URL and API key)*
3. **Make your first request (list models)**
   
   ```
   litellm-proxy models list
   ```
   
   If the CLI is set up correctly, you should see a list of available models or a table output.
4. **Troubleshooting**
   
   - If you see an error, check your environment variables and proxy server status.

## Authentication using CLI[​](#authentication-using-cli "Direct link to Authentication using CLI")

You can use the CLI to authenticate to the LiteLLM Gateway. This is great if you're trying to give a large number of developers self-serve access to the LiteLLM Gateway.

### Prerequisites[​](#prerequisites "Direct link to Prerequisites")

Beta Feature - Required Environment Variable

CLI SSO Authentication is currently in beta. You must set this environment variable **when starting up your LiteLLM Proxy**:

```
export EXPERIMENTAL_UI_LOGIN="True"
litellm --config config.yaml
```

Or add it to your proxy startup command:

```
EXPERIMENTAL_UI_LOGIN="True" litellm --config config.yaml
```

### Steps[​](#steps "Direct link to Steps")

1. **Set up the proxy URL**
   
   ```
   export LITELLM_PROXY_URL=http://localhost:4000
   ```
   
   *(Replace with your actual proxy URL)*
2. **Login**
   
   This will open a browser window to authenticate. If you have connected LiteLLM Proxy to your SSO provider, you can login with your SSO credentials. Once logged in, you can use the CLI to make requests to the LiteLLM Gateway.
3. **Test your authentication**
   
   ```
   litellm-proxy models list
   ```
   
   This will list all the models available to you.

## Main Commands[​](#main-commands "Direct link to Main Commands")

### Models Management[​](#models-management "Direct link to Models Management")

- List, add, update, get, and delete models on the proxy.
- Example:
  
  ```
  litellm-proxy models list
  litellm-proxy models add gpt-4 \
    --param api_key=sk-123 \
    --param max_tokens=2048
  litellm-proxy models update <model-id> -p temperature=0.7
  litellm-proxy models delete <model-id>
  ```
  
  [API used (OpenAPI)](https://litellm-api.up.railway.app/#/model%20management)

### Credentials Management[​](#credentials-management "Direct link to Credentials Management")

- List, create, get, and delete credentials for LLM providers.
- Example:
  
  ```
  litellm-proxy credentials list
  litellm-proxy credentials create azure-prod \
    --info='{"custom_llm_provider": "azure"}' \
    --values='{"api_key": "sk-123", "api_base": "https://prod.azure.openai.com"}'
  litellm-proxy credentials get azure-cred
  litellm-proxy credentials delete azure-cred
  ```
  
  [API used (OpenAPI)](https://litellm-api.up.railway.app/#/credential%20management)

### Keys Management[​](#keys-management "Direct link to Keys Management")

- List, generate, get info, and delete API keys.
- Example:
  
  ```
  litellm-proxy keys list
  litellm-proxy keys generate \
    --models=gpt-4 \
    --spend=100 \
    --duration=24h \
    --key-alias=my-key
  litellm-proxy keys info --key sk-key1
  litellm-proxy keys delete --keys sk-key1,sk-key2 --key-aliases alias1,alias2
  ```
  
  [API used (OpenAPI)](https://litellm-api.up.railway.app/#/key%20management)

### User Management[​](#user-management "Direct link to User Management")

- List, create, get info, and delete users.
- Example:
  
  ```
  litellm-proxy users list
  litellm-proxy users create \
    --email=user@example.com \
    --role=internal_user \
    --alias="Alice" \
    --team=team1 \
    --max-budget=100.0
  litellm-proxy users get --id <user-id>
  litellm-proxy users delete <user-id>
  ```
  
  [API used (OpenAPI)](https://litellm-api.up.railway.app/#/Internal%20User%20management)

### Chat Completions[​](#chat-completions "Direct link to Chat Completions")

- Ask for chat completions from the proxy server.
- Example:
  
  ```
  litellm-proxy chat completions gpt-4 -m "user:Hello, how are you?"
  ```
  
  [API used (OpenAPI)](https://litellm-api.up.railway.app/#/chat%2Fcompletions)

### General HTTP Requests[​](#general-http-requests "Direct link to General HTTP Requests")

- Make direct HTTP requests to the proxy server.
- Example:
  
  ```
  litellm-proxy http request \
    POST /chat/completions \
    --json '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
  ```
  
  [All APIs (OpenAPI)](https://litellm-api.up.railway.app/#/)

## Environment Variables[​](#environment-variables "Direct link to Environment Variables")

- `LITELLM_PROXY_URL`: Base URL of the proxy server
- `LITELLM_PROXY_API_KEY`: API key for authentication

## Examples[​](#examples "Direct link to Examples")

1. **List all models:**
   
   ```
   litellm-proxy models list
   ```
2. **Add a new model:**
   
   ```
   litellm-proxy models add gpt-4 \
     --param api_key=sk-123 \
     --param max_tokens=2048
   ```
3. **Create a credential:**
   
   ```
   litellm-proxy credentials create azure-prod \
     --info='{"custom_llm_provider": "azure"}' \
     --values='{"api_key": "sk-123", "api_base": "https://prod.azure.openai.com"}'
   ```
4. **Generate an API key:**
   
   ```
   litellm-proxy keys generate \
     --models=gpt-4 \
     --spend=100 \
     --duration=24h \
     --key-alias=my-key
   ```
5. **Chat completion:**
   
   ```
   litellm-proxy chat completions gpt-4 \
     -m "user:Write a story"
   ```
6. **Custom HTTP request:**
   
   ```
   litellm-proxy http request \
     POST /chat/completions \
     --json '{"model": "gpt-4", "messages": [{"role": "user", "content": "Hello"}]}'
   ```

## Error Handling[​](#error-handling "Direct link to Error Handling")

The CLI will display error messages for:

- Server not accessible
- Authentication failures
- Invalid parameters or JSON
- Nonexistent models/credentials
- Any other operation failures

Use the `--debug` flag for detailed debugging output.

For full command reference and advanced usage, see the [CLI README](https://github.com/BerriAI/litellm/blob/main/litellm/proxy/client/cli/README.md).