---
title: AI21 | liteLLM
url: https://docs.litellm.ai/docs/providers/ai21
source: sitemap
fetched_at: 2026-01-21T19:47:46.821224321-03:00
rendered_js: false
word_count: 162
summary: This document provides instructions and configuration details for using AI21 models through LiteLLM's Python SDK and Proxy Server. It covers supported model identifiers, API key setup, and how to pass standard or provider-specific parameters.
tags:
    - litellm
    - ai21
    - python-sdk
    - proxy-server
    - llm-integration
    - model-parameters
category: guide
---

LiteLLM supports the following [AI21](https://www.ai21.com/studio/pricing) models:

- `jamba-1.5-mini`
- `jamba-1.5-large`
- `j2-light`
- `j2-mid`
- `j2-ultra`

tip

**We support ALL AI21 models, just set `model=ai21/<any-model-on-ai21>` as a prefix when sending litellm requests**. **See all litellm supported AI21 models [here](https://models.litellm.ai)**

### API KEYS[​](#api-keys "Direct link to API KEYS")

```
import os 
os.environ["AI21_API_KEY"]="your-api-key"
```

## **LiteLLM Python SDK Usage**[​](#litellm-python-sdk-usage "Direct link to litellm-python-sdk-usage")

### Sample Usage[​](#sample-usage "Direct link to Sample Usage")

```
from litellm import completion 

# set env variable 
os.environ["AI21_API_KEY"]="your-api-key"

messages =[{"role":"user","content":"Write me a poem about the blue sky"}]

completion(model="ai21/jamba-1.5-mini", messages=messages)
```

## **LiteLLM Proxy Server Usage**[​](#litellm-proxy-server-usage "Direct link to litellm-proxy-server-usage")

Here's how to call a ai21 model with the LiteLLM Proxy Server

1. Modify the config.yaml

```
model_list:
-model_name: my-model
litellm_params:
model: ai21/<your-model-name># add ai21/ prefix to route as ai21 provider
api_key: api-key                 # api key to send your model
```

2. Start the proxy

```
$ litellm --config /path/to/config.yaml
```

3. Send Request to LiteLLM Proxy Server

<!--THE END-->

- OpenAI Python v1.0.0+
- curl

```
import openai
client = openai.OpenAI(
    api_key="sk-1234",# pass litellm proxy key, if you're using virtual keys
    base_url="http://0.0.0.0:4000"# litellm-proxy-base url
)

response = client.chat.completions.create(
    model="my-model",
    messages =[
{
"role":"user",
"content":"what llm are you"
}
],
)

print(response)
```

## Supported OpenAI Parameters[​](#supported-openai-parameters "Direct link to Supported OpenAI Parameters")

[param](https://docs.litellm.ai/docs/completion/input)typeAI21 equivalent`tools`**Optional\[list]**`tools``response_format`**Optional\[dict]**`response_format``max_tokens`**Optional\[int]**`max_tokens``temperature`**Optional\[float]**`temperature``top_p`**Optional\[float]**`top_p``stop`**Optional\[Union\[str, list]]**`stop``n`**Optional\[int]**`n``stream`**Optional\[bool]**`stream``seed`**Optional\[int]**`seed``tool_choice`**Optional\[str]**`tool_choice``user`**Optional\[str]**`user`

## Supported AI21 Parameters[​](#supported-ai21-parameters "Direct link to Supported AI21 Parameters")

paramtype[AI21 equivalent](https://docs.ai21.com/reference/jamba-15-api-ref#request-parameters)`documents`**Optional\[List\[Dict]]**`documents`

## Passing AI21 Specific Parameters - `documents`[​](#passing-ai21-specific-parameters----documents "Direct link to passing-ai21-specific-parameters----documents")

LiteLLM allows you to pass all AI21 specific parameters to the `litellm.completion` function. Here is an example of how to pass the `documents` parameter to the `litellm.completion` function.

- LiteLLM Python SDK
- LiteLLM Proxy Server

```
response =await litellm.acompletion(
    model="jamba-1.5-large",
    messages=[{"role":"user","content":"what does the document say"}],
    documents =[
{
"content":"hello world",
"metadata":{
"source":"google",
"author":"ishaan"
}
}
]
)

```

tip

**We support ALL AI21 models, just set `model=ai21/<any-model-on-ai21>` as a prefix when sending litellm requests** **See all litellm supported AI21 models [here](https://models.litellm.ai)**

## AI21 Models[​](#ai21-models "Direct link to AI21 Models")

Model NameFunction CallRequired OS Variablesjamba-1.5-mini`completion('jamba-1.5-mini', messages)``os.environ['AI21_API_KEY']`jamba-1.5-large`completion('jamba-1.5-large', messages)``os.environ['AI21_API_KEY']`j2-light`completion('j2-light', messages)``os.environ['AI21_API_KEY']`j2-mid`completion('j2-mid', messages)``os.environ['AI21_API_KEY']`j2-ultra`completion('j2-ultra', messages)``os.environ['AI21_API_KEY']`