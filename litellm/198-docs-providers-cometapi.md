---
title: CometAPI | liteLLM
url: https://docs.litellm.ai/docs/providers/cometapi
source: sitemap
fetched_at: 2026-01-21T19:48:45.738930107-03:00
rendered_js: false
word_count: 160
summary: This document explains how to integrate LiteLLM with CometAPI to access over 500 AI models through a unified interface. It provides comprehensive examples for authentication, synchronous and asynchronous completions, streaming, and error handling.
tags:
    - litellm
    - cometapi
    - llm-integration
    - python-sdk
    - streaming-api
    - async-completion
category: guide
---

LiteLLM supports all AI models from [CometAPI](https://www.cometapi.com/). CometAPI provides access to 500+ AI models through a unified API interface, including cutting-edge models like GPT-5, Claude Opus 4.1, and various other state-of-the-art language models.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/LiteLLM_CometAPI.ipynb)

## Authentication[​](#authentication "Direct link to Authentication")

To use CometAPI models, you need to obtain an API key from [CometAPI Token Console](https://api.cometapi.com/console/token). CometAPI offers free tokens for new users - you can get your free API key instantly by registering.

## Usage[​](#usage "Direct link to Usage")

Set your CometAPI key as an environment variable and use the completion function:

```
import os
from litellm import completion

# Set API key
os.environ["COMETAPI_KEY"]="your_comet_api_key_here"

# Define messages
messages =[{"content":"Hello, how are you?","role":"user"}]

# Method 1: Using environment variable (recommended)
response = completion(
    model="cometapi/gpt-5",
    messages=messages
)

print(response.choices[0].message.content)
```

### Alternative Usage - Explicit API Key[​](#alternative-usage---explicit-api-key "Direct link to Alternative Usage - Explicit API Key")

You can also pass the API key explicitly:

```
import os
from litellm import completion

# Define messages
messages =[{"content":"Hello, how are you?","role":"user"}]

# Method 2: Explicitly passing API key
response = completion(
    model="cometapi/gpt-4o",
    messages=messages,
    api_key="your_comet_api_key_here"
)

print(response.choices[0].message.content)
```

## Usage - Streaming[​](#usage---streaming "Direct link to Usage - Streaming")

Just set `stream=True` when calling completion:

```
import os
from litellm import completion

os.environ["COMETAPI_KEY"]="your_comet_api_key_here"

messages =[{"content":"Hello, how are you?","role":"user"}]

response = completion(
    model="cometapi/gpt-5",
    messages=messages,
    stream=True
)

for chunk in response:
print(chunk.choices[0].delta.content or"", end="")
```

## Usage - Async Streaming[​](#usage---async-streaming "Direct link to Usage - Async Streaming")

For async streaming, use `acompletion`:

```
from litellm import acompletion
import asyncio, os, traceback

asyncdefcompletion_call():
try:
        os.environ["COMETAPI_KEY"]="your_comet_api_key_here"

print("test acompletion + streaming")
        response =await acompletion(
            model="cometapi/chatgpt-4o-latest",
            messages=[{"content":"Hello, how are you?","role":"user"}],
            stream=True
)
print(f"response: {response}")
asyncfor chunk in response:
print(chunk)
except:
print(f"error occurred: {traceback.format_exc()}")
pass

# Run the async function
await completion_call()
```

CometAPI offers access to 500+ AI models through a unified API. Some popular models include:

Model NameFunction Callcometapi/gpt-5`completion('cometapi/gpt-5', messages)`cometapi/gpt-5-mini`completion('cometapi/gpt-5-mini', messages)`cometapi/gpt-5-nano`completion('cometapi/gpt-5-nano', messages)`cometapi/gpt-oss-20b`completion('cometapi/gpt-oss-20b', messages)`cometapi/gpt-oss-120b`completion('cometapi/gpt-oss-120b', messages)`cometapi/chatgpt-4o-latest`completion('cometapi/chatgpt-4o-latest', messages)`

For a complete list of available models, visit the [CometAPI Models page](https://www.cometapi.com/model/).

## Environment Variables[​](#environment-variables "Direct link to Environment Variables")

VariableDescriptionRequired`COMETAPI_KEY`Your CometAPI API keyYes

## Error Handling[​](#error-handling "Direct link to Error Handling")

```
import os
from litellm import completion

try:
    os.environ["COMETAPI_KEY"]="your_comet_api_key_here"

    messages =[{"content":"Hello, how are you?","role":"user"}]

    response = completion(
        model="cometapi/gpt-5",
        messages=messages
)

print(response.choices[0].message.content)

except Exception as e:
print(f"Error: {e}")
```