---
description: Auto-generated documentation index
generated: 2026-01-22T15:44:12.325Z
source: https://docs.litellm.ai/sitemap.xml
total_docs: 732
categories: 13
---

# Project Documentation Index

> Organized index for AI agent consumption. Documents follow logical learning sequence.

## Metadata Summary

| Property | Value |
|----------|-------|
| **Source** | https://docs.litellm.ai/sitemap.xml |
| **Generated** | 2026-01-22T15:44:12.325Z |
| **Total Documents** | 732 |
| **Categories** | Introduction & Overview, Quick Start & Installation, Tutorials & How-To, Concepts & Fundamentals, Configuration & Settings, Integration & Connection, Authentication & Security, API & Reference, Operations & Deployment, Automation & Workflow, Advanced Topics, Changelog & Releases, Meta & Resources |

---

## Document Index

### 1. Introduction & Overview (001-019)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 001 | `001-docs-a2a.md` | Agent Gateway (A2A Protocol) - Overview - liteLLM | This document explains how to integrate, manage, and invoke A2A-compatible agents from various provi... | litellm, a2a-protocol, ai-gateway |
| 002 | `002-docs-mcp.md` | MCP Overview - liteLLM | This document explains how to configure and use LiteLLM Proxy as an MCP Gateway to manage tools and ... | litellm-proxy, mcp-gateway, model-context-protocol |
| 003 | `003-docs-pass-through-intro.md` | Why Pass-Through Endpoints? - liteLLM | This document explains how LiteLLM's passthrough endpoints function to forward provider-specific req... | litellm-proxy, passthrough-endpoints, api-forwarding |
| 004 | `004-docs-proxy-configs.md` | Overview - liteLLM | This document provides instructions for setting up and managing the LiteLLM Proxy using a YAML confi... | litellm, config-yaml, proxy-server |
| 005 | `005-docs-proxy-user-keys.md` | Langchain, OpenAI SDK, LlamaIndex, Instructor, Curl examples - liteLLM | This document demonstrates how to use the OpenAI Python client to send a single chat completion requ... | openai-sdk, chat-completions, multi-model |
| 006 | `006-docs-search.md` | Overview - liteLLM | This document explains how to use LiteLLM to perform search operations using its Python SDK and AI G... | litellm, search-api, python-sdk |
| 007 | `007-docs-secret-managers-overview.md` | Secret Managers Overview - liteLLM | This document explains how to configure LiteLLM to integrate with external key management systems fo... | secret-management, litellm-configuration, kms-integration |
| 008 | `008-docs-secret.md` | Secret Managers Overview - liteLLM | This document explains how to configure LiteLLM to read and write API keys and virtual keys using va... | secret-management, litellm, aws-secret-manager |
| 009 | `009-index.md` | LiteLLM - Getting Started - liteLLM | LiteLLM provides a unified interface and proxy server to call over 100 different LLMs using the Open... | litellm, llm-gateway, openai-compatibility |
| 010 | `010-release-notes-v1-80-0.md` | v1.80.0-stable - Introducing Agent Hub: Register, Publish, and Share Agents | This document details the v1.80.0 release of LiteLLM, highlighting new features like Agent Hub, Runw... | litellm, release-notes, vector-store-api |
| 011 | `011-release-notes-v1-80-8.md` | v1.80.8-stable - Introducing A2A Agent Gateway | This document outlines the features and updates in LiteLLM version 1.80.8-stable, including the new ... | litellm-release, agent-gateway, llm-proxy |
| 012 | `012-docs-tutorials-claude-responses-api.md` | Claude Code Quickstart - liteLLM | This guide explains how to integrate Claude Code with LiteLLM proxy to access and manage Claude mode... | litellm, claude-code, proxy-server |
| 013 | `013-docs-default-code-snippet.md` | Get Started - liteLLM | This document demonstrates how to use the litellm library to make standardized completion calls acro... | litellm, python-sdk, llm-integration |
| 014 | `014-docs-proxy-docker-quick-start.md` | Getting Started Tutorial - liteLLM | This tutorial provides a step-by-step guide to setting up and configuring the LiteLLM Proxy for Azur... | litellm-proxy, azure-openai, docker-deployment |
| 015 | `015-docs-proxy-guardrails-quick-start.md` | Guardrails - Quick Start - liteLLM | This document provides instructions for configuring and implementing AI guardrails, such as PII mask... | litellm, ai-gateway, guardrails |
| 016 | `016-docs-proxy-quick-start.md` | CLI - Quick Start - liteLLM | This document provides a comprehensive guide for setting up and using the LiteLLM Proxy to manage mu... | litellm-proxy, llm-gateway, cli-setup |
| 017 | `017-docs-proxy-ui-logs.md` | Getting Started with UI Logs - liteLLM | This document explains how to configure and manage logging in LiteLLM, including tracking spend and ... | litellm, logging, spend-tracking |
| 018 | `018-docs-proxy-ui.md` | Quick Start - liteLLM | This document explains how to set up, access, and configure the LiteLLM Admin UI for managing models... | litellm, admin-ui, proxy-server |
| 019 | `019-docs.md` | LiteLLM - Getting Started - liteLLM | LiteLLM provides a unified interface to call over 100 LLMs using the OpenAI input/output format via ... | llm-integration, openai-format, python-sdk |

### 2. Quick Start & Installation (020-021)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 020 | `020-docs-proxy-db-deadlocks.md` | High Availability Setup (Resolve DB Deadlocks) - liteLLM | This document explains how to configure a Redis transaction buffer in LiteLLM to prevent PostgreSQL ... | litellm, redis, postgresql |
| 021 | `021-docs-tutorials-installation.md` | Set up environment - liteLLM | This document provides instructions and direct links for obtaining API keys from various LLM provide... | api-keys, llm-providers, environment-setup |

### 3. Tutorials & How-To (022-468)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 022 | `022-blog-anthropic-advanced-features.md` | Day 0 Support: Claude 4.5 Opus (+Advanced Features) | This document explains how to implement advanced Anthropic Claude 4.5 features, such as Tool Search ... | litellm, anthropic-claude, tool-calling |
| 023 | `023-blog-gemini-3-flash.md` | DAY 0 Support: Gemini 3 Flash on LiteLLM | This document details the integration of Google Gemini 3 Flash Preview into LiteLLM, explaining how ... | litellm, gemini-3-flash, google-gemini |
| 024 | `024-blog-gemini-3.md` | DAY 0 Support: Gemini 3 on LiteLLM | This document explains how to use Gemini 3 Pro Preview with LiteLLM, focusing on thought signatures,... | litellm, gemini-3-pro, thought-signatures |
| 025 | `025-blog-tags-advanced-features.md` | One post tagged with "advanced features" - liteLLM | This document demonstrates how to implement Anthropic's tool search and deferred tool loading functi... | litellm, anthropic-claude, tool-calling |
| 026 | `026-blog-tags-anthropic.md` | One post tagged with "anthropic" - liteLLM | This document demonstrates how to implement deferred tool loading and tool searching using LiteLLM w... | litellm, anthropic-claude, function-calling |
| 027 | `027-blog-tags-claude.md` | One post tagged with "claude" - liteLLM | This document demonstrates how to implement deferred tool loading and tool search functionality usin... | litellm, anthropic-claude, tool-calling |
| 028 | `028-blog-tags-day-0-support.md` | 2 posts tagged with "day 0 support" - liteLLM | This document provides a guide for using Gemini 3 Pro Preview with LiteLLM, focusing on endpoint sup... | litellm, google-gemini, gemini-3 |
| 029 | `029-blog-tags-effort.md` | One post tagged with "effort" - liteLLM | This document demonstrates how to implement deferred tool loading and tool search functionality usin... | litellm, anthropic-claude, tool-calling |
| 030 | `030-blog-tags-gemini.md` | 2 posts tagged with "gemini" - liteLLM | This guide provides instructions and best practices for using the Gemini 3 Pro Preview model with Li... | gemini-3, litellm, thought-signatures |
| 031 | `031-blog-tags-llms.md` | 2 posts tagged with "llms" - liteLLM | This guide explains how to integrate and use the Gemini 3 Pro Preview model with LiteLLM, focusing o... | gemini-3, litellm, thought-signatures |
| 032 | `032-blog-tags-programmatic-tool-calling.md` | One post tagged with "programmatic tool calling" - liteLLM | This document demonstrates how to implement tool search and deferred tool loading using LiteLLM with... | litellm, anthropic-claude, tool-calling |
| 033 | `033-blog-tags-tool-search.md` | One post tagged with "tool search" - liteLLM | This document demonstrates how to implement tool searching and deferred tool loading using LiteLLM w... | litellm, anthropic-claude, tool-calling |
| 034 | `034-blog.md` | Blog - liteLLM | This document provides instructions and best practices for using Gemini 3 Pro Preview with LiteLLM, ... | litellm, gemini-3, google-gemini |
| 035 | `035-contributing.md` | Contributing to Documentation - liteLLM | This document provides instructions for setting up, running, and contributing to the LiteLLM documen... | litellm, documentation, mkdocs |
| 036 | `036-docs-a2a-agent-permissions.md` | Agent Permission Management - liteLLM | This document explains how to manage access control for A2A agents in LiteLLM by restricting agent p... | litellm, agent-permissions, access-control |
| 037 | `037-docs-a2a-cost-tracking.md` | A2A Agent Cost Tracking - liteLLM | This guide explains how to configure and verify custom cost tracking for A2A agents in LiteLLM using... | litellm, cost-tracking, a2a-agents |
| 038 | `038-docs-adding-provider-adding-guardrail-support.md` | Adding Guardrail Support to Endpoints - liteLLM | This document explains how to implement guardrail translation support for LiteLLM endpoints by creat... | litellm, guardrails, api-integration |
| 039 | `039-docs-adding-provider-directory-structure.md` | Directory Structure - liteLLM | This document outlines the required directory structure and file organization for integrating a new ... | litellm, provider-integration, directory-structure |
| 040 | `040-docs-adding-provider-new-rerank-provider.md` | Add Rerank Provider - liteLLM | This document provides instructions on how to integrate new rerank providers into LiteLLM by impleme... | litellm, rerank-api, provider-integration |
| 041 | `041-docs-adding-provider-simple-guardrail-tutorial.md` | Adding a New Guardrail Integration - liteLLM | This document provides step-by-step instructions for building and registering a custom guardrail cla... | litellm, guardrails, custom-hooks |
| 042 | `042-docs-ai-tools.md` | AI Tools - liteLLM | This document explains how to monitor usage metrics and associated costs for AI-powered coding assis... | cost-tracking, usage-monitoring, litellm |
| 043 | `043-docs-audio-transcription.md` | /audio/transcriptions - liteLLM | This guide explains how to implement audio transcription using LiteLLM, covering supported providers... | litellm, audio-transcription, speech-to-text |
| 044 | `044-docs-batches.md` | /batches - liteLLM | This document explains how to manage batch completions and file uploads using LiteLLM, including sup... | batch-processing, file-management, model-routing |
| 045 | `045-docs-bedrock-converse.md` | /converse - liteLLM | This document explains how to configure and use the LiteLLM Proxy to access Amazon Bedrock's convers... | litellm, amazon-bedrock, api-proxy |
| 046 | `046-docs-bedrock-invoke.md` | /invoke - liteLLM | This document provides a guide for setting up and using the LiteLLM Proxy to interact with Amazon Be... | litellm-proxy, amazon-bedrock, load-balancing |
| 047 | `047-docs-budget-manager.md` | Budget Manager - liteLLM | This document explains how to implement budget management and cost tracking for LLM API calls using ... | litellm, budget-management, cost-tracking |
| 048 | `048-docs-caching-all-caches.md` | Caching - In-Memory, Redis, s3, gcs, Redis Semantic Cache, Disk - liteLLM | This document provides a comprehensive guide on implementing caching in LiteLLM using various backen... | litellm, caching, redis-cache |
| 049 | `049-docs-caching-caching-api.md` | Hosted Cache - api.litellm.ai - liteLLM | This document demonstrates how to initialize and use LiteLLM's hosted caching for completions, embed... | litellm, caching, hosted-cache |
| 050 | `050-docs-caching-local-caching.md` | LiteLLM - Local Caching - liteLLM | This document explains how to implement and configure caching for completion and embedding calls in ... | litellm, caching, redis |
| 051 | `051-docs-completion-audio.md` | Using Audio Models - liteLLM | This document provides a code example for using litellm to process audio inputs and generate multimo... | litellm, audio-processing, python-sdk |
| 052 | `052-docs-completion-batching.md` | Batching Completion() - liteLLM | This document explains how to use LiteLLM's batch completion features to process multiple prompts si... | litellm, batch-completion, parallel-processing |
| 053 | `053-docs-completion-computer-use.md` | Computer Use - liteLLM | This document explains how to implement computer use capabilities via LiteLLM to enable AI models to... | litellm, computer-use, tool-calling |
| 054 | `054-docs-completion-document-understanding.md` | Using PDF Input - liteLLM | This document explains how to send PDF files and other document types to LLM providers using LiteLLM... | litellm, pdf-input, multimodal-llm |
| 055 | `055-docs-completion-function-call.md` | Function Calling - liteLLM | This document explains how to check for function calling support in LiteLLM and provides a step-by-s... | litellm, function-calling, parallel-function-calling |
| 056 | `056-docs-completion-image-generation-chat.md` | Image Generation in Chat Completions, Responses API - liteLLM | Explains how to use LiteLLM to generate images through the chat completions endpoint using supported... | litellm, image-generation, gemini |
| 057 | `057-docs-completion-json-mode.md` | Structured Outputs (JSON Mode) - liteLLM | This document provides a technical guide on implementing structured outputs and JSON schemas using L... | litellm, structured-outputs, json-schema |
| 058 | `058-docs-completion-knowledgebase.md` | Using Vector Stores (Knowledge Bases) - liteLLM | This document explains how to integrate and manage various vector stores with LiteLLM to provide mod... | litellm, vector-stores, rag-integration |
| 059 | `059-docs-completion-mock-requests.md` | Mock Completion() Responses - Save Testing Costs üí∞ - liteLLM | This document explains how to use the mock_response parameter in LiteLLM to simulate API responses f... | litellm, mock-response, unit-testing |
| 060 | `060-docs-completion-multiple-deployments.md` | Multiple Deployments - liteLLM | This document explains how to configure LiteLLM with multiple model deployments to retrieve the firs... | litellm, model-deployments, multi-provider |
| 061 | `061-docs-completion-predict-outputs.md` | Predicted Outputs - liteLLM | This document demonstrates how to use the litellm library to refactor C# code using the prediction p... | litellm, code-refactoring, python |
| 062 | `062-docs-completion-prompt-caching.md` | Prompt Caching - liteLLM | This document explains how LiteLLM supports and implements prompt caching across various providers i... | litellm, prompt-caching, openai |
| 063 | `063-docs-completion-prompt-formatting.md` | Prompt Formatting - liteLLM | This document explains how LiteLLM handles prompt template translation between models and provides i... | litellm, prompt-templating, huggingface |
| 064 | `064-docs-completion-provider-specific-params.md` | Provider-specific Params - liteLLM | This document explains how to pass provider-specific parameters through LiteLLM using the SDK or Pro... | litellm, provider-parameters, api-configuration |
| 065 | `065-docs-completion-reliable-completions.md` | Reliability - Retries, Fallbacks - liteLLM | This document explains how to use LiteLLM's reliability features, specifically request retries and m... | litellm, error-handling, fallbacks |
| 066 | `066-docs-completion-shared-session.md` | Shared Session Support - liteLLM | This document explains how to share aiohttp.ClientSession instances in LiteLLM to optimize performan... | litellm, aiohttp, client-session |
| 067 | `067-docs-completion-vision.md` | Using Vision Models - liteLLM | This document explains how to use vision-capable models with LiteLLM, including instructions for pas... | litellm, vision-models, multimodal |
| 068 | `068-docs-completion-web-fetch.md` | Web Fetch - liteLLM | This document explains how to use the web fetch tool with LiteLLM to retrieve full content from spec... | litellm, anthropic-claude, web-fetch |
| 069 | `069-docs-completion-web-search.md` | Web Search - liteLLM | This guide explains how to enable and configure web search capabilities across multiple AI providers... | litellm, web-search, ai-search |
| 070 | `070-docs-containers.md` | /containers - liteLLM | This document explains how to manage isolated OpenAI code interpreter sessions using LiteLLM's SDK, ... | litellm, openai, code-interpreter |
| 071 | `071-docs-contribute-integration-custom-webhook-api.md` | Contribute Custom Webhook API - liteLLM | This document provides a step-by-step tutorial on adding native webhook integrations to LiteLLM by c... | litellm, webhooks, callbacks |
| 072 | `072-docs-contributing-adding-openai-compatible-providers.md` | Adding OpenAI-Compatible Providers - liteLLM | This document provides instructions on how to integrate OpenAI-compatible LLM providers into LiteLLM... | litellm, openai-compatible, provider-integration |
| 073 | `073-docs-contributing.md` | Contributing - UI - liteLLM | This document provides instructions for setting up a local development environment to contribute to ... | litellm, ui-development, local-setup |
| 074 | `074-docs-debugging-local-debugging.md` | Local Debugging - liteLLM | This document explains how to perform local debugging and logging for LLM calls using LiteLLM's buil... | litellm, debugging, logging |
| 075 | `075-docs-enterprise.md` | Enterprise - liteLLM | This document outlines the LiteLLM Enterprise features, deployment models, and support services, cov... | enterprise-license, sso-authentication, self-hosted-proxy |
| 076 | `076-docs-extras-code-quality.md` | Code Quality - liteLLM | This document outlines the coding standards and development tools used by LiteLLM to maintain code q... | python-style-guide, linting, formatting |
| 077 | `077-docs-extras-contributing-code.md` | Contributing Code - liteLLM | Outlines the required procedures and technical setup for contributing to the LiteLLM project, includ... | contribution-guide, pull-request, unit-testing |
| 078 | `078-docs-extras-contributing.md` | Contributing to Documentation - liteLLM | This document provides instructions for setting up a local development environment to run, preview, ... | documentation, local-setup, docusaurus |
| 079 | `079-docs-extras-creating-adapters.md` | Call any LiteLLM model in your custom format - liteLLM | This document explains how to create and implement custom adapters in LiteLLM to translate between p... | litellm, custom-adapters, api-translation |
| 080 | `080-docs-extras-gemini-img-migration.md` | Gemini Image Generation Migration Guide - liteLLM | This document details a breaking change in LiteLLM v1.77.0 that updates the response format for Gemi... | litellm, gemini, image-generation |
| 081 | `081-docs-files-endpoints.md` | Provider Files Endpoints - liteLLM | This document explains how to manage file uploads and operations through the LiteLLM proxy, featurin... | litellm, file-management, multi-account |
| 082 | `082-docs-fine-tuning.md` | /fine_tuning - liteLLM | This document explains how to configure and use LiteLLM to manage files and execute fine-tuning jobs... | litellm, fine-tuning, llm-proxy |
| 083 | `083-docs-generateContent.md` | /generateContent - liteLLM | This document provides instructions and code examples for using LiteLLM to interact with Google Gemi... | litellm, google-gemini, python-sdk |
| 084 | `084-docs-guides-code-interpreter.md` | Code Interpreter - liteLLM | This document provides instructions for using OpenAI's Code Interpreter tool through the LiteLLM Pyt... | litellm, openai, code-interpreter |
| 085 | `085-docs-guides-finetuned-models.md` | Calling Finetuned Models - liteLLM | This document provides the syntax and configuration details for calling fine-tuned models from OpenA... | litellm, openai, vertex-ai |
| 086 | `086-docs-guides-security-settings.md` | SSL, HTTP Proxy Security Settings - liteLLM | This document explains how to configure SSL and TLS settings in LiteLLM, covering custom CA bundles,... | litellm, ssl-configuration, tls-security |
| 087 | `087-docs-image-edits.md` | /images/edits - liteLLM | This document explains how to use LiteLLM's image editing capabilities across multiple providers lik... | image-editing, litellm, openai-api |
| 088 | `088-docs-integrations-community.md` | Be an Integration Partner - liteLLM | This document outlines the onboarding process for LiteLLM integration partners, providing instructio... | litellm, integration-partners, community-support |
| 089 | `089-docs-integrations-letta.md` | Letta Integration - liteLLM | This guide explains how to integrate the Letta framework with LiteLLM Proxy and SDK to build memory-... | letta, litellm, ai-agents |
| 090 | `090-docs-langchain.md` | Using ChatLiteLLM() - Langchain - liteLLM | This document provides a guide on integrating LangChain's ChatLiteLLM with various observability too... | litellm, langchain, observability |
| 091 | `091-docs-load-test-advanced.md` | LiteLLM Proxy - 1K RPS Load test on locust - liteLLM | This document provides instructions and a code example for performing load tests on LLM deployments ... | load-testing, llm-performance, locust |
| 092 | `092-docs-load-test-rpm.md` | Multi-Instance TPM/RPM (litellm.Router) - liteLLM | This document provides scripts and instructions for load testing the LiteLLM Router and Proxy to ver... | litellm, load-testing, rate-limiting |
| 093 | `093-docs-load-test-sdk.md` | LiteLLM SDK vs OpenAI - liteLLM | This document provides a code implementation for performing asynchronous load testing across LiteLLM... | litellm, load-testing, python |
| 094 | `094-docs-load-test.md` | LiteLLM Proxy - Locust Load Test - liteLLM | This document provides instructions for performing load testing on a LiteLLM Proxy using the Locust ... | litellm, locust, load-testing |
| 095 | `095-docs-mcp-cost.md` | MCP Cost Tracking - liteLLM | This document explains how to track and manage costs for Model Context Protocol (MCP) tool calls in ... | litellm, mcp-protocol, cost-tracking |
| 096 | `096-docs-mcp-guardrail.md` | MCP Guardrails - liteLLM | This document explains how to configure and implement security guardrails for MCP tool calls in Lite... | litellm, mcp-tools, guardrails |
| 097 | `097-docs-mcp-troubleshoot.md` | MCP Troubleshooting Guide - liteLLM | This document provides a comprehensive troubleshooting guide for diagnosing connectivity and configu... | litellm, mcp-proxy, troubleshooting |
| 098 | `098-docs-migration-policy.md` | Migration Policy - liteLLM | This document outlines the policy and support procedures for beta features that may transition to th... | beta-features, enterprise-tier, licensing-policy |
| 099 | `099-docs-migration.md` | Migration Guide - LiteLLM v1.0.0+ - liteLLM | This document outlines the breaking changes and migration steps required for upgrading to LiteLLM ve... | litellm, breaking-changes, migration-guide |
| 100 | `100-docs-observability-agentops-integration.md` | üñáÔ∏è AgentOps - LLM Observability Platform - liteLLM | This document explains how to integrate AgentOps with LiteLLM using callback functions to enable com... | agentops, litellm, observability |
| 101 | `101-docs-observability-argilla.md` | Argilla - liteLLM | This document provides instructions on how to configure and create a dataset in Argilla using the Py... | argilla, dataset-creation, python-client |
| 102 | `102-docs-observability-arize-integration.md` | Arize AI - liteLLM | This document provides instructions for integrating Arize AI with LiteLLM to enable LLM observabilit... | arize-ai, litellm, llm-observability |
| 103 | `103-docs-observability-athina-integration.md` | Athina - liteLLM | This document explains how to integrate the Athina monitoring and evaluation platform with LiteLLM u... | athina-ai, litellm, llm-monitoring |
| 104 | `104-docs-observability-azure-sentinel.md` | Azure Sentinel - liteLLM | This document provides a comprehensive guide for integrating LiteLLM with Azure Sentinel via the Azu... | litellm, azure-sentinel, azure-monitor |
| 105 | `105-docs-observability-braintrust.md` | Braintrust - Evals + Logging - liteLLM | This guide explains how to integrate LiteLLM with Braintrust to automate logging, evaluation, and tr... | litellm, braintrust, observability |
| 106 | `106-docs-observability-cloudzero.md` | CloudZero Integration - liteLLM | This document explains how to integrate LiteLLM with CloudZero's AnyCost API to export and track LLM... | litellm, cloudzero, cost-tracking |
| 107 | `107-docs-observability-custom-callback.md` | Custom Callbacks - liteLLM | This document explains how to implement custom callback classes and functions in LiteLLM to log even... | litellm, callbacks, logging |
| 108 | `108-docs-observability-datadog.md` | DataDog - liteLLM | This document provides instructions for integrating LiteLLM with Datadog services including Logs, LL... | litellm, datadog, observability |
| 109 | `109-docs-observability-deepeval-integration.md` | üî≠ DeepEval - Open-Source Evals with Tracing - liteLLM | This document explains how to integrate the Confident AI observatory using deepeval to trace and mon... | deepeval, llm-monitoring, llm-tracing |
| 110 | `110-docs-observability-focus.md` | Focus Export (Experimental) - liteLLM | This document describes the experimental feature in LiteLLM for exporting usage data in the FinOps F... | litellm, finops, focus-format |
| 111 | `111-docs-observability-gcs-bucket-integration.md` | Google Cloud Storage Buckets - liteLLM | This document provides step-by-step instructions for configuring LiteLLM to log request and response... | litellm, google-cloud-storage, logging |
| 112 | `112-docs-observability-greenscale-integration.md` | Greenscale - Track LLM Spend and Responsible Usage - liteLLM | This document explains how to integrate Greenscale with liteLLM to monitor GenAI spending and usage ... | litellm, greenscale, llm-monitoring |
| 113 | `113-docs-observability-helicone-integration.md` | Helicone - OSS LLM Observability Platform - liteLLM | This document demonstrates how to integrate LiteLLM with Helicone to route requests and configure ad... | litellm, helicone, api-integration |
| 114 | `114-docs-observability-humanloop.md` | Humanloop - liteLLM | This document provides instructions on how to integrate Humanloop with LiteLLM for prompt management... | humanloop, litellm, prompt-management |
| 115 | `115-docs-observability-lago.md` | Lago - Usage Based Billing - liteLLM | This document provides instructions for integrating Lago with LiteLLM to enable usage-based billing ... | lago, litellm, usage-based-billing |
| 116 | `116-docs-observability-langfuse-integration.md` | ü™¢ Langfuse - Logging LLM Input/Output - liteLLM | This document explains how to integrate Langfuse with LiteLLM to enable model tracing, prompt manage... | langfuse, litellm, llm-observability |
| 117 | `117-docs-observability-langfuse-otel-integration.md` | ü™¢ Langfuse OpenTelemetry Integration - liteLLM | This document provides a comprehensive guide for integrating Langfuse with LiteLLM using the OpenTel... | langfuse, opentelemetry, litellm |
| 118 | `118-docs-observability-langsmith-integration.md` | Langsmith - Logging LLM Input/Output - liteLLM | This document explains how to integrate LiteLLM with LangSmith for automated logging and tracing of ... | litellm, langsmith, logging |
| 119 | `119-docs-observability-langtrace-integration.md` | Langtrace AI - liteLLM | Explains how to integrate Langtrace with LiteLLM to automatically log LLM responses from various pro... | litellm, langtrace, logging |
| 120 | `120-docs-observability-levo-integration.md` | Levo AI - liteLLM | This document explains how to integrate the Levo AI observability platform with LiteLLM to enable mo... | levo, litellm, observability |
| 121 | `121-docs-observability-literalai-integration.md` | Literal AI - Log, Evaluate, Monitor - liteLLM | This document explains how to integrate Literal AI with LiteLLM to enable observability, tracing, an... | literal-ai, litellm, observability |
| 122 | `122-docs-observability-logfire-integration.md` | Logfire - liteLLM | This document provides instructions for integrating Logfire with LiteLLM to enable observability, an... | logfire, litellm, observability |
| 123 | `123-docs-observability-lunary-integration.md` | üåô Lunary - GenAI Observability - liteLLM | This document provides instructions for integrating Lunary with LiteLLM to enable observability, pro... | lunary, litellm, observability |
| 124 | `124-docs-observability-mlflow.md` | üîÅ MLflow - OSS LLM Observability and Evaluation - liteLLM | This document explains how to integrate MLflow with LiteLLM to enable auto-tracing, observability, a... | mlflow, litellm, observability |
| 125 | `125-docs-observability-openmeter.md` | OpenMeter - Usage-Based Billing - liteLLM | This document provides instructions on integrating OpenMeter with LiteLLM to automatically log LLM u... | openmeter, litellm, billing |
| 126 | `126-docs-observability-opentelemetry-integration.md` | OpenTelemetry - Tracing LLMs with any observability tool - liteLLM | This document provides instructions for integrating OpenTelemetry with LiteLLM to enable observabili... | opentelemetry, litellm, observability |
| 127 | `127-docs-observability-opik-integration.md` | Comet Opik - Logging + Evals - liteLLM | This document explains how to integrate Opik with LiteLLM to track and evaluate LLM prompts and resp... | opik, litellm, llm-evaluation |
| 128 | `128-docs-observability-phoenix-integration.md` | Arize Phoenix OSS - liteLLM | This document provides instructions for integrating LiteLLM with Arize Phoenix to enable tracing and... | litellm, arize-phoenix, tracing |
| 129 | `129-docs-observability-posthog-integration.md` | PostHog - Tracking LLM Usage Analytics - liteLLM | This document explains how to integrate PostHog with LiteLLM to track and analyze LLM application me... | posthog, litellm, product-analytics |
| 130 | `130-docs-observability-promptlayer-integration.md` | Promptlayer Tutorial - liteLLM | This document explains how to integrate liteLLM with Promptlayer using success callbacks to log and ... | litellm, promptlayer, logging |
| 131 | `131-docs-observability-qualifire-integration.md` | Qualifire - LLM Evaluation, Guardrails & Observability - liteLLM | This document explains how to integrate Qualifire with LiteLLM to enable real-time evaluations, guar... | qualifire, litellm, ai-observability |
| 132 | `132-docs-observability-raw-request-response.md` | Raw Request/Response Logging - liteLLM | This document provides a code example for integrating LiteLLM with Langfuse to automatically log and... | litellm, langfuse, llm-observability |
| 133 | `133-docs-observability-scrub-data.md` | Scrub Logged Data - liteLLM | This document explains how to redact sensitive information or mask PII in message logs using custom ... | litellm, data-masking, pii-redaction |
| 134 | `134-docs-observability-sentry.md` | Sentry - Log LLM Exceptions - liteLLM | This document explains how to integrate LiteLLM with Sentry for error monitoring, breadcrumbing, and... | litellm, sentry, error-monitoring |
| 135 | `135-docs-observability-signoz.md` | SigNoz LiteLLM Integration - liteLLM | This guide explains how to integrate LiteLLM with SigNoz using OpenTelemetry to capture logs, traces... | litellm, signoz, opentelemetry |
| 136 | `136-docs-observability-slack-integration.md` | Slack - Logging LLM Input/Output, Exceptions - liteLLM | This document explains how to implement and register a custom Slack alert callback function in LiteL... | litellm, slack-integration, callbacks |
| 137 | `137-docs-observability-sumologic-integration.md` | Sumo Logic - liteLLM | This document provides instructions for integrating LiteLLM with Sumo Logic to enable real-time obse... | litellm, sumo-logic, observability |
| 138 | `138-docs-observability-supabase-integration.md` | Supabase Tutorial - liteLLM | This document explains how to integrate LiteLLM with Supabase to log LLM requests and track total sp... | litellm, supabase, llm-logging |
| 139 | `139-docs-observability-wandb-integration.md` | Weights & Biases - Logging LLM Input/Output - liteLLM | This document provides instructions on integrating Weights & Biases with LiteLLM to log model respon... | litellm, weights-and-biases, wandb |
| 140 | `140-docs-oidc.md` | [BETA] OpenID Connect (OIDC) - liteLLM | This document explains how to configure LiteLLM to use OpenID Connect (OIDC) for secure authenticati... | litellm, oidc, authentication |
| 141 | `141-docs-pass-through-anthropic-completion.md` | Anthropic Passthrough - liteLLM | Explains how to use LiteLLM Proxy as a pass-through for native Anthropic API endpoints to enable fea... | litellm-proxy, anthropic, api-passthrough |
| 142 | `142-docs-pass-through-assembly-ai.md` | Assembly AI - liteLLM | This document explains how to use LiteLLM Proxy as a pass-through for Assembly AI endpoints, enablin... | litellm, assembly-ai, api-proxy |
| 143 | `143-docs-pass-through-azure-passthrough.md` | Azure Passthrough - liteLLM | This document explains how to use LiteLLM pass-through endpoints to access Azure OpenAI features not... | litellm, azure-openai, pass-through |
| 144 | `144-docs-pass-through-bedrock.md` | Bedrock (boto3) SDK - liteLLM | This document explains how to use LiteLLM's pass-through endpoints to call AWS Bedrock services in t... | aws-bedrock, litellm-proxy, pass-through-api |
| 145 | `145-docs-pass-through-cohere.md` | Cohere SDK - liteLLM | This document explains how to use LiteLLM Proxy as a pass-through for Cohere's native API endpoints ... | litellm, cohere, pass-through-endpoints |
| 146 | `146-docs-pass-through-google-ai-studio.md` | Google AI Studio SDK - liteLLM | This document explains how to configure and use LiteLLM Proxy as a pass-through for Google AI Studio... | litellm-proxy, google-ai-studio, gemini-api |
| 147 | `147-docs-pass-through-langfuse.md` | Langfuse SDK - liteLLM | This document explains how to configure LiteLLM Proxy as a pass-through for Langfuse endpoints, allo... | litellm-proxy, langfuse, observability |
| 148 | `148-docs-pass-through-mistral.md` | Mistral - liteLLM | This document explains how to use LiteLLM Proxy as a pass-through for native Mistral API endpoints, ... | mistral-ai, litellm-proxy, pass-through-endpoints |
| 149 | `149-docs-pass-through-openai-passthrough.md` | OpenAI Passthrough - liteLLM | This document explains how to use LiteLLM's pass-through endpoints to access newer OpenAI features l... | litellm, openai-api, assistants-api |
| 150 | `150-docs-pass-through-vertex-ai-live-websocket.md` | Vertex AI Live API WebSocket Passthrough - liteLLM | This document explains how to configure and use LiteLLM's WebSocket passthrough for the Vertex AI Li... | vertex-ai, websocket, litellm |
| 151 | `151-docs-pass-through-vertex-ai-search-datastores.md` | Vertex AI Search Datastores - liteLLM | This document provides instructions on how to integrate and use the Vertex AI Discovery Engine Searc... | vertex-ai, discovery-engine, search-api |
| 152 | `152-docs-pass-through-vertex-ai.md` | Vertex AI SDK - liteLLM | This document explains how to use LiteLLM's pass-through endpoints to call Vertex AI services in the... | vertex-ai, litellm-proxy, pass-through |
| 153 | `153-docs-pass-through-vllm.md` | VLLM - liteLLM | This document explains how to configure and use LiteLLM Proxy as a pass-through for native VLLM endp... | litellm-proxy, vllm, pass-through |
| 154 | `154-docs-projects-Google-ADK.md` | Google ADK (Agent Development Kit) - liteLLM | Google ADK is an open-source Python framework for building, evaluating, and deploying AI agents with... | google-adk, python, ai-agents |
| 155 | `155-docs-projects-GPT-Migrate.md` | GPT Migrate - liteLLM | This document provides a high-level overview of strategies and tools for migrating an existing codeb... | code-migration, software-modernization, framework-transition |
| 156 | `156-docs-projects-Harbor.md` | Harbor - liteLLM | Harbor is a framework designed for evaluating, benchmarking, and optimizing language model agents ac... | agent-evaluation, llm-benchmarking, agent-optimization |
| 157 | `157-docs-projects-openai-agents.md` | OpenAI Agents SDK - liteLLM | This document explains how to use the LiteLLM extension with the OpenAI Agents SDK to integrate vari... | openai-agents-sdk, litellm, multi-agent-systems |
| 158 | `158-docs-provider-registration.md` | Integrate as a Model Provider - liteLLM | This document provides a step-by-step technical guide for integrating new chat providers into the li... | litellm, provider-integration, custom-llm |
| 159 | `159-docs-providers-ai21.md` | AI21 - liteLLM | This document provides instructions and configuration details for using AI21 models through LiteLLM'... | litellm, ai21, python-sdk |
| 160 | `160-docs-providers-aiml.md` | AI/ML API - liteLLM | This document provides instructions and code examples for integrating the AI/ML API with LiteLLM to ... | aiml-api, litellm-integration, text-generation |
| 161 | `161-docs-providers-amazon-nova.md` | Amazon Nova - liteLLM | This document provides instructions and code examples for integrating Amazon Nova foundation models ... | amazon-nova, litellm, api-integration |
| 162 | `162-docs-providers-anthropic-programmatic-tool-calling.md` | Anthropic Programmatic Tool Calling - liteLLM | This document explains programmatic tool calling, a feature that enables Claude to invoke tools via ... | programmatic-tool-calling, code-execution, litellm |
| 163 | `163-docs-providers-anthropic-tool-input-examples.md` | Anthropic Tool Input Examples - liteLLM | This document explains how to use the input_examples field in LiteLLM to provide Claude models with ... | litellm, claude, tool-use |
| 164 | `164-docs-providers-anthropic-tool-search.md` | Anthropic Tool Search - liteLLM | This document explains how to implement dynamic tool search with Claude models using LiteLLM to effi... | litellm, claude-ai, tool-search |
| 165 | `165-docs-providers-anyscale.md` | Anyscale - liteLLM | This document provides instructions and code examples for integrating Anyscale Endpoints with LiteLL... | anyscale, litellm, api-integration |
| 166 | `166-docs-providers-aws-polly.md` | AWS Polly Text to Speech (tts) - liteLLM | This document explains how to integrate AWS Polly for text-to-speech synthesis using the LiteLLM SDK... | aws-polly, litellm, text-to-speech |
| 167 | `167-docs-providers-aws-sagemaker.md` | AWS Sagemaker - liteLLM | This document provides instructions and examples for integrating LiteLLM with AWS Sagemaker Huggingf... | aws-sagemaker, litellm, huggingface-jumpstart |
| 168 | `168-docs-providers-azure-ai-agents.md` | Azure AI Foundry Agents - liteLLM | This document provides instructions for integrating Azure AI Foundry Agents with LiteLLM, covering a... | azure-ai-foundry, litellm, agents |
| 169 | `169-docs-providers-azure-ai-azure-ai-vector-stores-passthrough.md` | Azure AI Search - Vector Store (Passthrough API) - liteLLM | This document explains how to configure and use LiteLLM as a proxy for Azure AI Search, allowing dev... | litellm, azure-ai-search, vector-store |
| 170 | `170-docs-providers-azure-ai-azure-model-router.md` | Azure Model Router - liteLLM | This document explains how to integrate Azure Model Router with LiteLLM, covering configuration via ... | azure-model-router, litellm, azure-ai-foundry |
| 171 | `171-docs-providers-azure-ai-img-edit.md` | Azure AI Image Editing - liteLLM | This document provides a comprehensive guide on using Azure AI's FLUX models for image editing via t... | azure-ai, flux-models, image-editing |
| 172 | `172-docs-providers-azure-ai-img.md` | Azure AI Image Generation (Black Forest Labs - Flux) - liteLLM | This document provides a comprehensive guide for integrating Azure AI FLUX models for image generati... | azure-ai, flux-models, image-generation |
| 173 | `173-docs-providers-azure-ai-speech.md` | Azure AI Speech (Cognitive Services) - liteLLM | This document provides a comprehensive guide on integrating Azure AI Speech with LiteLLM for text-to... | azure-ai-speech, litellm, text-to-speech |
| 174 | `174-docs-providers-azure-ai-vector-stores.md` | Azure AI Search - Vector Store (Unified API) - liteLLM | Explains how to perform vector searches on Azure AI Search using LiteLLM's unified API, covering con... | litellm, azure-ai-search, vector-store |
| 175 | `175-docs-providers-azure-ai.md` | Azure AI Studio - liteLLM | This document provides a code example for implementing tool use and function calling with LiteLLM us... | litellm, azure-ai, function-calling |
| 176 | `176-docs-providers-azure-azure-anthropic.md` | Azure Anthropic (Claude via Azure Foundry) - liteLLM | This document explains how to integrate and use Anthropic Claude models deployed via Microsoft Azure... | litellm, azure-foundry, claude |
| 177 | `177-docs-providers-azure-azure-embedding.md` | Azure OpenAI Embeddings - liteLLM | This document provides instructions on how to configure and use Azure OpenAI embedding models using ... | litellm, azure-openai, embeddings |
| 178 | `178-docs-providers-azure-document-intelligence.md` | Azure Document Intelligence OCR - liteLLM | This document provides a comprehensive guide on integrating Azure Document Intelligence with LiteLLM... | azure-document-intelligence, litellm, ocr |
| 179 | `179-docs-providers-azure-ocr.md` | Azure AI OCR (Mistral) - liteLLM | This document provides instructions and code examples for using Azure AI OCR models through LiteLLM ... | azure-ai, ocr, litellm |
| 180 | `180-docs-providers-azure-videos.md` | Azure Video Generation - liteLLM | This document provides instructions for integrating Azure OpenAI's video generation models, includin... | azure-openai, video-generation, sora |
| 181 | `181-docs-providers-azure.md` | Azure OpenAI - liteLLM | This document provides instructions for integrating Azure OpenAI Service and Azure Foundry models wi... | azure-openai, litellm, python-sdk |
| 182 | `182-docs-providers-baseten.md` | Baseten - liteLLM | This document explains how to integrate and use Baseten Model APIs and dedicated deployments within ... | litellm, baseten, api-integration |
| 183 | `183-docs-providers-bedrock-agentcore.md` | Bedrock AgentCore - liteLLM | This document explains how to integrate and call Amazon Bedrock AgentCore runtimes using the LiteLLM... | amazon-bedrock, agentcore, litellm |
| 184 | `184-docs-providers-bedrock-agents.md` | Bedrock Agents - liteLLM | This document provides instructions for calling Amazon Bedrock Agents through LiteLLM using the Open... | amazon-bedrock, bedrock-agents, litellm |
| 185 | `185-docs-providers-bedrock-batches.md` | Bedrock Batches - liteLLM | This document provides a comprehensive guide on configuring and using LiteLLM to execute asynchronou... | amazon-bedrock, litellm, batch-inference |
| 186 | `186-docs-providers-bedrock-image-gen.md` | AWS Bedrock - Image Generation - liteLLM | This document provides instructions on using the LiteLLM library to perform image generation through... | aws-bedrock, image-generation, litellm |
| 187 | `187-docs-providers-bedrock-imported.md` | Bedrock Imported Models - liteLLM | This document outlines how to configure and use various AWS Bedrock imported models, including Deeps... | aws-bedrock, litellm, deepseek |
| 188 | `188-docs-providers-bedrock-vector-store.md` | Bedrock Knowledge Bases - liteLLM | This document explains how to integrate and use AWS Bedrock Knowledge Bases as a vector store within... | aws-bedrock, knowledge-bases, litellm |
| 189 | `189-docs-providers-bedrock-writer.md` | Bedrock - Writer Palmyra - liteLLM | This document provides instructions for integrating Writer Palmyra foundation models from Amazon Bed... | amazon-bedrock, litellm, writer-palmyra |
| 190 | `190-docs-providers-bedrock.md` | AWS Bedrock - liteLLM | This document provides a comprehensive guide for integrating Amazon Bedrock foundation models using ... | amazon-bedrock, litellm, aws-sdk |
| 191 | `191-docs-providers-bytez.md` | Bytez - liteLLM | This document provides instructions for integrating LiteLLM with Bytez models, covering SDK and prox... | litellm, bytez, multi-modal |
| 192 | `192-docs-providers-cerebras.md` | Cerebras - liteLLM | This document provides examples of using LiteLLM to perform synchronous and streaming completions wi... | litellm, cerebras, python |
| 193 | `193-docs-providers-chatgpt.md` | ChatGPT Subscription - liteLLM | This document explains how to integrate ChatGPT Pro/Max subscription models with LiteLLM using OAuth... | litellm, chatgpt-api, oauth-device-flow |
| 194 | `194-docs-providers-chutes.md` | Chutes - liteLLM | This document provides instructions for integrating the Chutes AI deployment platform with LiteLLM, ... | chutes, litellm, ai-deployment |
| 195 | `195-docs-providers-clarifai.md` | Clarifai - liteLLM | This document demonstrates how to use the LiteLLM library to integrate various Clarifai-hosted large... | clarifai, litellm, llm-integration |
| 196 | `196-docs-providers-cloudflare-workers.md` | Cloudflare Workers AI - liteLLM | This document provides instructions and code samples for integrating Cloudflare Workers AI text gene... | cloudflare-workers-ai, litellm, text-generation |
| 197 | `197-docs-providers-cohere.md` | Cohere - liteLLM | This document provides instructions and code examples for integrating Cohere's chat, embedding, and ... | litellm, cohere, python-sdk |
| 198 | `198-docs-providers-cometapi.md` | CometAPI - liteLLM | This document explains how to integrate LiteLLM with CometAPI to access over 500 AI models through a... | litellm, cometapi, llm-integration |
| 199 | `199-docs-providers-custom-llm-server.md` | Custom API Server (Custom Format) - liteLLM | This document provides instructions on integrating custom LLM providers and internal APIs into LiteL... | litellm, custom-llm, api-integration |
| 200 | `200-docs-providers-dashscope.md` | Dashscope (Qwen API) - liteLLM | This document provides instructions for integrating Alibaba DashScope Qwen models with LiteLLM, incl... | dashscope, qwen, litellm |
| 201 | `201-docs-providers-databricks.md` | Databricks - liteLLM | This document explains how to integrate and use models hosted on Databricks with LiteLLM, covering v... | databricks, litellm, authentication |
| 202 | `202-docs-providers-datarobot.md` | DataRobot - liteLLM | This document provides instructions and code examples for integrating LiteLLM with DataRobot models ... | litellm, datarobot, llm-gateway |
| 203 | `203-docs-providers-deepgram.md` | Deepgram - liteLLM | This document provides instructions for integrating Deepgram's speech-to-text services using LiteLLM... | litellm, deepgram, audio-transcription |
| 204 | `204-docs-providers-deepinfra.md` | DeepInfra - liteLLM | This document provides instructions and code examples for integrating DeepInfra's chat and rerank mo... | deepinfra, litellm, llm-integration |
| 205 | `205-docs-providers-deepseek.md` | Deepseek - liteLLM | This document provides instructions and code samples for integrating Deepseek AI models via LiteLLM,... | deepseek, litellm, python |
| 206 | `206-docs-providers-docker-model-runner.md` | Docker Model Runner - liteLLM | This document provides instructions on how to integrate and use LiteLLM with Docker Model Runner for... | docker-model-runner, litellm, local-llm |
| 207 | `207-docs-providers-elevenlabs.md` | ElevenLabs - liteLLM | This document provides instructions for integrating ElevenLabs speech-to-text and text-to-speech ser... | elevenlabs, litellm, speech-to-text |
| 208 | `208-docs-providers-empower.md` | Empower - liteLLM | This document provides instructions and code examples for integrating Empower models with LiteLLM, c... | litellm, empower, python-sdk |
| 209 | `209-docs-providers-fal-ai.md` | Fal AI - liteLLM | This document explains how to integrate Fal AI image generation models with LiteLLM using both the P... | fal-ai, litellm, image-generation |
| 210 | `210-docs-providers-featherless-ai.md` | Featherless AI - liteLLM | This document provides instructions and code samples for integrating Featherless AI models with Lite... | litellm, featherless-ai, python-sdk |
| 211 | `211-docs-providers-fireworks-ai.md` | Fireworks AI - liteLLM | This document provides instructions on how to integrate LiteLLM with Fireworks AI for chat completio... | litellm, fireworks-ai, llm-integration |
| 212 | `212-docs-providers-friendliai.md` | FriendliAI - liteLLM | This document provides instructions and code examples for integrating FriendliAI models with LiteLLM... | friendliai, litellm-integration, llm-inference |
| 213 | `213-docs-providers-galadriel.md` | Galadriel - liteLLM | This document explains how to integrate Galadriel AI models with LiteLLM, including environment vari... | litellm, galadriel, model-integration |
| 214 | `214-docs-providers-gemini-file-search.md` | Gemini File Search - liteLLM | This document explains how to use LiteLLM to interface with Google Gemini's File Search for document... | litellm, google-gemini, rag |
| 215 | `215-docs-providers-gemini-videos.md` | Gemini Video Generation (Veo) - liteLLM | This document provides a comprehensive guide on using Google's Veo video generation models via LiteL... | litellm, google-veo, video-generation |
| 216 | `216-docs-providers-gemini.md` | Gemini - Google AI Studio - liteLLM | This document provides technical instructions for integrating Google AI Studio's Gemini models using... | google-ai-studio, gemini-api, litellm |
| 217 | `217-docs-providers-github-copilot.md` | GitHub Copilot - liteLLM | This document provides instructions for integrating GitHub Copilot with LiteLLM, covering authentica... | github-copilot, litellm, authentication |
| 218 | `218-docs-providers-github.md` | Github - liteLLM | This document explains how to integrate GitHub-hosted models with LiteLLM, demonstrating basic compl... | litellm, github-models, function-calling |
| 219 | `219-docs-providers-google-ai-studio-files.md` | [BETA] Google AI Studio (Gemini) Files API - liteLLM | This document demonstrates how to upload audio files and perform multimodal completions using the Li... | litellm, gemini-api, audio-processing |
| 220 | `220-docs-providers-google-ai-studio-image-gen.md` | Google AI Studio Image Generation - liteLLM | This document explains how to integrate and use Google AI Studio's Imagen models for image generatio... | google-ai-studio, imagen, litellm |
| 221 | `221-docs-providers-google-ai-studio-realtime.md` | Gemini Realtime API - Google AI Studio - liteLLM | This document provides a code example demonstrating how to connect to the LiteLLM Realtime API using... | litellm, websockets, node-js |
| 222 | `222-docs-providers-gradient-ai.md` | GradientAI - liteLLM | This document explains how to integrate and use GradientAI models within the LiteLLM framework, cove... | litellm, gradient-ai, python-sdk |
| 223 | `223-docs-providers-groq.md` | Groq - liteLLM | This document provides a comprehensive guide on integrating Groq models with LiteLLM, including setu... | litellm, groq, api-integration |
| 224 | `224-docs-providers-helicone.md` | Helicone - liteLLM | This document provides instructions and code examples for integrating the Helicone AI gateway with L... | helicone, litellm, ai-gateway |
| 225 | `225-docs-providers-heroku.md` | Heroku - liteLLM | This document explains how to provision and integrate Heroku-hosted AI models with LiteLLM using spe... | heroku, litellm, model-provisioning |
| 226 | `226-docs-providers-huggingface-rerank.md` | HuggingFace Rerank - liteLLM | This document explains how to integrate and use HuggingFace reranking models via LiteLLM, including ... | huggingface, rerank, litellm |
| 227 | `227-docs-providers-huggingface.md` | Hugging Face - liteLLM | This document explains how to access Hugging Face models through various inference providers using L... | litellm, huggingface, inference-api |
| 228 | `228-docs-providers-hyperbolic.md` | Hyperbolic - liteLLM | This document provides instructions for integrating Hyperbolic's AI models with LiteLLM, covering co... | hyperbolic-ai, litellm, llm-api |
| 229 | `229-docs-providers-infinity.md` | Infinity - liteLLM | This document explains how to integrate and use the Infinity text-embedding and reranking API throug... | infinity, litellm, embeddings |
| 230 | `230-docs-providers-jina-ai.md` | Jina AI - liteLLM | This document provides a code example for using the LiteLLM rerank function to order documents by re... | litellm, rerank, jina-ai |
| 231 | `231-docs-providers-langgraph.md` | LangGraph - liteLLM | This document explains how to integrate LangGraph agents with LiteLLM, allowing users to call agents... | langgraph, litellm, openai-format |
| 232 | `232-docs-providers-lemonade.md` | Lemonade - liteLLM | This document provides a guide for integrating Lemonade Server with LiteLLM, detailing how to config... | lemonade-server, litellm, openai-compatible |
| 233 | `233-docs-providers-litellm-proxy.md` | LiteLLM Proxy (LLM Gateway) - liteLLM | This document provides instructions and code examples for routing LLM requests through the LiteLLM P... | litellm, proxy-gateway, llm-api |
| 234 | `234-docs-providers-llamafile.md` | Llamafile - liteLLM | This document explains how to integrate and use llamafile with LiteLLM for chat completions and embe... | litellm, llamafile, openai-compatible |
| 235 | `235-docs-providers-lm-studio.md` | LM Studio - liteLLM | This document provides instructions for integrating LM Studio models with LiteLLM, covering configur... | lm-studio, litellm, local-llm |
| 236 | `236-docs-providers-manus.md` | Manus - liteLLM | This document demonstrates how to manage files and use them with the Manus provider through the Lite... | litellm, manus-ai, file-management |
| 237 | `237-docs-providers-meta-llama.md` | Meta Llama - liteLLM | This document demonstrates how to implement tool use and function calling with Meta Llama models usi... | litellm, meta-llama, function-calling |
| 238 | `238-docs-providers-minimax.md` | MiniMax - liteLLM | This document explains how to integrate MiniMax language models using LiteLLM, detailing support for... | minimax, litellm, anthropic-api |
| 239 | `239-docs-providers-mistral.md` | Mistral AI API - liteLLM | This document provides code examples and instructions for integrating Mistral AI models with LiteLLM... | litellm, mistral-ai, python |
| 240 | `240-docs-providers-moonshot.md` | Moonshot AI - liteLLM | This document explains how to integrate and use Moonshot AI models with LiteLLM through the Python S... | moonshot-ai, litellm, python-sdk |
| 241 | `241-docs-providers-morph.md` | Morph - liteLLM | This document explains how to integrate and use Morph AI models with LiteLLM, covering environment s... | litellm, morph-ai, api-integration |
| 242 | `242-docs-providers-nano-gpt.md` | NanoGPT - liteLLM | This document provides instructions for integrating the NanoGPT model provider with LiteLLM, coverin... | nanogpt, litellm, openai-compatible |
| 243 | `243-docs-providers-nebius.md` | Nebius AI Studio - liteLLM | This document provides code examples for performing standard and streaming chat completions using th... | litellm, nebius-ai-studio, python |
| 244 | `244-docs-providers-nlp-cloud.md` | NLP Cloud - liteLLM | This document provides instructions on how to integrate and use LiteLLM with NLP Cloud, covering API... | litellm, nlp-cloud, python-sdk |
| 245 | `245-docs-providers-novita.md` | Novita AI - liteLLM | This document provides a code example for implementing tool calling with Novita AI models using the ... | litellm, novita-ai, tool-calling |
| 246 | `246-docs-providers-nscale.md` | Nscale (EU Sovereign) - liteLLM | This document provides instructions for integrating Nscale's AI inference services with LiteLLM to p... | nscale, litellm, text-generation |
| 247 | `247-docs-providers-nvidia-nim-rerank.md` | Nvidia NIM - Rerank - liteLLM | This document provides instructions for integrating Nvidia NIM Rerank models with LiteLLM using the ... | nvidia-nim, litellm, rerank-api |
| 248 | `248-docs-providers-oci.md` | Oracle Cloud Infrastructure (OCI) - liteLLM | This document provides instructions on integrating LiteLLM with Oracle Cloud Infrastructure (OCI) Ge... | litellm, oracle-cloud, oci-genai |
| 249 | `249-docs-providers-ollama.md` | Ollama - liteLLM | This document provides a guide for using LiteLLM to interface with Ollama models, detailing setup, s... | litellm, ollama, python-sdk |
| 250 | `250-docs-providers-openai-compatible.md` | OpenAI-Compatible Endpoints - liteLLM | This document explains how to configure and use LiteLLM to route requests to OpenAI-compatible endpo... | litellm, openai-compatible, llm-proxy |
| 251 | `251-docs-providers-openai-responses-api.md` | OpenAI - Response API - liteLLM | This document provides instructions and code examples for using the LiteLLM Python SDK and Proxy ser... | litellm-python-sdk, litellm-proxy, openai-sdk |
| 252 | `252-docs-providers-openai-text-to-speech.md` | OpenAI - Text-to-speech - liteLLM | This document provides instructions for implementing text-to-speech and audio transcription services... | litellm, text-to-speech, python-sdk |
| 253 | `253-docs-providers-openai-videos.md` | OpenAI Video Generation - liteLLM | This document provides a comprehensive guide on using LiteLLM to interface with OpenAI's video gener... | litellm, openai-sora, video-generation |
| 254 | `254-docs-providers-openai.md` | OpenAI - liteLLM | This document provides instructions and code examples for integrating OpenAI chat, vision, and embed... | litellm, openai, chat-completion |
| 255 | `255-docs-providers-openrouter.md` | OpenRouter - liteLLM | This document provides instructions and code examples for integrating LiteLLM with OpenRouter to per... | litellm, openrouter, text-completion |
| 256 | `256-docs-providers-ovhcloud.md` | üÜï OVHCloud AI Endpoints - liteLLM | This document provides instructions and code samples for integrating OVHCloud AI Endpoints with Lite... | ovhcloud, litellm, ai-endpoints |
| 257 | `257-docs-providers-petals.md` | Petals - liteLLM | This document provides instructions and code examples for integrating and running Petals large langu... | petals, litellm, python |
| 258 | `258-docs-providers-poe.md` | Poe - liteLLM | This document explains how to integrate and use Quora's Poe AI platform with LiteLLM, covering envir... | poe-api, litellm-integration, python-sdk |
| 259 | `259-docs-providers-predibase.md` | Predibase - liteLLM | This document provides instructions for integrating Predibase models with LiteLLM, covering setup fo... | litellm, predibase, llm-integration |
| 260 | `260-docs-providers-publicai.md` | PublicAI - liteLLM | This document provides instructions for integrating PublicAI models into LiteLLM using the Python SD... | publicai, litellm, python-sdk |
| 261 | `261-docs-providers-pydantic-ai-agent.md` | Pydantic AI Agents - liteLLM | This document provides a step-by-step guide on how to integrate Pydantic AI agents with the LiteLLM ... | pydantic-ai, litellm, a2a-gateway |
| 262 | `262-docs-providers-ragflow-vector-store.md` | RAGFlow Vector Stores - liteLLM | This document explains how to integrate and manage RAGFlow datasets using LiteLLM, covering configur... | litellm, ragflow, vector-store |
| 263 | `263-docs-providers-ragflow.md` | RAGFlow - liteLLM | This document explains how to integrate RAGFlow with LiteLLM, detailing the specific model naming co... | litellm, ragflow, api-integration |
| 264 | `264-docs-providers-recraft.md` | Recraft - liteLLM | This document provides a guide for integrating Recraft AI with LiteLLM to perform image generation a... | litellm, recraft, image-generation |
| 265 | `265-docs-providers-replicate.md` | Replicate - liteLLM | This document explains how to integrate and use Replicate models through LiteLLM, including authenti... | litellm, replicate, python-sdk |
| 266 | `266-docs-providers-runwayml-images.md` | RunwayML - Image Generation - liteLLM | This document provides a comprehensive guide for integrating and using RunwayML's Gen-4 image genera... | litellm, runwayml, image-generation |
| 267 | `267-docs-providers-runwayml-text-to-speech.md` | RunwayML - Text-to-Speech - liteLLM | This document provides a comprehensive guide for integrating RunwayML's text-to-speech API with Lite... | litellm, runwayml, text-to-speech |
| 268 | `268-docs-providers-runwayml-videos.md` | RunwayML - Video Generation - liteLLM | This document provides a comprehensive guide on using LiteLLM to generate videos via the RunwayML Ge... | litellm, runwayml, video-generation |
| 269 | `269-docs-providers-sambanova.md` | SambaNova - liteLLM | This document provides a comprehensive guide for integrating SambaNova AI models with LiteLLM, cover... | sambanova, litellm, python-sdk |
| 270 | `270-docs-providers-sap.md` | SAP Generative AI Hub - liteLLM | This document provides a guide for integrating LiteLLM with SAP Generative AI Hub, covering authenti... | litellm, sap-ai-core, generative-ai-hub |
| 271 | `271-docs-providers-snowflake.md` | Snowflake - liteLLM | This document provides instructions on how to authenticate and execute completion and embedding call... | snowflake, litellm, authentication |
| 272 | `272-docs-providers-stability.md` | Stability AI - liteLLM | This document provides a guide for using LiteLLM to integrate with Stability AI's REST API for image... | stability-ai, litellm, image-generation |
| 273 | `273-docs-providers-synthetic.md` | Synthetic - liteLLM | This document provides instructions for integrating Synthetic's privacy-focused AI models with LiteL... | synthetic, litellm, ai-integration |
| 274 | `274-docs-providers-text-completion-openai.md` | OpenAI (Text Completion) - liteLLM | This document explains how to use OpenAI text completion and instruct models with LiteLLM, covering ... | openai, litellm, text-completion |
| 275 | `275-docs-providers-triton-inference-server.md` | Triton Inference Server - liteLLM | This document explains how to use LiteLLM to interface with NVIDIA Triton Inference Server for chat ... | triton-inference-server, litellm, chat-completion |
| 276 | `276-docs-providers-v0.md` | v0 - liteLLM | This document provides instructions and code examples for integrating v0.dev AI models with LiteLLM,... | v0-dev, litellm, code-generation |
| 277 | `277-docs-providers-vertex-ai-agent-engine.md` | Vertex AI Agent Engine - liteLLM | This document provides instructions for integrating Vertex AI Agent Engine with LiteLLM, covering Py... | vertex-ai, agent-engine, litellm |
| 278 | `278-docs-providers-vertex-ai-videos.md` | Vertex AI Video Generation (Veo) - liteLLM | This document provides instructions and code examples for integrating LiteLLM with Google Cloud's Ve... | litellm, vertex-ai, veo |
| 279 | `279-docs-providers-vertex-batch.md` | Vertex Batch APIs - liteLLM | This document provides a step-by-step guide for performing batch predictions on Vertex AI using an O... | vertex-ai, batch-prediction, litellm |
| 280 | `280-docs-providers-vertex-image.md` | Vertex AI Image Generation - liteLLM | This document provides instructions and code examples for performing image generation through Google... | vertex-ai, image-generation, litellm |
| 281 | `281-docs-providers-vertex-ocr.md` | Vertex AI OCR - liteLLM | This document provides a comprehensive guide for implementing Vertex AI OCR using LiteLLM, covering ... | vertex-ai, ocr, litellm |
| 282 | `282-docs-providers-vertex-self-deployed.md` | Vertex AI - Self Deployed Models - liteLLM | This document explains how to deploy and access models from Vertex AI Model Garden and custom endpoi... | vertex-ai, google-cloud-platform, model-garden |
| 283 | `283-docs-providers-vertex-speech.md` | Vertex AI Text to Speech - liteLLM | This document provides a comprehensive guide for integrating Google Cloud Text-to-Speech (Chirp3 HD)... | google-cloud, vertex-ai, text-to-speech |
| 284 | `284-docs-providers-vertex.md` | VertexAI [Gemini] - liteLLM | This document provides technical instructions for integrating Google Vertex AI with LiteLLM, coverin... | vertex-ai, litellm, google-cloud |
| 285 | `285-docs-providers-vllm-batches.md` | vLLM - Batch + Files API - liteLLM | This document explains how LiteLLM integrates with vLLM's Batch and Files API to support asynchronou... | litellm, vllm, batch-api |
| 286 | `286-docs-providers-vllm.md` | VLLM - liteLLM | This document explains how to integrate LiteLLM with vLLM models using OpenAI-compatible endpoints, ... | vllm, litellm, openai-compatible |
| 287 | `287-docs-providers-volcano.md` | Volcano Engine (Volcengine) - liteLLM | This document provides instructions and code samples for integrating Volcengine (Doubao) chat and em... | volcengine, litellm, python-sdk |
| 288 | `288-docs-providers-wandb-inference.md` | Weights & Biases Inference - liteLLM | This document explains how to integrate LiteLLM with the W&B Inference service to perform text gener... | litellm, wandb-inference, text-generation |
| 289 | `289-docs-providers-watsonx-audio-transcription.md` | WatsonX Audio Transcription - liteLLM | This document provides a code example for transcribing audio files using the LiteLLM library with th... | litellm, audio-transcription, ibm-watsonx |
| 290 | `290-docs-providers-watsonx.md` | IBM watsonx.ai - liteLLM | This document provides a comprehensive guide for integrating IBM watsonx.ai models and embeddings wi... | ibm-watsonx, litellm, chat-completion |
| 291 | `291-docs-providers-xiaomi-mimo.md` | Xiaomi MiMo - liteLLM | This document provides instructions for integrating Xiaomi MiMo models using LiteLLM, covering API k... | xiaomi-mimo, litellm, api-integration |
| 292 | `292-docs-providers-xinference.md` | Xinference [Xorbits Inference] - liteLLM | This document provides instructions and code examples for integrating LiteLLM with Xinference to per... | xinference, litellm, embeddings |
| 293 | `293-docs-providers-zai.md` | Z.AI (Zhipu AI) - liteLLM | This document provides instructions and code examples for integrating Z.AI GLM text and chat models ... | z-ai, litellm, glm-models |
| 294 | `294-docs-proxy-ai-hub.md` | AI Hub - liteLLM | This document explains how administrators can share and manage models, agents, and MCP servers acros... | ai-hub, model-sharing, agent-discovery |
| 295 | `295-docs-proxy-alerting.md` | Alerting / Webhooks - liteLLM | This document provides instructions for configuring and managing real-time alerts for LLM proxy perf... | litellm, alerting, monitoring |
| 296 | `296-docs-proxy-api.md` | üîë LiteLLM Keys (Access Claude-2, Llama2-70b, etc.) - liteLLM | This document explains how to use the free LiteLLM community key to test various language model prov... | litellm, community-key, llm-testing |
| 297 | `297-docs-proxy-arize-phoenix-prompts.md` | Arize Phoenix Prompt Management - liteLLM | This document provides instructions for integrating Arize Phoenix prompt management with the LiteLLM... | arize-phoenix, litellm, prompt-management |
| 298 | `298-docs-proxy-auto-routing.md` | Auto Routing - liteLLM | This document explains how to configure and implement auto routing in LiteLLM to automatically selec... | litellm, auto-routing, model-selection |
| 299 | `299-docs-proxy-billing.md` | Billing - liteLLM | This document provides instructions for integrating LiteLLM with Lago to enable usage-based billing ... | litellm, lago, usage-based-billing |
| 300 | `300-docs-proxy-call-hooks.md` | Modify / Reject Incoming Requests - liteLLM | This document explains how to implement and configure callback hooks in LiteLLM Proxy to intercept, ... | litellm-proxy, callback-hooks, middleware |
| 301 | `301-docs-proxy-cli-sso.md` | CLI Authentication - liteLLM | This guide explains how to install and use the LiteLLM CLI to authenticate to the LiteLLM Gateway vi... | litellm-cli, sso-authentication, gateway-access |
| 302 | `302-docs-proxy-control-plane-and-data-plane.md` | Control Plane for Multi-region Architecture (Enterprise) - liteLLM | This guide explains how to implement a distributed LiteLLM architecture by separating regional worke... | litellm, multi-region, deployment-architecture |
| 303 | `303-docs-proxy-cost-tracking.md` | Spend Tracking - liteLLM | This document provides instructions on how to set up and manage spend tracking for various LLMs usin... | cost-tracking, litellm-proxy, spend-management |
| 304 | `304-docs-proxy-custom-auth.md` | Custom Auth - liteLLM | This document explains how to implement custom API key authentication for the LiteLLM proxy by defin... | litellm, api-key-auth, custom-authentication |
| 305 | `305-docs-proxy-custom-prompt-management.md` | Custom Prompt Management - liteLLM | This document explains how to integrate custom prompt management systems with LiteLLM by implementin... | litellm, prompt-management, custom-hooks |
| 306 | `306-docs-proxy-custom-sso.md` | ‚ú® Event Hooks for SSO Login - liteLLM | This document explains how to implement custom SSO login handlers and integrate OAuth proxies with t... | litellm, sso-authentication, oauth-proxy |
| 307 | `307-docs-proxy-customer-routing.md` | [DEPRECATED] Region-based Routing - liteLLM | This document explains how to configure LiteLLM to route specific customers to AI models located in ... | litellm, model-routing, data-residency |
| 308 | `308-docs-proxy-customer-usage.md` | Customer Usage - liteLLM | This document explains how to track and visualize end-user spend and usage metrics by associating AP... | customer-usage, spend-tracking, usage-analytics |
| 309 | `309-docs-proxy-customers.md` | Customers / End-User Budgets - liteLLM | This guide explains how to track and manage LLM usage spend for customers using LiteLLM Proxy, inclu... | litellm-proxy, spend-tracking, budget-management |
| 310 | `310-docs-proxy-debugging.md` | Debugging - liteLLM | This document explains how to enable and use debugging features in LiteLLM to monitor raw API reques... | litellm, debugging, logging |
| 311 | `311-docs-proxy-deleted-keys-teams.md` | Deleted Keys & Teams Audit Logs - liteLLM | This document explains how to view audit trails for deleted API keys and teams in LiteLLM, including... | litellm-proxy, audit-logs, api-keys |
| 312 | `312-docs-proxy-deploy.md` | Docker, Helm, Terraform - liteLLM | This document provides comprehensive instructions for deploying and configuring the LiteLLM OSS prox... | litellm, docker, kubernetes |
| 313 | `313-docs-proxy-dynamic-logging.md` | Dynamic Callback Management - liteLLM | This document explains how to dynamically manage and disable LiteLLM callbacks using request headers... | litellm, callback-management, logging-control |
| 314 | `314-docs-proxy-dynamic-rate-limit.md` | Dynamic TPM/RPM Allocation - liteLLM | This document provides a code example for demonstrating and testing rate limiting across multiple AP... | rate-limiting, openai-proxy, token-management |
| 315 | `315-docs-proxy-email.md` | Email Notifications - liteLLM | This document provides instructions for configuring and customizing automated email notifications fo... | litellm, email-notifications, smtp-configuration |
| 316 | `316-docs-proxy-embedding.md` | Embeddings - /embeddings - liteLLM | Provides a quick-start guide for setting up the LiteLLM proxy to handle embedding requests for multi... | litellm, embedding-models, proxy-server |
| 317 | `317-docs-proxy-endpoint-activity.md` | Endpoint Activity - liteLLM | This document explains how LiteLLM automatically tracks and visualizes usage, cost, and performance ... | endpoint-activity, usage-analytics, cost-tracking |
| 318 | `318-docs-proxy-enterprise.md` | ‚ú® Enterprise Features - liteLLM | This document explains how to configure and use LiteLLM enterprise features for security, budget tra... | litellm, enterprise-features, security |
| 319 | `319-docs-proxy-error-diagnosis.md` | Diagnosing Errors - Provider vs Gateway - liteLLM | This guide explains how to identify whether an error message originates from an LLM provider or the ... | litellm, error-handling, debugging |
| 320 | `320-docs-proxy-guardrails-aim-security.md` | Aim Security - liteLLM | This document explains how to set up and integrate Aim Guard with LiteLLM to enforce security polici... | aim-security, litellm, guardrails |
| 321 | `321-docs-proxy-guardrails-aporia-api.md` | Aporia - liteLLM | This document explains how to integrate Aporia guardrails with LiteLLM to detect PII in requests and... | aporia, litellm, guardrails |
| 322 | `322-docs-proxy-guardrails-bedrock.md` | Bedrock Guardrails - liteLLM | This document explains how to integrate and configure AWS Bedrock guardrails within LiteLLM to enfor... | litellm, aws-bedrock, guardrails |
| 323 | `323-docs-proxy-guardrails-custom-guardrail.md` | Custom Guardrail - liteLLM | This document explains how to implement and integrate custom guardrails in LiteLLM by creating a Pyt... | litellm, custom-guardrails, api-gateway |
| 324 | `324-docs-proxy-guardrails-dynamoai.md` | DynamoAI Guardrails - liteLLM | This document explains how to integrate and configure DynamoAI guardrails within LiteLLM for content... | litellm, dynamoai, guardrails |
| 325 | `325-docs-proxy-guardrails-enkryptai.md` | EnkryptAI Guardrails - liteLLM | This document provides instructions for integrating EnkryptAI guardrails with LiteLLM to perform con... | litellm, enkryptai, content-moderation |
| 326 | `326-docs-proxy-guardrails-grayswan.md` | Gray Swan Cygnal Guardrail - liteLLM | This document explains how to integrate Gray Swan Cygnal as a safety guardrail within LiteLLM to mon... | litellm, gray-swan-cygnal, guardrails |
| 327 | `327-docs-proxy-guardrails-guardrail-load-balancing.md` | Guardrail Load Balancing - liteLLM | This document explains how to configure LiteLLM to load balance guardrail requests across multiple d... | litellm, guardrails, load-balancing |
| 328 | `328-docs-proxy-guardrails-guardrails-ai.md` | Guardrails AI - liteLLM | This document provides instructions on integrating Guardrails AI with LiteLLM to implement output va... | guardrails-ai, litellm, llm-security |
| 329 | `329-docs-proxy-guardrails-hiddenlayer.md` | HiddenLayer Guardrails - liteLLM | This document explains how to integrate LiteLLM with HiddenLayer to implement AI security guardrails... | litellm, hiddenlayer, security-guardrails |
| 330 | `330-docs-proxy-guardrails-ibm-guardrails.md` | IBM Guardrails - liteLLM | This document explains how to integrate LiteLLM with IBM's FMS Guardrails to implement content safet... | litellm, ibm-guardrails, content-safety |
| 331 | `331-docs-proxy-guardrails-javelin.md` | Javelin Guardrails - liteLLM | This document provides instructions for integrating Javelin AI safety guardrails into LiteLLM to ena... | ai-safety, litellm, content-moderation |
| 332 | `332-docs-proxy-guardrails-litellm-content-filter.md` | LiteLLM Content Filter (Built-in Guardrails) - liteLLM | This document explains how to implement LiteLLM's built-in content filter guardrail to detect and fi... | litellm, content-filtering, guardrails |
| 333 | `333-docs-proxy-guardrails-noma-security.md` | Noma Security - liteLLM | This document provides instructions for integrating Noma Security guardrails into LiteLLM to enable ... | noma-security, litellm, ai-safety |
| 334 | `334-docs-proxy-guardrails-onyx-security.md` | Onyx Security - liteLLM | This document provides a step-by-step guide for integrating Onyx Guard into LiteLLM to implement AI ... | litellm, onyx-guard, ai-security |
| 335 | `335-docs-proxy-guardrails-openai-moderation.md` | OpenAI Moderation - liteLLM | This guide explains how to integrate and configure OpenAI's Moderation API as a guardrail in LiteLLM... | litellm, openai-moderation, guardrails |
| 336 | `336-docs-proxy-guardrails-panw-prisma-airs.md` | PANW Prisma AIRS - liteLLM | This document provides a guide for integrating PANW Prisma AIRS guardrails with LiteLLM to provide s... | litellm, prisma-airs, guardrails |
| 337 | `337-docs-proxy-guardrails-pii-masking-v2.md` | PII, PHI Masking - Presidio - liteLLM | This document explains how to configure and deploy the Microsoft Presidio guardrail in LiteLLM to ma... | pii-masking, presidio, litellm |
| 338 | `338-docs-proxy-guardrails-pillar-security.md` | Pillar Security - liteLLM | This document explains how to integrate Pillar Security with LiteLLM Proxy to implement AI security ... | litellm, pillar-security, ai-security |
| 339 | `339-docs-proxy-guardrails-prompt-injection.md` | In-memory Prompt Injection Detection - liteLLM | This document explains how to configure and use LiteLLM's prompt injection detection features, inclu... | prompt-injection, security, litellm |
| 340 | `340-docs-proxy-guardrails-prompt-security.md` | Prompt Security - liteLLM | This document provides instructions for integrating Prompt Security guardrails with LiteLLM to prote... | prompt-security, litellm, guardrails |
| 341 | `341-docs-proxy-guardrails-secret-detection.md` | ‚ú® Secret Detection/Redaction (Enterprise-only) - liteLLM | This document explains how to use and configure LiteLLM's secret detection feature to automatically ... | litellm, secret-detection, data-redaction |
| 342 | `342-docs-proxy-guardrails-test-playground.md` | Guardrail Testing Playground - liteLLM | This document provides instructions on how to use the LiteLLM Guardrail Testing Playground to compar... | litellm, guardrails, security-testing |
| 343 | `343-docs-proxy-guardrails-tool-permission.md` | LiteLLM Tool Permission Guardrail - liteLLM | This document explains how to configure LiteLLM Tool Permission Guardrails to control and restrict m... | litellm, guardrails, tool-calling |
| 344 | `344-docs-proxy-guardrails-zscaler-ai-guard.md` | Zscaler AI Guard - liteLLM | This document provides instructions for integrating Zscaler AI Guard with LiteLLM to monitor and con... | zscaler-ai-guard, litellm-integration, security-guardrails |
| 345 | `345-docs-proxy-litellm-managed-files.md` | [BETA] LiteLLM Managed Files - liteLLM | This document explains how to use LiteLLM Enterprise to manage file uploads across multiple AI provi... | litellm, file-management, access-control |
| 346 | `346-docs-proxy-litellm-prompt-management.md` | LiteLLM AI Gateway Prompt Management - liteLLM | This document explains how to use the LiteLLM AI Gateway dashboard to create, manage, and version dy... | litellm-gateway, prompt-management, version-control |
| 347 | `347-docs-proxy-logging.md` | Logging - liteLLM | This document provides instructions on how to configure and manage logging, request tracking, and da... | litellm-proxy, logging, monitoring |
| 348 | `348-docs-proxy-managed-batches.md` | [BETA] LiteLLM Managed Files with Batches - liteLLM | This document explains how to configure and use LiteLLM's Enterprise Batch feature to load balance a... | litellm-proxy, batch-processing, azure-openai |
| 349 | `349-docs-proxy-managed-finetuning.md` | ‚ú® [BETA] LiteLLM Managed Files with Finetuning - liteLLM | This document explains how to configure and use the LiteLLM Proxy to manage fine-tuning jobs across ... | litellm, fine-tuning, openai-proxy |
| 350 | `350-docs-proxy-management-cli.md` | LiteLLM Proxy CLI - liteLLM | This document explains how to use the litellm-proxy CLI tool to manage models, credentials, and user... | litellm-proxy, cli, model-management |
| 351 | `351-docs-proxy-master-key-rotations.md` | Rotating Master Key - liteLLM | This document outlines the step-by-step procedure for rotating the master key used to encrypt models... | master-key-rotation, database-encryption, litellm |
| 352 | `352-docs-proxy-model-access-groups.md` | Model Access Groups - liteLLM | This document explains how to organize multiple AI models into named groups to simplify access contr... | access-control, model-management, api-key-permissions |
| 353 | `353-docs-proxy-model-access-guide.md` | How Model Access Works - liteLLM | This document explains the organization of model deployments into model groups within LiteLLM to ena... | litellm, model-groups, load-balancing |
| 354 | `354-docs-proxy-model-access.md` | Restrict Model Access - liteLLM | This document explains how to manage access to specific models using virtual keys and team IDs, whil... | access-control, litellm-proxy, model-restriction |
| 355 | `355-docs-proxy-model-compare-ui.md` | Model Compare Playground UI - liteLLM | This document explains how to use the Model Compare Playground UI to perform side-by-side evaluation... | model-comparison, llm-playground, performance-metrics |
| 356 | `356-docs-proxy-model-discovery.md` | Model Discovery - liteLLM | This document explains how to configure and use a proxy to retrieve an accurate list of available mo... | proxy-configuration, api-endpoint, model-listing |
| 357 | `357-docs-proxy-multiple-admins.md` | ‚ú® Audit Logs - liteLLM | This document explains how Proxy Admins can use audit logs to track entity changes and perform manag... | audit-logs, proxy-admin, compliance |
| 358 | `358-docs-proxy-native-litellm-prompt.md` | LiteLLM Prompt Management (GitOps) - liteLLM | This document explains how to use LiteLLM to load and execute prompts stored as .prompt files from l... | litellm, prompt-management, dotprompt |
| 359 | `359-docs-proxy-oauth2.md` | Oauth 2.0 Authentication - liteLLM | This document provides instructions for configuring the LiteLLM Proxy to authenticate chat and embed... | litellm-proxy, oauth2-authentication, security-configuration |
| 360 | `360-docs-proxy-pass-through-guardrails.md` | Guardrails on Pass-Through Endpoints - liteLLM | This document explains how to enable and configure security guardrails for LiteLLM pass-through endp... | litellm, guardrails, pass-through-endpoints |
| 361 | `361-docs-proxy-pricing-calculator.md` | Pricing Calculator (Cost Estimation) - liteLLM | This document provides a step-by-step walkthrough for using the Pricing Calculator in the LiteLLM UI... | litellm, cost-estimation, llm-pricing |
| 362 | `362-docs-proxy-prometheus.md` | üìà Prometheus metrics - liteLLM | This document explains how to configure and use the LiteLLM Prometheus metrics endpoint to monitor p... | litellm, prometheus-metrics, monitoring |
| 363 | `363-docs-proxy-provider-margins.md` | Fee/Price Margin on LLM Costs - liteLLM | This document provides a step-by-step guide for configuring percentage-based or fixed-amount price m... | litellm-ui, cost-tracking, price-margins |
| 364 | `364-docs-proxy-public-teams.md` | [BETA] Public Teams - liteLLM | This guide explains how to configure LiteLLM to expose specific teams to users for selection during ... | litellm, team-management, user-signup |
| 365 | `365-docs-proxy-rate-limit-tiers.md` | ‚ú® Budget / Rate Limit Tiers - liteLLM | This document provides instructions on defining tiers with rate limits and assigning these budgets t... | litellm, rate-limiting, budget-management |
| 366 | `366-docs-proxy-reliability.md` | Fallbacks - liteLLM | This document explains how to configure and manage model fallbacks in LiteLLM to handle API failures... | litellm, model-fallbacks, error-handling |
| 367 | `367-docs-proxy-rules.md` | Post-Call Rules - liteLLM | This document explains how to implement custom post-call validation rules in LiteLLM Proxy to evalua... | litellm, proxy-rules, post-call-validation |
| 368 | `368-docs-proxy-self-serve.md` | Internal User Self-Serve - liteLLM | This document explains how to manage user roles, permissions, and automated team provisioning within... | litellm-proxy, user-management, sso-integration |
| 369 | `369-docs-proxy-server.md` | [OLD PROXY üëâ [NEW proxy here](./simple_proxy)] Local LiteLLM Proxy Server - liteLLM | This document explains how to set up and use LiteLLM Proxy, an OpenAI-compatible server that enables... | litellm, openai-compatibility, llm-proxy |
| 370 | `370-docs-proxy-service-accounts.md` | [Beta] Service Accounts - liteLLM | This document explains how to create and manage service account keys for production projects to ensu... | service-account, api-key-management, litellm-proxy |
| 371 | `371-docs-proxy-shared-health-check.md` | Shared Health Check State Across Pods - liteLLM | This document explains how to coordinate health checks across multiple LiteLLM proxy pods using Redi... | litellm, health-checks, redis |
| 372 | `372-docs-proxy-sync-models-github.md` | Auto Sync New Models (Day-0 Launches) - liteLLM | This document explains how to use LiteLLM's auto-sync feature to update model pricing and context wi... | litellm, model-pricing, auto-sync |
| 373 | `373-docs-proxy-tag-budgets.md` | Setting Tag Budgets - liteLLM | This document explains how to use tags to monitor costs and enforce budget limits for LLM API reques... | cost-tracking, budget-management, llm-usage |
| 374 | `374-docs-proxy-tag-routing.md` | Tag Based Routing - liteLLM | This document explains how to implement tag-based request routing in LiteLLM Proxy to manage model a... | litellm, tag-routing, request-routing |
| 375 | `375-docs-proxy-team-based-routing.md` | [DEPRECATED] Team-based Routing - liteLLM | This document explains how to implement team-based model routing and aliases within the LiteLLM prox... | litellm, model-routing, team-management |
| 376 | `376-docs-proxy-team-budgets.md` | Setting Team Budgets - liteLLM | This document explains how to configure and enforce budget limits for teams in LiteLLM, including se... | litellm, team-management, budget-limits |
| 377 | `377-docs-proxy-team-logging.md` | Team/Key Based Logging - liteLLM | This document explains how to configure team-specific and key-specific logging callbacks to enable g... | logging-callbacks, team-management, litellm-proxy |
| 378 | `378-docs-proxy-team-model-add.md` | ‚ú® Allow Teams to Add Models - liteLLM | This document explains how to allow teams to register their own custom models and API keys using the... | model-management, team-configuration, api-integration |
| 379 | `379-docs-proxy-temporary-budget-increase.md` | ‚ú® Temporary Budget Increase - liteLLM | This document provides instructions on how to create a LiteLLM Virtual Key and apply a temporary bud... | litellm, virtual-key, budget-management |
| 380 | `380-docs-proxy-token-auth.md` | OIDC - JWT-based Auth - liteLLM | This document explains how to configure and use JWT authentication for the LiteLLM proxy, including ... | jwt-auth, oidc-integration, litellm-proxy |
| 381 | `381-docs-proxy-ui-bulk-edit-users.md` | Bulk Edit Users - liteLLM | This document explains how to perform bulk edits to assign multiple existing users to a specific tea... | user-management, bulk-edit, team-assignment |
| 382 | `382-docs-proxy-ui-credentials.md` | Adding LLM Credentials - liteLLM | This document provides step-by-step instructions for adding, reusing, and managing LLM provider cred... | llm-credentials, api-keys, provider-management |
| 383 | `383-docs-proxy-ui-logs-sessions.md` | Session Logs - liteLLM | This document explains how to group related API requests into sessions using session IDs or previous... | litellm, session-management, request-grouping |
| 384 | `384-docs-proxy-user-onboarding.md` | User Onboarding Guide - liteLLM | This document provides a comprehensive guide for administrators to onboard users and for end users t... | litellm-proxy, user-onboarding, api-key-management |
| 385 | `385-docs-proxy-veo-video-generation.md` | Veo Video Generation with Google AI Studio - liteLLM | This document provides a Python script demonstrating the workflow for generating videos with the Gem... | python, video-generation, gemini-api |
| 386 | `386-docs-proxy-virtual-keys.md` | Virtual Keys - liteLLM | Explains how to set up and manage virtual keys for the LiteLLM Proxy to track usage spend and contro... | litellm, api-proxy, virtual-keys |
| 387 | `387-docs-reasoning-content.md` | 'Thinking' / 'Reasoning Content' - liteLLM | This document provides a code implementation for executing function calls using the LiteLLM library,... | litellm, function-calling, tool-use |
| 388 | `388-docs-rerank.md` | /rerank - liteLLM | This document provides instructions for implementing document reranking across multiple providers us... | litellm, rerank, python-sdk |
| 389 | `389-docs-response-api.md` | /responses - liteLLM | This document demonstrates how to configure and use LiteLLM's Router to ensure consistent routing to... | litellm, model-routing, azure-openai |
| 390 | `390-docs-routing.md` | Router - Load Balancing - liteLLM | This document explains how to use the LiteLLM Router to manage load balancing, failover, and reliabi... | litellm, load-balancing, model-routing |
| 391 | `391-docs-rules.md` | Rules - liteLLM | This document explains how to implement custom pre-call and post-call validation rules in LiteLLM to... | litellm, validation-rules, error-handling |
| 392 | `392-docs-scheduler.md` | [BETA] Request Prioritization - liteLLM | This document explains how to implement request prioritization in LiteLLM to manage high-traffic LLM... | litellm, request-prioritization, llm-api |
| 393 | `393-docs-sdk-custom-pricing.md` | Custom Pricing - SageMaker, Azure, etc - liteLLM | This document explains how to manually register and track custom pricing for SageMaker and Azure mod... | litellm, sagemaker, azure |
| 394 | `394-docs-search-dataforseo.md` | DataForSEO Search - liteLLM | This document explains how to integrate and use the DataForSEO search provider with LiteLLM via the ... | dataforseo, litellm, search-api |
| 395 | `395-docs-search-exa-ai.md` | Exa AI Search - liteLLM | This document provides instructions and code examples for integrating Exa AI Search with LiteLLM usi... | litellm, exa-ai, search-api |
| 396 | `396-docs-search-firecrawl.md` | Firecrawl Search - liteLLM | This code example demonstrates how to use the LiteLLM search function with Firecrawl as the provider... | litellm, firecrawl, search-api |
| 397 | `397-docs-search-google-pse.md` | Google Programmable Search Engine (PSE) - liteLLM | This document demonstrates how to use the litellm library to perform web searches using the Google P... | litellm, google-pse, search-api |
| 398 | `398-docs-search-parallel-ai.md` | Parallel AI Search - liteLLM | This document provides instructions for integrating Parallel AI Search with LiteLLM using the Python... | litellm, parallel-ai, python-sdk |
| 399 | `399-docs-search-perplexity.md` | Perplexity AI Search - liteLLM | This document provides instructions for integrating Perplexity Search into LiteLLM using the Python ... | litellm, perplexity-ai, search-api |
| 400 | `400-docs-search-searxng.md` | SearXNG Search - liteLLM | This document provides instructions on integrating the SearXNG metasearch engine with LiteLLM using ... | searxng, litellm, metasearch-engine |
| 401 | `401-docs-search-tavily.md` | Tavily Search - liteLLM | This document provides instructions and code examples for integrating Tavily Search using the LiteLL... | litellm, tavily-search, python-sdk |
| 402 | `402-docs-secret-managers-custom-secret-manager.md` | Custom Secret Manager - liteLLM | This document provides instructions for integrating custom secret management systems with LiteLLM by... | litellm, secret-management, custom-integration |
| 403 | `403-docs-secret-managers-cyberark.md` | CyberArk Conjur - liteLLM | This guide provides instructions for integrating CyberArk Conjur with LiteLLM to manage, read, and w... | cyberark-conjur, secrets-management, litellm-proxy |
| 404 | `404-docs-secret-managers-google-kms.md` | Google Key Management Service - liteLLM | This guide explains how to configure and use Google KMS for managing encrypted environment variables... | google-kms, litellm, encryption |
| 405 | `405-docs-skills.md` | /skills - Anthropic Skills API - liteLLM | This document explains how to create, manage, and use reusable AI capabilities via the Anthropic Ski... | litellm, anthropic-skills, python-sdk |
| 406 | `406-docs-text-to-speech.md` | /audio/speech - liteLLM | This document provides instructions for implementing and configuring Text-to-Speech (TTS) functional... | litellm, text-to-speech, tts |
| 407 | `407-docs-troubleshoot-cpu-issues.md` | CPU Issue Classification & Reproduction - liteLLM | This document provides a framework for identifying, reproducing, and reporting CPU performance issue... | litellm, cpu-usage, performance-troubleshooting |
| 408 | `408-docs-troubleshoot-memory-issues.md` | Memory Issue Classification & Reproduction - liteLLM | This document outlines a structured approach for identifying, reproducing, and reporting memory issu... | litellm, memory-management, troubleshooting |
| 409 | `409-docs-troubleshoot.md` | Troubleshooting & Support - liteLLM | This document outlines the specific information required to effectively report issues with LiteLLM a... | troubleshooting, issue-reporting, debugging |
| 410 | `410-docs-tutorials-anthropic-file-usage.md` | Using Anthropic File API with LiteLLM Proxy - liteLLM | This tutorial explains how to upload files and perform data analysis using Claude-4 through the Lite... | litellm-proxy, anthropic-claude, file-management |
| 411 | `411-docs-tutorials-azure-openai.md` | Replacing OpenAI ChatCompletion with Completion() - liteLLM | This document provides a quick-start guide for implementing chat completions using LiteLLM with Open... | litellm, openai, azure-openai |
| 412 | `412-docs-tutorials-claude-code-customer-tracking.md` | Claude Code - Granular Cost Tracking - liteLLM | This document explains how to track Claude Code usage and attribute costs to specific customers or p... | claude-code, litellm-proxy, cost-attribution |
| 413 | `413-docs-tutorials-claude-code-max-subscription.md` | Using Claude Code Max Subscription - liteLLM | This document provides a step-by-step guide for routing Claude Code Max subscription traffic through... | litellm, claude-code, ai-gateway |
| 414 | `414-docs-tutorials-claude-code-plugin-marketplace.md` | Claude Code Plugin Marketplace - liteLLM | This document explains how to set up and manage a centralized Claude Code plugin marketplace using L... | litellm, claude-code, plugin-management |
| 415 | `415-docs-tutorials-claude-code-websearch.md` | Claude Code - WebSearch Across All Providers - liteLLM | This document explains how to configure LiteLLM to enable web search capabilities for Claude Code wh... | litellm, claude-code, web-search |
| 416 | `416-docs-tutorials-claude-mcp.md` | Use Claude Code with MCPs - liteLLM | This tutorial provides a step-by-step guide on connecting and authenticating Model Context Protocol ... | mcp, litellm-proxy, claude-code |
| 417 | `417-docs-tutorials-claude-non-anthropic-models.md` | Use Claude Code with Non-Anthropic Models - liteLLM | This tutorial explains how to integrate Claude Code with non-Anthropic LLM providers using LiteLLM a... | claude-code, litellm, api-proxy |
| 418 | `418-docs-tutorials-compare-llms-2.md` | Comparing LLMs on a Test Set using LiteLLM - liteLLM | This document demonstrates how to use the LiteLLM library to call multiple large language models usi... | litellm, multi-model, python-sdk |
| 419 | `419-docs-tutorials-compare-llms.md` | Benchmark LLMs - liteLLM | This document provides instructions for setting up and running performance benchmarks to compare res... | litellm, benchmarking, llm-performance |
| 420 | `420-docs-tutorials-cost-tracking-coding.md` | Track Usage for Coding Tools - liteLLM | This document explains how to track and monitor usage, costs, and engagement for AI-powered coding t... | litellm-proxy, usage-tracking, cost-monitoring |
| 421 | `421-docs-tutorials-cursor-integration.md` | Cursor Integration - liteLLM | This document provides instructions for routing Cursor IDE requests through a LiteLLM proxy to enabl... | cursor-ide, litellm, proxy-configuration |
| 422 | `422-docs-tutorials-default-team-self-serve.md` | Onboard Users for AI Exploration - liteLLM | This document explains how to configure default teams to automatically assign new users, allowing th... | litellm, default-teams, user-onboarding |
| 423 | `423-docs-tutorials-elasticsearch-logging.md` | Elasticsearch Logging with LiteLLM - liteLLM | This document provides a step-by-step guide for integrating LiteLLM with Elasticsearch via OpenTelem... | litellm, elasticsearch, opentelemetry |
| 424 | `424-docs-tutorials-eval-suites.md` | Evaluate LLMs - MLflow Evals, Auto Eval - liteLLM | This document explains how to integrate LiteLLM with evaluation tools like MLflow and AutoEvals to b... | litellm, mlflow, autoevals |
| 425 | `425-docs-tutorials-fallbacks.md` | Using completion() with Fallbacks for Reliability - liteLLM | This document explains how to use model fallbacks and error handling within the completion function ... | model-fallbacks, error-handling, api-reliability |
| 426 | `426-docs-tutorials-finetuned-chat-gpt.md` | Using Fine-Tuned gpt-3.5-turbo - liteLLM | This document explains how to use the LiteLLM library to call fine-tuned OpenAI models and configure... | litellm, openai, fine-tuned-models |
| 427 | `427-docs-tutorials-first-playground.md` | Create your first LLM playground - liteLLM | This tutorial provides a step-by-step guide to building a functional LLM playground by setting up a ... | litellm, llm-playground, flask |
| 428 | `428-docs-tutorials-gemini-realtime-with-audio.md` | Call Gemini Realtime API with Audio Input/Output - liteLLM | This document provides a Python implementation for streaming audio data to a realtime WebSocket API ... | python, websockets, realtime-api |
| 429 | `429-docs-tutorials-github-copilot-integration.md` | GitHub Copilot - liteLLM | This tutorial explains how to route GitHub Copilot requests through LiteLLM Proxy to access various ... | github-copilot, litellm-proxy, llm-gateway |
| 430 | `430-docs-tutorials-google-adk.md` | Google ADK with LiteLLM - liteLLM | This tutorial explains how to build intelligent agents using the Google Agent Development Kit (ADK) ... | google-adk, litellm, llm-agents |
| 431 | `431-docs-tutorials-gradio-integration.md` | Gradio Chatbot + LiteLLM Tutorial - liteLLM | This document provides a brief tutorial on integrating LiteLLM streaming completion calls into a Gra... | litellm, gradio, chatbot |
| 432 | `432-docs-tutorials-huggingface-codellama.md` | CodeLlama - Code Infilling - liteLLM | This tutorial explains how to use LiteLLM to perform code infilling tasks with CodeLlama models host... | code-infilling, litellm, codellama |
| 433 | `433-docs-tutorials-huggingface-tutorial.md` | Llama2 - Huggingface Tutorial - liteLLM | This document provides instructions on how to use LiteLLM to call models via Hugging Face inference ... | huggingface, litellm, inference-endpoints |
| 434 | `434-docs-tutorials-instructor.md` | Instructor - liteLLM | This document explains how to integrate LiteLLM with the instructor library to generate validated st... | litellm, instructor-library, pydantic |
| 435 | `435-docs-tutorials-litellm-gemini-cli.md` | Gemini CLI - liteLLM | This tutorial explains how to integrate the Gemini CLI with LiteLLM Proxy to enable unified model ac... | gemini-cli, litellm-proxy, model-routing |
| 436 | `436-docs-tutorials-litellm-proxy-aporia.md` | Aporia Guardrails with LiteLLM Gateway - liteLLM | This tutorial explains how to integrate LiteLLM AI Gateway with Aporia guardrails to detect PII in p... | litellm, aporia, guardrails |
| 437 | `437-docs-tutorials-litellm-qwen-code-cli.md` | Qwen Code CLI - liteLLM | This tutorial provides instructions for integrating the Qwen Code CLI with LiteLLM Proxy to manage m... | qwen-code, litellm, proxy-integration |
| 438 | `438-docs-tutorials-litellm-Test-Multiple-Providers.md` | Reliability test Multiple LLM Providers with LiteLLM - liteLLM | This document demonstrates how to perform batch completion and load testing across multiple LLM prov... | llm-benchmarking, load-testing, latency-analysis |
| 439 | `439-docs-tutorials-lm-evaluation-harness.md` | Benchmark LLMs - LM Harness, FastEval, Flask - liteLLM | This document provides step-by-step instructions for running various LLM benchmarks using the LiteLL... | llm-benchmarking, litellm-proxy, lm-eval-harness |
| 440 | `440-docs-tutorials-mock-completion.md` | Mock Completion Responses - Save Testing Costs - liteLLM | This document explains how to use the mock_response parameter in LiteLLM to simulate API responses f... | litellm, mock-response, unit-testing |
| 441 | `441-docs-tutorials-model-config-proxy.md` | Customize Prompt Templates on OpenAI-Compatible server - liteLLM | This tutorial explains how to define and implement custom prompt templates for specific models withi... | litellm, prompt-templates, llm-configuration |
| 442 | `442-docs-tutorials-model-fallbacks.md` | Model Fallbacks w/ LiteLLM - liteLLM | This document demonstrates how to implement model fallbacks and handle context window exceptions acr... | litellm, model-fallback, error-handling |
| 443 | `443-docs-tutorials-msft-sso.md` | Microsoft SSO: Sync Groups, Members with LiteLLM - liteLLM | This document provides instructions for synchronizing Microsoft Entra ID groups and memberships with... | microsoft-entra-id, azure-ad, sso-integration |
| 444 | `444-docs-tutorials-oobabooga.md` | Oobabooga Text Web API Tutorial - liteLLM | This document explains how to integrate LiteLLM with a local Oobabooga model server to perform text ... | litellm, oobabooga, local-llm |
| 445 | `445-docs-tutorials-openai-codex.md` | OpenAI Codex - liteLLM | This guide provides step-by-step instructions for integrating OpenAI Codex with LiteLLM, allowing us... | litellm, openai-codex, llm-proxy |
| 446 | `446-docs-tutorials-openweb-ui.md` | Open WebUI - liteLLM | This guide explains how to integrate Open WebUI with LiteLLM to manage access to multiple LLMs, trac... | open-webui, litellm, llm-proxy |
| 447 | `447-docs-tutorials-presidio-pii-masking.md` | Presidio PII Masking with LiteLLM - Complete Tutorial - liteLLM | This tutorial explains how to integrate Microsoft Presidio with LiteLLM Gateway to automatically det... | pii-masking, litellm-gateway, microsoft-presidio |
| 448 | `448-docs-tutorials-prompt-caching.md` | Auto-Inject Prompt Caching Checkpoints - liteLLM | This document explains how to use LiteLLM's auto-injection feature to automatically insert prompt ca... | litellm, prompt-caching, cost-optimization |
| 449 | `449-docs-tutorials-provider-specific-params.md` | provider_specific_params - liteLLM | This document explains how LiteLLM automatically maps and translates model parameters like max_token... | litellm, parameter-mapping, max-tokens |
| 450 | `450-docs-tutorials-scim-litellm.md` | SCIM with LiteLLM - liteLLM | This tutorial provides instructions for connecting identity providers to LiteLLM SCIM endpoints to a... | scim, sso, identity-management |
| 451 | `451-docs-tutorials-tag-management.md` | [Beta] Routing based on request metadata - liteLLM | This document explains how to configure and use tag-based routing in the LiteLLM proxy to control wh... | litellm-proxy, tag-routing, access-control |
| 452 | `452-docs-tutorials-text-completion.md` | Using Text Completion Format - with Completion() - liteLLM | This tutorial explains how to use LiteLLM to interface with various language models using the OpenAI... | litellm, text-completion, openai-api-format |
| 453 | `453-docs-tutorials-TogetherAI-liteLLM.md` | Llama2 Together AI Tutorial - liteLLM | This document explains how to use LiteLLM to interact with Together AI models, covering basic comple... | litellm, together-ai, llama-2 |
| 454 | `454-docs-vertex-batch-passthrough.md` | /batchPredictionJobs - liteLLM | This document explains how to configure and use LiteLLM to manage Vertex AI batch prediction jobs, i... | vertex-ai, batch-prediction, litellm |
| 455 | `455-docs-videos.md` | /videos - liteLLM | This document provides instructions and code examples for using LiteLLM to generate and manage video... | litellm, video-generation, python-sdk |
| 456 | `456-observability-callbacks.md` | Callbacks - liteLLM | This document explains how to configure success and failure callbacks in liteLLM to automatically se... | litellm, callbacks, monitoring |
| 457 | `457-observability-helicone-integration.md` | Helicone Tutorial - liteLLM | This document explains how to integrate LiteLLM with Helicone to monitor and log LLM requests across... | helicone, litellm, observability |
| 458 | `458-observability-supabase-integration.md` | Supabase Tutorial - liteLLM | This document explains how to integrate Supabase with liteLLM to log request data and track costs ac... | supabase, litellm, request-logging |
| 459 | `459-release-notes-tags-deepgram.md` | One post tagged with "deepgram" - liteLLM | This document provides instructions on performing audio transcription with Deepgram using LiteLLM an... | litellm, audio-transcription, deepgram |
| 460 | `460-release-notes-tags-dependency-upgrades.md` | One post tagged with "dependency upgrades" - liteLLM | This document demonstrates how to perform audio transcription using LiteLLM with Deepgram and explai... | litellm, audio-transcription, deepgram |
| 461 | `461-release-notes-tags-docker-image.md` | One post tagged with "docker image" - liteLLM | This document explains the update of the LiteLLM Docker base image to a Chainguard image to eliminat... | docker-image, security, vulnerability-management |
| 462 | `462-release-notes-tags-fireworks-ai.md` | One post tagged with "fireworks ai" - liteLLM | This document explains how to perform audio transcription using LiteLLM with Deepgram and describes ... | litellm, transcription, deepgram |
| 463 | `463-release-notes-tags-virtual-key-management.md` | One post tagged with "virtual key management" - liteLLM | This document explains how to monitor guardrail performance, list available guardrails, and test gua... | litellm, guardrails, api-monitoring |
| 464 | `464-release-notes-tags-vision.md` | One post tagged with "vision" - liteLLM | This document demonstrates how to use LiteLLM for audio transcription via Deepgram and explains the ... | litellm, audio-transcription, deepgram |
| 465 | `465-release-notes-tags-vulnerability.md` | One post tagged with "vulnerability" - liteLLM | This document explains the transition of the LiteLLM base Docker image to a Chainguard-based image t... | docker-image, security, vulnerability-management |
| 466 | `466-release-notes-v1.57.3.md` | v1.57.3 - New Base Docker Image | This document announces the migration of the LiteLLM base Docker image to a Chainguard Python image ... | docker-image, security, vulnerability-management |
| 467 | `467-stream.md` | Streaming Responses & Async Completion - liteLLM | This document explains how to implement streaming responses, asynchronous completion, and token usag... | litellm, streaming, async-completion |
| 468 | `468-troubleshoot.md` | Troubleshooting - liteLLM | This document provides instructions for installing a specific stable version of the litellm library ... | litellm, installation, stable-version |

### 4. Concepts & Fundamentals (469-481)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 469 | `469-docs-projects-Agent-Lightning.md` | Agent Lightning - liteLLM | Agent Lightning is an open-source Microsoft framework for training and optimizing AI agents using te... | ai-agents, reinforcement-learning, prompt-optimization |
| 470 | `470-docs-projects-GraphRAG.md` | Microsoft GraphRAG - liteLLM | This document introduces GraphRAG, a data pipeline and transformation suite that uses LLMs and knowl... | graphrag, knowledge-graph, retrieval-augmented-generation |
| 471 | `471-docs-projects-HolmesGPT.md` | HolmesGPT - liteLLM | This document introduces HolmesGPT, an AI-powered observability assistant for incident response that... | observability, incident-response, ai-assistant |
| 472 | `472-docs-projects-OpenInterpreter.md` | OpenInterpreter - liteLLM | This document introduces Open Interpreter, an open-source tool that enables large language models to... | open-interpreter, llm, code-execution |
| 473 | `473-docs-projects-Otter.md` | Otter - liteLLM | This document introduces Otter, a multi-modal artificial intelligence model derived from OpenFlaming... | multi-modal-model, open-flamingo, instruction-following |
| 474 | `474-docs-projects-PDL.md` | PDL - liteLLM | This document introduces the Prompt Declaration Language (PDL), a declarative YAML-based approach fo... | prompt-programming, yaml, declarative-language |
| 475 | `475-docs-projects-YiVal.md` | YiVal - liteLLM | YiVal is an open-source GenAI-Ops framework designed to automate the tuning and evaluation of AIGC p... | genai-ops, prompt-engineering, model-evaluation |
| 476 | `476-docs-proxy-access-control.md` | Role-based Access Controls (RBAC) - liteLLM | This document explains the hierarchical role-based access control (RBAC) system in LiteLLM, defining... | rbac, litellm, access-control |
| 477 | `477-docs-proxy-architecture.md` | Life of a Request - liteLLM | This document outlines the high-level architecture and request flow of the LiteLLM Proxy Server, det... | litellm, proxy-server, architecture |
| 478 | `478-docs-proxy-image-handling.md` | Image URL Handling - liteLLM | Explains how LiteLLM automatically converts image URLs to base64 strings for incompatible LLM APIs a... | litellm, image-processing, base64-conversion |
| 479 | `479-docs-proxy-multi-tenant-architecture.md` | Multi-Tenant Architecture with LiteLLM - liteLLM | This document explains LiteLLM's hierarchical multi-tenant architecture for managing LLM access, cos... | multi-tenancy, litellm, access-control |
| 480 | `480-docs-proxy-user-management-heirarchy.md` | User Management Hierarchy - liteLLM | This document describes the hierarchical relationship between organizations, teams, users, and budge... | litellm, user-hierarchy, organization-management |
| 481 | `481-docs-router-architecture.md` | Router Architecture (Fallbacks / Retries) - liteLLM | This document describes the high-level architecture and internal request flow of the LiteLLM Router,... | litellm, router-architecture, request-flow |

### 5. Configuration & Settings (482-531)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 482 | `482-docs-completion-drop-params.md` | Drop Unsupported Params - liteLLM | This document explains how to manage unsupported OpenAI parameters in LiteLLM by dropping them autom... | litellm, parameter-management, openai-compatibility |
| 483 | `483-docs-completion-http-handler-config.md` | Custom HTTP Handler - liteLLM | This document explains how to configure and inject custom aiohttp sessions into LiteLLM to optimize ... | litellm, aiohttp, python |
| 484 | `484-docs-completion-model-alias.md` | Model Alias - liteLLM | This document explains how to use LiteLLM's model alias mapping feature to link user-friendly displa... | litellm, model-aliasing, python |
| 485 | `485-docs-completion-usage.md` | Usage - liteLLM | This document explains how to retrieve token usage statistics in LiteLLM when using streaming respon... | litellm, token-usage, streaming |
| 486 | `486-docs-mcp-control.md` | MCP Permission Management - liteLLM | This document explains how to configure and manage access controls for MCP servers in LiteLLM, cover... | mcp-server, access-control, permissions |
| 487 | `487-docs-observability-callbacks.md` | Callbacks - liteLLM | This document explains how to configure input, success, and failure callbacks in liteLLM to integrat... | litellm, callbacks, logging |
| 488 | `488-docs-observability-generic-api.md` | Generic API Callback (Webhook) - liteLLM | This document explains how to configure and use LiteLLM's generic HTTP callbacks to send logs to ext... | litellm, http-callbacks, logging |
| 489 | `489-docs-old-guardrails.md` | üõ°Ô∏è [Beta] Guardrails - liteLLM | This document provides instructions for configuring security guardrails on LiteLLM Proxy, including ... | litellm, security-guardrails, prompt-injection |
| 490 | `490-docs-provider-registration-add-model-pricing.md` | Add Model Pricing & Context Window - liteLLM | This document provides the schema and instructions for contributing model metadata, including pricin... | model-metadata, pricing-configuration, token-limits |
| 491 | `491-docs-providers-anthropic-effort.md` | Anthropic Effort Parameter - liteLLM | This document explains how to use the effort parameter with Claude Opus 4.5 to balance response thor... | claude-opus-4-5, token-optimization, reasoning-effort |
| 492 | `492-docs-proxy-admin-ui-sso.md` | ‚ú® SSO for Admin UI - liteLLM | This document provides instructions for configuring Single Sign-On (SSO) with providers like Okta, G... | sso-configuration, okta-setup, authentication |
| 493 | `493-docs-proxy-budget-reset-and-tz.md` | budget_reset_and_tz - liteLLM | This document explains how LiteLLM handles automatic budget reset schedules and provides instruction... | litellm, budget-management, timezone-configuration |
| 494 | `494-docs-proxy-caching.md` | Caching - liteLLM | This document explains how to configure and use LiteLLM's caching systems to store and reuse LLM res... | litellm, caching, redis-config |
| 495 | `495-docs-proxy-clientside-auth.md` | Clientside LLM Credentials - liteLLM | This document demonstrates how to define and pass a custom configuration object to an LLM proxy to m... | litellm, model-routing, fallback-logic |
| 496 | `496-docs-proxy-config-management.md` | File Management - liteLLM | This document explains how to use the include directive in LiteLLM configuration files to import and... | litellm, yaml-configuration, include-directive |
| 497 | `497-docs-proxy-config-settings.md` | All settings - liteLLM | This document provides a comprehensive reference list of environment variables and configuration set... | environment-variables, configuration-settings, cloud-integrations |
| 498 | `498-docs-proxy-custom-pricing.md` | Custom LLM Pricing - liteLLM | This document explains how to configure and customize cost tracking in LiteLLM, including overriding... | litellm, cost-tracking, pricing-configuration |
| 499 | `499-docs-proxy-custom-root-ui.md` | UI - Custom Root Path - liteLLM | This guide explains how to configure and run the LiteLLM proxy on a custom base URL path using the S... | litellm, proxy-server, custom-root-path |
| 500 | `500-docs-proxy-db-info.md` | What is stored in the DB - liteLLM | This document explains how LiteLLM Proxy utilizes a PostgreSQL database for logging and provides ins... | litellm-proxy, postgresql, logging |
| 501 | `501-docs-proxy-forward-client-headers.md` | Forward Client Headers to LLM API - liteLLM | This document explains how to configure LiteLLM to selectively forward client request headers to und... | litellm, header-forwarding, proxy-configuration |
| 502 | `502-docs-proxy-guardrails-azure-content-guardrail.md` | Azure Content Safety Guardrail - liteLLM | This document explains how to integrate and configure Azure Content Safety guardrails within LiteLLM... | litellm, azure-content-safety, guardrails |
| 503 | `503-docs-proxy-guardrails-lakera-ai.md` | Lakera AI - liteLLM | This document provides instructions on configuring and implementing Lakera guardrails within LiteLLM... | litellm, lakera, guardrails |
| 504 | `504-docs-proxy-guardrails-model-armor.md` | Google Cloud Model Armor - liteLLM | This document explains how to integrate and configure Google Cloud Model Armor guardrails within Lit... | litellm, google-cloud, model-armor |
| 505 | `505-docs-proxy-guardrails-qualifire.md` | Qualifire - liteLLM | This document provides instructions for integrating Qualifire guardrails with LiteLLM to monitor and... | litellm, qualifire, llm-guardrails |
| 506 | `506-docs-proxy-ip-address.md` | IP Address Filtering - liteLLM | This document explains how to restrict access to LiteLLM proxy endpoints by configuring a list of al... | litellm, ip-restriction, access-control |
| 507 | `507-docs-proxy-jwt-auth-arch.md` | Control Model Access with OIDC (Azure AD/Keycloak/etc.) - liteLLM | This document explains how to configure JWT authentication and role-based access control (RBAC) for ... | litellm-proxy, jwt-authentication, rbac |
| 508 | `508-docs-proxy-load-balancing.md` | Proxy - Load Balancing - liteLLM | This document explains how to implement load balancing for multiple LLM deployments using the LiteLL... | litellm-proxy, load-balancing, model-routing |
| 509 | `509-docs-proxy-pagerduty.md` | PagerDuty Alerting - liteLLM | This document provides instructions for configuring and testing PagerDuty alerts within LiteLLM to m... | litellm, pagerduty, monitoring |
| 510 | `510-docs-proxy-pass-through.md` | Create Pass Through Endpoints - liteLLM | This document explains how to configure pass-through endpoints in LiteLLM Proxy to route requests to... | litellm-proxy, api-gateway, pass-through-endpoints |
| 511 | `511-docs-proxy-prod.md` | ‚ö° Best Practices for Production - liteLLM | This document provides comprehensive configuration best practices and deployment recommendations for... | litellm-production, performance-optimization, kubernetes-deployment |
| 512 | `512-docs-proxy-prompt-management.md` | Prompt Management - liteLLM | This document explains how to integrate and manage prompts in LiteLLM by defining them within a conf... | prompt-management, litellm-proxy, config-yaml |
| 513 | `513-docs-proxy-provider-budget-routing.md` | Budget Routing - liteLLM | This document explains how to configure and manage usage budgets in LiteLLM across providers, specif... | litellm, budget-management, cost-control |
| 514 | `514-docs-proxy-provider-discounts.md` | Provider Discounts - liteLLM | This document explains how to configure and apply percentage-based cost discounts for specific LLM p... | litellm, cost-management, discount-configuration |
| 515 | `515-docs-proxy-public-routes.md` | Control Public & Private Routes - liteLLM | This document explains how to configure route access controls for the LiteLLM proxy, including setti... | litellm-proxy, route-management, authentication |
| 516 | `516-docs-proxy-reject-clientside-metadata-tags.md` | Reject Client-Side Metadata Tags - liteLLM | This document explains the reject_clientside_metadata_tags setting, which prevents users from overri... | api-security, configuration, metadata-tags |
| 517 | `517-docs-proxy-spend-logs-deletion.md` | ‚ú® Maximum Retention Period for Spend Logs - liteLLM | This document explains how to configure and automate the deletion of old spend logs to manage databa... | log-retention, database-management, spend-logs |
| 518 | `518-docs-proxy-timeout.md` | Timeouts - liteLLM | This document explains how to configure global, per-model, and per-request timeouts for synchronous ... | litellm, router, timeout-configuration |
| 519 | `519-docs-proxy-users.md` | Budgets, Rate Limits - liteLLM | This document outlines how to configure and manage spending limits for LiteLLM Proxy across global, ... | litellm, proxy-server, budget-management |
| 520 | `520-docs-realtime.md` | /realtime - liteLLM | This document explains how to implement load balancing for realtime audio requests across Azure and ... | azure-openai, litellm, load-balancing |
| 521 | `521-docs-routing-load-balancing.md` | Routing, Loadbalancing & Fallbacks - liteLLM | Explains how timeout settings configured in the router apply to the entire duration of a call and pr... | timeout-configuration, router-settings, api-completion |
| 522 | `522-docs-secret-managers-aws-kms.md` | AWS Key Management V1 - liteLLM | This document explains how to use AWS Key Management Service (KMS) to store and decrypt a hashed cop... | aws-kms, key-management, security |
| 523 | `523-docs-secret-managers-aws-secret-manager.md` | AWS Secret Manager - liteLLM | This document provides instructions for configuring LiteLLM to use AWS Secret Manager for storing an... | aws-secret-manager, key-management, iam-roles |
| 524 | `524-docs-secret-managers-azure-key-vault.md` | Azure Key Vault - liteLLM | This document provides instructions for configuring the LiteLLM Proxy Server to use Azure Key Vault ... | litellm, proxy-server, azure-key-vault |
| 525 | `525-docs-secret-managers-google-secret-manager.md` | Google Secret Manager - liteLLM | This document explains how to configure and integrate Google Secret Manager as a key management syst... | google-secret-manager, litellm-proxy, key-management |
| 526 | `526-docs-secret-managers-hashicorp-vault.md` | Hashicorp Vault - liteLLM | This document explains how to configure and use Hashicorp Vault with LiteLLM for managing, reading, ... | hashicorp-vault, secret-management, litellm-proxy |
| 527 | `527-docs-set-keys.md` | Setting API Keys, Base, Version - liteLLM | This document explains how to configure LiteLLM API settings such as keys, bases, and versions using... | litellm, api-configuration, environment-variables |
| 528 | `528-docs-wildcard-routing.md` | Provider specific Wildcard routing - liteLLM | This document demonstrates how to configure the LiteLLM Router using wildcard patterns to dynamicall... | litellm, router-configuration, wildcard-routing |
| 529 | `529-release-notes-tags-budgets-rate-limits.md` | One post tagged with "budgets/rate limits" - liteLLM | This document outlines how to configure usage budget tiers for rate limiting and details updates reg... | litellm, budget-management, rate-limiting |
| 530 | `530-release-notes-tags-key-management.md` | One post tagged with "key management" - liteLLM | This document provides instructions on defining usage tiers with rate limits and highlights updates ... | rate-limiting, budget-management, api-logging |
| 531 | `531-release-notes-v1.59.0.md` | v1.59.0 | This document explains how to enable and view message and response logs in the LiteLLM Admin UI by m... | litellm, admin-ui, logging |

### 6. Integration & Connection (532-557)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 532 | `532-docs-adding-provider-generic-guardrail-api.md` | [BETA] Generic Guardrail API - Integrate Without a PR - liteLLM | This document explains how to integrate custom guardrail providers with LiteLLM using a standardized... | litellm, guardrails, api-integration |
| 533 | `533-docs-providers-abliteration.md` | Abliteration - liteLLM | This document provides instructions for integrating Abliteration with LiteLLM using its OpenAI-compa... | abliteration, litellm, openai-compatible |
| 534 | `534-docs-providers-aleph-alpha.md` | Aleph Alpha - liteLLM | This document provides instructions for using Aleph Alpha models with LiteLLM, including API key con... | litellm, aleph-alpha, llm-integration |
| 535 | `535-docs-providers-anthropic.md` | Anthropic - liteLLM | This document provides technical details and instructions for integrating Anthropic's Claude models ... | litellm, anthropic, claude |
| 536 | `536-docs-providers-apertis.md` | Apertis AI (Stima API) - liteLLM | This document provides an overview and integration guide for Apertis AI, detailing how to access ove... | apertis-ai, stima-api, litellm |
| 537 | `537-docs-providers-azure-azure-responses.md` | Azure Responses API - liteLLM | This document explains how to integrate and use the Azure OpenAI Responses API with LiteLLM, includi... | azure-openai, litellm, responses-api |
| 538 | `538-docs-providers-azure-azure-speech.md` | Azure Text to Speech (tts) - liteLLM | This document provides a code implementation for converting text to speech using an Azure-hosted mod... | text-to-speech, azure-openai, speech-synthesis |
| 539 | `539-docs-providers-bedrock-embedding.md` | Bedrock Embedding - liteLLM | This document provides a technical reference for using LiteLLM with AWS Bedrock embedding models, in... | aws-bedrock, litellm, embedding-models |
| 540 | `540-docs-providers-bedrock-rerank.md` | AWS Bedrock - Rerank API - liteLLM | This document explains how to integrate and use the Bedrock Rerank API through LiteLLM using a Coher... | bedrock, rerank-api, litellm |
| 541 | `541-docs-providers-codestral.md` | Codestral API [Mistral AI] - liteLLM | This document provides instructions and code examples for integrating Codestral models via LiteLLM f... | codestral, litellm, text-completion |
| 542 | `542-docs-providers-compactifai.md` | CompactifAI - liteLLM | This document provides a technical overview and integration guide for CompactifAI's OpenAI-compatibl... | compactifai, openai-compatibility, model-compression |
| 543 | `543-docs-providers-gigachat.md` | GigaChat - liteLLM | This document provides technical instructions for integrating Sber AI's GigaChat models with the Lit... | gigachat, litellm, sber-ai |
| 544 | `544-docs-providers-lambda-ai.md` | Lambda AI - liteLLM | This document provides instructions for integrating Lambda AI with LiteLLM, covering model support, ... | lambda-ai, litellm, api-integration |
| 545 | `545-docs-providers-llamagate.md` | LlamaGate - liteLLM | This document provides a technical overview and integration guide for LlamaGate, an OpenAI-compatibl... | llamagate, api-gateway, litellm |
| 546 | `546-docs-providers-milvus-vector-stores.md` | Milvus - Vector Store - liteLLM | This document provides a Python implementation of a client for interacting with the Milvus REST API ... | milvus-vector-db, rest-api-v2, python-client |
| 547 | `547-docs-providers-nvidia-nim.md` | Nvidia NIM - liteLLM | This document provides instructions and code examples for integrating Nvidia NIM models with LiteLLM... | nvidia-nim, litellm, api-integration |
| 548 | `548-docs-providers-perplexity.md` | Perplexity AI (pplx-api) - liteLLM | This document provides instructions and code examples for integrating Perplexity AI models with the ... | litellm, perplexity-ai, api-integration |
| 549 | `549-docs-providers-togetherai.md` | Together AI - liteLLM | This document provides instructions and code examples for integrating Together AI models with LiteLL... | together-ai, litellm, python-sdk |
| 550 | `550-docs-providers-topaz.md` | Topaz - liteLLM | This document provides a code example for generating image variations using the LiteLLM library and ... | litellm, image-variation, topaz-api |
| 551 | `551-docs-providers-vercel-ai-gateway.md` | Vercel AI Gateway - liteLLM | This document explains how to integrate LiteLLM with Vercel AI Gateway to access multiple AI provide... | vercel-ai-gateway, litellm, api-integration |
| 552 | `552-docs-providers-vertex-embedding.md` | Vertex AI Embedding - liteLLM | This document provides a technical reference for using LiteLLM to interface with Vertex AI embedding... | litellm, vertex-ai, embeddings |
| 553 | `553-docs-providers-vertex-partner.md` | Vertex AI - Anthropic, DeepSeek, Model Garden - liteLLM | This document provides configuration details and code examples for accessing partner models like Cla... | vertex-ai, litellm, model-routing |
| 554 | `554-docs-providers-voyage.md` | Voyage AI - liteLLM | This document provides technical specifications and implementation examples for using VoyageAI's emb... | voyage-ai, embeddings, reranking |
| 555 | `555-docs-providers-xai.md` | xAI - liteLLM | This document provides technical specifications and implementation guides for using xAI's Grok model... | xai, grok, litellm |
| 556 | `556-docs-providers.md` | Providers - liteLLM | This document serves as a comprehensive index of all model providers supported by LiteLLM, detailing... | model-providers, llm-integration, openai-compatible |
| 557 | `557-release-notes-v1-81-0.md` | v1.81.0 - Claude Code - Web Search Across All Providers | This document outlines the updates and new features in LiteLLM version 1.81.0, including cross-provi... | release-notes, litellm-proxy, image-processing |

### 7. Authentication & Security (558-569)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 558 | `558-blog-authors.md` | Authors - liteLLM | This document identifies the core leadership and team members of the LiteLLM project, including its ... | litellm, team-profiles, leadership |
| 559 | `559-docs-data-security.md` | Data Privacy and Security - liteLLM | This document outlines the security infrastructure, privacy policies, and compliance certifications ... | security-compliance, data-privacy, soc2 |
| 560 | `560-docs-proxy-guardrails-lasso-security.md` | Lasso Security - liteLLM | This document demonstrates the API response behavior when a chat completion request is blocked by a ... | api-response, guardrails, security-filtering |
| 561 | `561-docs-proxy-security-encryption-faq.md` | LiteLLM Self-Hosted Security & Encryption FAQ - liteLLM | This document outlines the security protocols for LiteLLM, detailing how data in transit and at rest... | litellm, encryption, security-configuration |
| 562 | `562-release-notes-authors.md` | Authors - liteLLM | This document provides professional profiles and contact links for the leadership team of LiteLLM, i... | litellm, leadership, executive-team |
| 563 | `563-release-notes-tags-custom-auth.md` | One post tagged with "custom auth" - liteLLM | This document outlines several new and updated enterprise features for LiteLLM, including batch API ... | litellm-enterprise, batches-api, cost-tracking |
| 564 | `564-release-notes-tags-security.md` | 4 posts tagged with "security" - liteLLM | This document details the release notes for LiteLLM version 1.67.4, highlighting new features such a... | release-notes, litellm-proxy, load-balancing |
| 565 | `565-release-notes-v1-72-6-stable.md` | v1.72.6-stable - MCP Gateway Permission Management | Detailed release notes for LiteLLM version 1.72.6 outlining new features like MCP permissions manage... | litellm, release-notes, mcp-server |
| 566 | `566-release-notes-v1-73-0-stable.md` | v1.73.0-stable - Set default team for new users | This document outlines the updates in LiteLLM version 1.73.0, detailing new features like default te... | litellm-release, user-management, passthrough-endpoints |
| 567 | `567-release-notes-v1-77-5.md` | v1.77.5-stable - MCP OAuth 2.0 Support | This document outlines the release notes for LiteLLM version 1.77.5-stable, detailing significant pe... | litellm, release-notes, performance-benchmarks |
| 568 | `568-release-notes-v1-78-0.md` | v1.78.0-stable - MCP Gateway: Control Tool Access by Team, Key | This document provides the release notes for LiteLLM version 1.78.0-stable, detailing performance en... | litellm-release, ai-gateway, performance-optimization |
| 569 | `569-release-notes-v1.67.4-stable.md` | v1.67.4-stable - Improved User Management | This document details the features and improvements in LiteLLM version 1.67.4-stable, including enha... | litellm, release-notes, load-balancing |

### 8. API & Reference (570-689)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 570 | `570-blog-tags.md` | Tags - liteLLM | This document provides an alphabetical index of metadata tags used to categorize blog posts and arti... | blog-index, litellm-tags, content-navigation |
| 571 | `571-completion-input.md` | Completion Function - completion() - liteLLM | This document outlines the request body parameters for the liteLLM chat completion API, detailing re... | litellm, chat-completion, api-parameters |
| 572 | `572-completion-output.md` | Completion Function - completion() - liteLLM | This document illustrates the standardized JSON response structure returned by a LiteLLM completion ... | litellm, api-response, json-structure |
| 573 | `573-completion-supported.md` | Generation/Completion/Chat Completion Models - liteLLM | This document outlines the supported AI models and providers for the liteLLM library, detailing the ... | llm-providers, litellm, api-configuration |
| 574 | `574-docs-aiohttp-benchmarks.md` | LiteLLM v1.71.1 Benchmarks - liteLLM | This document provides performance benchmark results for LiteLLM v1.71.1, comparing the efficiency o... | litellm, performance-benchmarks, aiohttp |
| 575 | `575-docs-anthropic-count-tokens.md` | /v1/messages/count_tokens - liteLLM | This document explains how to use LiteLLM's Anthropic-compatible endpoint to count tokens across mul... | litellm, token-counting, anthropic-api |
| 576 | `576-docs-anthropic-unified.md` | /v1/messages - liteLLM | This document explains how to use LiteLLM to call various LLM APIs using the Anthropic messages form... | litellm, anthropic-api, messages-format |
| 577 | `577-docs-apply-guardrail.md` | /guardrails/apply_guardrail - liteLLM | This document explains how to use the LiteLLM apply_guardrail endpoint to directly invoke safety and... | litellm, api-endpoint, guardrails |
| 578 | `578-docs-assistants.md` | /assistants - liteLLM | This document explains how to use the OpenAI Assistants API via LiteLLM, covering thread management,... | litellm, assistants-api, openai |
| 579 | `579-docs-benchmarks.md` | Benchmarks - liteLLM | This document provides performance benchmarks for the LiteLLM Proxy Server, offering infrastructure ... | litellm-proxy, benchmarks, performance-metrics |
| 580 | `580-docs-completion-input.md` | Input Params - liteLLM | This document provides a comprehensive reference for LiteLLM's completion parameters, detailing how ... | litellm, openai-compatibility, api-reference |
| 581 | `581-docs-completion-message-trimming.md` | Trimming Input Messages - liteLLM | This document explains how to use the litellm.trim_messages() utility function to ensure message lis... | litellm, token-management, message-trimming |
| 582 | `582-docs-completion-output.md` | Output - liteLLM | This document defines the standardized JSON response structure and data types for LiteLLM completion... | litellm, api-response, json-schema |
| 583 | `583-docs-completion-prefix.md` | Pre-fix Assistant Messages - liteLLM | This document illustrates the chat completion response structure and provides instructions on how to... | litellm, chat-completion, api-response |
| 584 | `584-docs-completion-stream.md` | Streaming + Async - liteLLM | This document provides a technical example of how LiteLLM handles repeated streaming chunks and the ... | litellm, streaming, error-handling |
| 585 | `585-docs-completion-token-usage.md` | Completion Token Usage & Cost - liteLLM | This document outlines LiteLLM's helper functions for tokenization, token counting, and cost estimat... | litellm, tokenization, cost-tracking |
| 586 | `586-docs-completion.md` | Chat Completions - liteLLM | This document provides an overview of the LiteLLM completion API, detailing input parameters, output... | litellm, completion-api, openai-compatibility |
| 587 | `587-docs-container-files.md` | /containers/files - liteLLM | This document provides a comprehensive guide and API reference for managing files within Code Interp... | litellm, code-interpreter, file-management |
| 588 | `588-docs-data-retention.md` | Data Retention Policy - liteLLM | This document outlines LiteLLM Cloud's policy regarding the retention, deletion, and protection of c... | data-retention, privacy-policy, litellm-cloud |
| 589 | `589-docs-embedding-async-embedding.md` | litellm.aembedding() - liteLLM | This document explains how to perform asynchronous text embedding calls using LiteLLM's aembedding f... | litellm, asynchronous, embeddings |
| 590 | `590-docs-embedding-moderation.md` | litellm.moderation() - liteLLM | This document explains how to use LiteLLM to interact with the OpenAI moderation endpoint to check c... | litellm, openai, moderation-api |
| 591 | `591-docs-embedding-supported-embedding.md` | /embeddings - liteLLM | Technical documentation for LiteLLM's embedding functions, detailing synchronous and asynchronous us... | litellm, embeddings, python-sdk |
| 592 | `592-docs-exception-mapping.md` | Exception Mapping - liteLLM | This document details how LiteLLM maps exceptions from multiple LLM providers to standardized OpenAI... | litellm, exception-handling, error-mapping |
| 593 | `593-docs-image-generation.md` | Image Generations - liteLLM | This document provides documentation and usage instructions for the LiteLLM image_generation functio... | litellm, image-generation, api-integration |
| 594 | `594-docs-image-variations.md` | [BETA] Image Variations - liteLLM | This document demonstrates how to use the LiteLLM library to generate image variations using OpenAI'... | litellm, image-variation, openai |
| 595 | `595-docs-integrations.md` | Integrations - liteLLM | This document lists various third-party tool and service integrations compatible with LiteLLM, cover... | litellm, integrations, observability |
| 596 | `596-docs-interactions.md` | /interactions - liteLLM | This document provides technical specifications and implementation guides for the LiteLLM Interactio... | litellm, interactions-api, python-sdk |
| 597 | `597-docs-mcp-usage.md` | Using your MCP - liteLLM | This document provides a curl command example for making a streaming request to an API endpoint that... | api-request, mcp-tools, litellm-proxy |
| 598 | `598-docs-moderation.md` | /moderations - liteLLM | This document provides a sample JSON response format from a text moderation API, illustrating how co... | moderation-api, content-safety, json-schema |
| 599 | `599-docs-observability-telemetry.md` | Telemetry - liteLLM | This document clarifies LiteLLM's policy regarding telemetry and data collection, stating that no us... | litellm, telemetry, data-privacy |
| 600 | `600-docs-ocr.md` | /ocr - liteLLM | This document provides technical documentation for implementing Optical Character Recognition (OCR) ... | litellm, ocr, python-sdk |
| 601 | `601-docs-project.md` | Projects built on LiteLLM - liteLLM | This document provides a directory of open-source projects, SDKs, and frameworks that integrate with... | ai-agents, llm-integrations, open-source-projects |
| 602 | `602-docs-projects-LiteLLM-Proxy.md` | LiteLLM Proxy - liteLLM | This document introduces the LiteLLM Proxy server, which provides a unified interface for over 50 LL... | litellm-proxy, llm-gateway, error-handling |
| 603 | `603-docs-projects-mini-swe-agent.md` | mini-swe-agent - liteLLM | A lightweight 100-line Python AI agent designed to automate solving GitHub issues using bash command... | ai-agent, python-automation, github-issues |
| 604 | `604-docs-projects-pgai.md` | pgai - liteLLM | This document introduces pgai, a PostgreSQL toolset for building AI applications, and provides links... | pgai, postgresql, litellm |
| 605 | `605-docs-projects-PROMPTMETHEUS.md` | PROMPTMETHEUS - liteLLM | PROMPTMETHEUS is a Prompt Engineering IDE designed for composing, testing, optimizing, and deploying... | prompt-engineering, ide, llm-development |
| 606 | `606-docs-projects-Railtracks.md` | Railtracks - liteLLM | This document introduces Railtracks, an open-source framework designed for building resilient agenti... | agentic-framework, ai-agents, open-source |
| 607 | `607-docs-projects-SalesGPT.md` | SalesGPT - liteLLM | This document provides information about SalesGPT, an open-source, context-aware AI sales assistant ... | sales-gpt, ai-assistant, sales-automation |
| 608 | `608-docs-projects-smolagents.md` | ü§ó Smolagents - liteLLM | This document introduces smolagents, a lightweight library designed for building AI agents that writ... | smolagents, hugging-face, python-agents |
| 609 | `609-docs-projects.md` | Projects Built on LiteLLM - liteLLM | This document provides a directory of AI-powered tools and models designed for data interaction, cod... | ai-tools, open-source, multi-modal-models |
| 610 | `610-docs-proxy-cli.md` | CLI Arguments - liteLLM | This document provides a comprehensive list of command-line interface arguments and environment vari... | litellm, cli-arguments, server-configuration |
| 611 | `611-docs-proxy-fallback-management.md` | [New] Fallback Management Endpoints - liteLLM | This document describes the dedicated API endpoints for configuring and managing model fallbacks, en... | model-fallbacks, api-endpoints, proxy-configuration |
| 612 | `612-docs-proxy-guardrails-pangea.md` | Pangea - liteLLM | This document provides an example of an API error response triggered by a Pangea guardrail violation... | pangea-ai-guard, prompt-injection, error-handling |
| 613 | `613-docs-proxy-health.md` | Health Checks - liteLLM | This document explains how to utilize and configure health check endpoints for the LiteLLM proxy to ... | litellm, health-check, monitoring |
| 614 | `614-docs-proxy-logging-spec.md` | StandardLoggingPayload Specification - liteLLM | This document defines the structure and fields of the StandardLoggingPayload used for tracking LLM r... | logging, observability, llm-monitoring |
| 615 | `615-docs-proxy-metrics.md` | üí∏ GET Daily Spend, Usage Metrics - liteLLM | This document illustrates the data structure used for reporting daily and total expenditures across ... | billing-data, usage-tracking, cost-analysis |
| 616 | `616-docs-proxy-model-management.md` | Model Management - liteLLM | This document explains how to use specific API endpoints to dynamically add new models and retrieve ... | litellm, model-management, api-endpoints |
| 617 | `617-docs-proxy-perf.md` | LiteLLM Proxy Performance - liteLLM | This document provides performance benchmarks for the LiteLLM proxy, detailing its impact on through... | litellm-proxy, benchmarks, throughput |
| 618 | `618-docs-proxy-release-cycle.md` | Release Cycle - liteLLM | This document outlines the release cycle and versioning strategy for LiteLLM Proxy, defining the cri... | release-cycle, versioning, litellm-proxy |
| 619 | `619-docs-proxy-request-headers.md` | Request Headers - liteLLM | This document outlines the specific HTTP headers supported by LiteLLM for managing request timeouts,... | litellm, http-headers, header-forwarding |
| 620 | `620-docs-proxy-response-headers.md` | Response Headers - liteLLM | This document outlines the standard and custom HTTP response headers returned by the LiteLLM proxy, ... | litellm, response-headers, rate-limiting |
| 621 | `621-docs-rag-ingest.md` | /rag/ingest - liteLLM | This document details the LiteLLM RAG ingestion pipeline, providing API specifications and configura... | rag-ingestion, vector-stores, openai |
| 622 | `622-docs-rag-query.md` | /rag/query - liteLLM | This document provides technical specifications and implementation examples for the RAG Query endpoi... | rag, vector-store, reranking |
| 623 | `623-docs-response-api-compact.md` | /responses/compact - liteLLM | This document explains how to use the OpenAI /responses/compact endpoint to compress conversation hi... | openai-api, conversation-history, message-compaction |
| 624 | `624-docs-search-linkup.md` | Linkup Search - liteLLM | This code snippet demonstrates how to perform a search using the Linkup provider within the LiteLLM ... | litellm, linkup, search-api |
| 625 | `625-docs-simple-proxy.md` | LiteLLM AI Gateway (LLM Proxy) - liteLLM | This document serves as a comprehensive directory and index for LiteLLM Proxy documentation, providi... | litellm-proxy, llm-gateway, api-management |
| 626 | `626-docs-supported-endpoints.md` | Supported Endpoints - liteLLM | This document describes how to manage files within Code Interpreter containers, covering automatical... | code-interpreter, file-management, containers |
| 627 | `627-docs-text-completion.md` | /completions - liteLLM | This document illustrates the structure and schema of a response object from a text completion API, ... | api-response, text-completion, json-structure |
| 628 | `628-docs-vector-store-files.md` | /vector_stores/\{vector_store_id\}/files - liteLLM | This document outlines the API endpoints and client methods for managing vector store files, detaili... | vector-stores, litellm-proxy, openai-api |
| 629 | `629-docs-vector-stores-create.md` | /vector_stores - Create Vector Store - liteLLM | This document explains how to create vector stores for document chunk storage and retrieval in RAG s... | vector-stores, rag, openai-api |
| 630 | `630-docs-vector-stores-search.md` | /vector_stores/search - Search Vector Store - liteLLM | This document explains how to perform vector store searches using the LiteLLM Python SDK and Proxy S... | litellm, vector-store, rag |
| 631 | `631-embedding-supported-embedding.md` | Embedding Models - liteLLM | This document provides the specific function call syntax and environment variable requirements for i... | openai-api, text-embeddings, environment-variables |
| 632 | `632-release-notes-archive.md` | Archive - liteLLM | This document provides a chronological index of LiteLLM release notes and version updates from 2024 ... | release-notes, changelog, version-history |
| 633 | `633-release-notes-tags-admin-ui.md` | 3 posts tagged with "admin ui" - liteLLM | This document provides a comprehensive list of updates and new features for LiteLLM, covering model ... | litellm-updates, changelog, llm-integration |
| 634 | `634-release-notes-tags-alerting.md` | One post tagged with "alerting" - liteLLM | This document outlines the updates and improvements for LiteLLM version 1.57.8, including new model ... | litellm, release-notes, llm-proxy |
| 635 | `635-release-notes-tags-azure-storage.md` | One post tagged with "azure_storage" - liteLLM | This document highlights key updates in the LiteLLM v1.55.8 stable release, including new features f... | litellm, stable-release, azure-data-lake |
| 636 | `636-release-notes-tags-batch.md` | One post tagged with "batch" - liteLLM | This document outlines the release notes and feature updates for LiteLLM versions 1.56.3 through 1.5... | litellm-release, model-updates, llm-proxy |
| 637 | `637-release-notes-tags-cost-tracking.md` | 2 posts tagged with "cost_tracking" - liteLLM | This document outlines the latest updates to LiteLLM, including SCIM integration for identity provid... | litellm, scim-integration, usage-tracking |
| 638 | `638-release-notes-tags-custom-prompt-management.md` | One post tagged with "custom_prompt_management" - liteLLM | This document defines the schema for an API usage report, detailing metrics such as token consumptio... | api-response, usage-metrics, token-consumption |
| 639 | `639-release-notes-tags-db-schema.md` | 2 posts tagged with "db schema" - liteLLM | This document provides a detailed list of recent updates, bug fixes, and new feature releases for Li... | litellm-updates, model-support, spend-tracking |
| 640 | `640-release-notes-tags-finetuning.md` | One post tagged with "finetuning" - liteLLM | This document provides release notes for LiteLLM, detailing new model support, API improvements, and... | release-notes, litellm, model-integration |
| 641 | `641-release-notes-tags-logging.md` | 4 posts tagged with "logging" - liteLLM | This document details recent updates and new features for LiteLLM, including support for new models,... | litellm, release-notes, llm-proxy |
| 642 | `642-release-notes-tags-management-endpoints.md` | 3 posts tagged with "management endpoints" - liteLLM | This document outlines the feature updates and model integrations for LiteLLM, including new alertin... | litellm-updates, model-integration, proxy-management |
| 643 | `643-release-notes-tags-mcp.md` | One post tagged with "mcp" - liteLLM | This document defines the data structure for tracking API usage metrics, including token consumption... | api-usage, billing-metrics, token-tracking |
| 644 | `644-release-notes-tags-new-models.md` | 2 posts tagged with "new models" - liteLLM | This document outlines new features, bug fixes, and model updates for LiteLLM, focusing on enhanced ... | litellm, release-notes, guardrails |
| 645 | `645-release-notes-tags-prometheus.md` | 2 posts tagged with "prometheus" - liteLLM | This document details the feature updates and release notes for LiteLLM, highlighting new model supp... | release-notes, litellm, model-updates |
| 646 | `646-release-notes-tags-reasoning-content.md` | 3 posts tagged with "reasoning_content" - liteLLM | This update for version 1.63.0 modifies the Anthropic thinking response structure to correctly retur... | anthropic, api-update, extended-thinking |
| 647 | `647-release-notes-tags-responses-api.md` | 3 posts tagged with "responses_api" - liteLLM | This document details the release notes for LiteLLM version 1.67.4-stable, featuring improvements in... | litellm, release-notes, load-balancing |
| 648 | `648-release-notes-tags-secret-management.md` | 2 posts tagged with "secret management" - liteLLM | This document outlines the release notes and feature updates for LiteLLM between versions v1.56.3 an... | litellm, release-notes, model-updates |
| 649 | `649-release-notes-tags-sso.md` | 2 posts tagged with "sso" - liteLLM | This document outlines the latest release updates for LiteLLM, featuring SCIM integration for automa... | litellm, scim-integration, usage-tracking |
| 650 | `650-release-notes-tags-team-management.md` | One post tagged with "team management" - liteLLM | This document outlines several new enterprise-level features for LiteLLM including batch cost tracki... | litellm-proxy, batches-api, guardrails |
| 651 | `651-release-notes-tags-team-models.md` | One post tagged with "team models" - liteLLM | This document outlines updates to LiteLLM v1.65.0 that restrict model creation to admins and expand ... | litellm, api-updates, access-control |
| 652 | `652-release-notes-tags-thinking.md` | 3 posts tagged with "thinking" - liteLLM | This document explains a fix in LiteLLM v1.63.0 that aligns the Anthropic thinking response structur... | litellm, anthropic, api-update |
| 653 | `653-release-notes-tags-ui.md` | 4 posts tagged with "ui" - liteLLM | This document outlines the updates and new features for LiteLLM versions v1.56.3 through v1.57.8, co... | litellm, release-notes, llm-proxy |
| 654 | `654-release-notes-tags-unified-file-id.md` | 2 posts tagged with "unified_file_id" - liteLLM | This document outlines new features and updates for LiteLLM, including SCIM integration for automate... | litellm, scim, usage-tracking |
| 655 | `655-release-notes-tags.md` | Tags - liteLLM | This document provides an alphabetical index of tags used to categorize and navigate LiteLLM release... | release-notes, litellm, tag-index |
| 656 | `656-release-notes-v1-72-0-stable.md` | v1.72.0-stable | This document outlines the release notes for LiteLLM version 1.72.0-stable, highlighting key updates... | litellm, release-notes, vector-store |
| 657 | `657-release-notes-v1-72-2-stable.md` | v1.72.2-stable | This document outlines the updates in LiteLLM version 1.72.2, detailing performance optimizations fo... | litellm, release-notes, api-performance |
| 658 | `658-release-notes-v1-73-6-stable.md` | v1.73.6-stable | This document outlines the release notes for LiteLLM v1.73.6, detailing new features such as gemini-... | litellm, release-notes, gemini-cli |
| 659 | `659-release-notes-v1-74-0-stable.md` | v1.74.0-stable | This document outlines the updates in LiteLLM version 1.74.0, detailing new features such as MCP Gat... | litellm, release-notes, mcp-gateway |
| 660 | `660-release-notes-v1-74-15.md` | v1.74.15-stable | This document outlines the updates in LiteLLM version 1.74.15-stable, highlighting new features such... | litellm, release-notes, model-deployment |
| 661 | `661-release-notes-v1-74-3-stable.md` | v1.74.3-stable | This document outlines the features and improvements introduced in LiteLLM v1.74.3-stable, focusing ... | litellm, release-notes, mcp-gateway |
| 662 | `662-release-notes-v1-74-7.md` | v1.74.7-stable | This document outlines the updates in LiteLLM version 1.74.7, featuring the introduction of a Vector... | litellm, release-notes, vector-stores |
| 663 | `663-release-notes-v1-74-9.md` | v1.74.9-stable - Auto-Router | This document details the updates in LiteLLM version 1.74.9-stable, focusing on new features like co... | litellm, release-notes, auto-routing |
| 664 | `664-release-notes-v1-75-5.md` | v1.75.5-stable - Redis latency improvements | This document outlines the release notes for LiteLLM v1.75.5-stable, highlighting new provider suppo... | release-notes, litellm, llm-proxy |
| 665 | `665-release-notes-v1-75-8.md` | v1.75.8-stable - Team Member Rate Limits | This document outlines the updates and new features in LiteLLM version 1.75.8, including support for... | litellm-release, model-support, rate-limiting |
| 666 | `666-release-notes-v1-76-1.md` | v1.76.1-stable - Gemini 2.5 Flash Image | Release notes for LiteLLM v1.76.1 detailing major performance optimizations, support for new models ... | litellm, release-notes, performance-optimization |
| 667 | `667-release-notes-v1-77-2.md` | v1.77.2-stable - Bedrock Batches API | This document provides the release notes for LiteLLM version 1.77.2, detailing new model support, Be... | litellm-release, changelog, api-integration |
| 668 | `668-release-notes-v1-77-3.md` | v1.77.3-stable - Priority Based Rate Limiting | This document outlines the release notes for LiteLLM version 1.77.3, highlighting performance improv... | litellm, release-notes, performance-optimization |
| 669 | `669-release-notes-v1-78-5.md` | v1.78.5-stable - Native OCR Support | Release notes for LiteLLM version 1.78.5-stable detailing new model support for Claude Haiku 4.5 and... | litellm-release, model-support, ocr-api |
| 670 | `670-release-notes-v1-79-0.md` | v1.79.0-stable - Search APIs | This document outlines the updates and new features in LiteLLM version 1.79.0, including native sear... | litellm, release-notes, model-integration |
| 671 | `671-release-notes-v1-79-1.md` | v1.79.1-stable - Guardrail Playground | This document details the release notes for LiteLLM v1.79.1, outlining new model support, provider u... | litellm-release, changelog, model-support |
| 672 | `672-release-notes-v1-80-10.md` | [Preview] v1.80.10.rc.1 - Agent Gateway: Azure Foundry & Bedrock AgentCore | This document details the release notes for LiteLLM version 1.80.10.rc.1, focusing on the new Agent ... | litellm, release-notes, agent-gateway |
| 673 | `673-release-notes-v1-80-11.md` | v1.80.11-stable - Google Interactions API | Release notes for LiteLLM v1.80.11 detailing performance optimizations, extensive new provider suppo... | litellm-release, model-providers, performance-optimization |
| 674 | `674-release-notes-v1-80-15.md` | v1.80.15-stable - Manus API Support | This document outlines the release highlights for LiteLLM version 1.80.15-stable.1, detailing perfor... | litellm, release-notes, llm-proxy |
| 675 | `675-release-notes-v1-80-5.md` | v1.80.5-stable - Gemini 3.0 Support | This document provides release notes for LiteLLM version 1.80.5, detailing new prompt management fea... | litellm, release-notes, prompt-management |
| 676 | `676-release-notes-v1.55.10.md` | v1.55.10 | This document outlines recent updates to LiteLLM features, including batch API cost tracking, new gu... | litellm, batches-api, cost-tracking |
| 677 | `677-release-notes-v1.56.1.md` | v1.56.1 | This document outlines updates to LiteLLM including budget and rate limit tier definitions, logging ... | litellm, rate-limiting, budget-management |
| 678 | `678-release-notes-v1.57.8-stable.md` | v1.57.8-stable | This document outlines the release notes and feature updates for LiteLLM, detailing new model suppor... | release-notes, litellm-updates, llm-proxy |
| 679 | `679-release-notes-v1.59.8-stable.md` | v1.59.8-stable | This document outlines the latest feature updates and bug fixes for LiteLLM, covering new model inte... | litellm, release-notes, model-integration |
| 680 | `680-release-notes-v1.63.0.md` | v1.63.0 - Anthropic 'thinking' response update | This document outlines an update to LiteLLM version 1.63.0 that aligns Anthropic streaming responses... | litellm, anthropic-api, streaming-responses |
| 681 | `681-release-notes-v1.65.0-stable.md` | v1.65.0-stable - Model Context Protocol | This document illustrates the structure of an API usage report, detailing metrics such as token cons... | usage-metrics, api-response, billing-data |
| 682 | `682-release-notes-v1.65.0.md` | v1.65.0 - Team Model Add - update | This document details updates to model management endpoints in version 1.65.0, focusing on expanded ... | litellm, model-management, access-control |
| 683 | `683-release-notes-v1.66.0-stable.md` | v1.66.0-stable - Realtime API Cost Tracking | This document outlines the updates in LiteLLM version v1.66.0-stable, including the introduction of ... | litellm-release, realtime-api, cost-tracking |
| 684 | `684-release-notes-v1.68.0-stable.md` | v1.68.0-stable | This document provides the release notes for LiteLLM version 1.68.0-stable, detailing new features s... | litellm-release, bedrock-knowledge-base, rate-limiting |
| 685 | `685-release-notes-v1.69.0-stable.md` | v1.69.0-stable - Loadbalance Batch API Models | This document provides the release notes for LiteLLM version 1.69.0-stable, detailing new features l... | litellm, release-notes, batch-api |
| 686 | `686-release-notes-v1.70.1-stable.md` | v1.70.1-stable - Gemini Realtime API Support | This document details the features and updates in LiteLLM v1.70.1-stable, including Gemini Realtime ... | litellm, release-notes, gemini-realtime |
| 687 | `687-release-notes-v1.71.1-stable.md` | v1.71.1-stable - 2x Higher Requests Per Second (RPS) | This document outlines the release notes for LiteLLM v1.71.1-stable, detailing performance enhanceme... | litellm-release, performance-scaling, aiohttp-transport |
| 688 | `688-release-notes.md` | Release Notes - liteLLM | This document outlines the release notes and technical updates for LiteLLM version 1.81.0, focusing ... | litellm-updates, claude-code, performance-optimization |
| 689 | `689-token-usage.md` | Token Usage - liteLLM | This document details LiteLLM's helper functions for calculating token usage and the financial costs... | litellm, token-usage, cost-calculation |

### 9. Operations & Deployment (690-692)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 690 | `690-release-notes-tags-credential-management.md` | 2 posts tagged with "credential management" - liteLLM | This document outlines the updates in LiteLLM version 1.63.14-stable, covering new model support, pe... | release-notes, litellm, llm-proxy |
| 691 | `691-release-notes-tags-prompt-management.md` | One post tagged with "prompt management" - liteLLM | This document outlines the release updates and new features for LiteLLM, covering model pricing chan... | litellm, release-notes, llm-proxy |
| 692 | `692-release-notes-tags-session-management.md` | One post tagged with "session_management" - liteLLM | This document provides the release notes for LiteLLM version 1.67.4-stable, detailing new features l... | release-notes, litellm, api-updates |

### 10. Automation & Workflow (693-693)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 693 | `693-release-notes-tags-batches.md` | One post tagged with "batches" - liteLLM | This document outlines new enterprise features and updates for LiteLLM, including cost tracking for ... | litellm, enterprise-features, batches-api |

### 11. Advanced Topics (694-694)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 694 | `694-release-notes-v1-76-3.md` | v1.76.3-stable - Performance, Video Generation & CloudZero Integration | Release notes for LiteLLM v1.76.3 detailing performance optimizations, video generation support, new... | litellm, release-notes, performance-optimization |

### 12. Changelog & Releases (695-718)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 695 | `695-blog-archive.md` | Archive - liteLLM | This document provides a chronological log of LiteLLM updates and blog posts documenting day-zero su... | litellm, model-support, gemini-3 |
| 696 | `696-release-notes-tags-claude-3-7-sonnet.md` | 3 posts tagged with "claude-3-7-sonnet" - liteLLM | This update for LiteLLM v1.63.0 fixes Anthropic thinking responses during streaming and aligns the r... | litellm, anthropic, extended-thinking |
| 697 | `697-release-notes-tags-fallbacks.md` | One post tagged with "fallbacks" - liteLLM | Summarizes key updates in the LiteLLM v1.55.8-stable release, including prompt management enhancemen... | litellm, release-notes, llm-proxy |
| 698 | `698-release-notes-tags-guardrails.md` | 3 posts tagged with "guardrails" - liteLLM | This document details a series of product updates and new features for LiteLLM, focusing on guardrai... | litellm, guardrails, logging |
| 699 | `699-release-notes-tags-humanloop.md` | One post tagged with "humanloop" - liteLLM | This document outlines the release notes and new features for LiteLLM, including model updates, prov... | litellm, changelog, llm-proxy |
| 700 | `700-release-notes-tags-langfuse.md` | 3 posts tagged with "langfuse" - liteLLM | This document highlights key updates in the LiteLLM v1.55.8 stable release, including Langfuse integ... | litellm, release-notes, langfuse |
| 701 | `701-release-notes-tags-llm-translation.md` | 3 posts tagged with "llm translation" - liteLLM | LiteLLM version 1.63.0 updates the Anthropic extended thinking response format by renaming signature... | litellm, anthropic, api-integration |
| 702 | `702-release-notes-tags-rerank.md` | One post tagged with "rerank" - liteLLM | This document outlines the updates in LiteLLM version 1.61.20-stable, highlighting new model support... | litellm, release-notes, claude-3-7 |
| 703 | `703-release-notes-tags-snowflake.md` | 2 posts tagged with "snowflake" - liteLLM | This document outlines the updates, new model integrations, and performance improvements introduced ... | litellm, release-notes, llm-proxy |
| 704 | `704-release-notes-tags-thinking-content.md` | 2 posts tagged with "thinking content" - liteLLM | This document outlines the updates and bug fixes introduced in LiteLLM v1.63.14-stable, including ne... | litellm, release-notes, llm-proxy |
| 705 | `705-release-notes-tags-ui-improvements.md` | One post tagged with "ui_improvements" - liteLLM | This document details the features and fixes in LiteLLM version 1.67.4, including enhanced user mana... | litellm-update, load-balancing, user-management |
| 706 | `706-release-notes-v1-76-0.md` | v1.76.0-stable - RPS Improvements | Release notes detailing recent bug fixes, new model integrations, and feature updates for the LiteLL... | litellm, changelog, release-notes |
| 707 | `707-release-notes-v1-77-7.md` | v1.77.7-stable - 2.9x Lower Median Latency | This document outlines the updates and key highlights for LiteLLM version 1.77.7.rc.1, focusing on s... | litellm, release-notes, performance-optimization |
| 708 | `708-release-notes-v1-79-3.md` | v1.79.3-stable - Built-in Guardrails on AI Gateway | This document details the release notes for LiteLLM version 1.79.3, highlighting new built-in guardr... | release-notes, litellm, ai-gateway |
| 709 | `709-release-notes-v1.55.8-stable.md` | v1.55.8-stable | This document summarizes five key updates in the LiteLLM v1.55.8-stable release, including prompt ma... | litellm, release-notes, prompt-management |
| 710 | `710-release-notes-v1.56.3.md` | v1.56.3 | This document details recent updates and new features for LiteLLM, including guardrail tracing, API ... | litellm, guardrails, api-updates |
| 711 | `711-release-notes-v1.56.4.md` | v1.56.4 | This document outlines recent updates to LiteLLM, including support for Deepgram speech-to-text and ... | litellm, deepgram, fireworks-ai |
| 712 | `712-release-notes-v1.57.7.md` | v1.57.7 | This document details recent updates to LiteLLM, including improvements to Langfuse prompt managemen... | litellm, langfuse, team-management |
| 713 | `713-release-notes-v1.61.20-stable.md` | v1.61.20-stable | This document outlines the updates and new features in LiteLLM version 1.61.20, focusing on support ... | litellm, release-notes, model-integration |
| 714 | `714-release-notes-v1.63.11-stable.md` | v1.63.11-stable | This document details the release notes for LiteLLM version 1.63.11-stable, highlighting new support... | litellm-release, responses-api, snowflake-cortex |
| 715 | `715-release-notes-v1.63.14-stable.md` | v1.63.14-stable | This document outlines the updates and bug fixes for LiteLLM version 1.63.14, focusing on performanc... | litellm, release-notes, llm-proxy |
| 716 | `716-release-notes-v1.63.2-stable.md` | v1.63.2-stable | This document details the release notes for LiteLLM version 1.63.2, highlighting updates to LLM tran... | litellm, release-notes, llm-proxy |
| 717 | `717-release-notes-v1.65.4-stable.md` | v1.65.4-stable | This document outlines the v1.65.4-stable release updates for LiteLLM, focusing on database deadlock... | release-notes, litellm, changelog |
| 718 | `718-release-notes-v1.67.0-stable.md` | v1.67.0-stable - SCIM Integration | This document outlines major updates to LiteLLM, including SCIM integration for identity providers, ... | litellm, scim-integration, usage-tracking |

### 13. Meta & Resources (719-732)

| # | File | Title | Summary | Keywords |
|---|------|-------|---------|----------|
| 719 | `719-contact.md` | Contact Us - liteLLM | This document provides contact information and support channels for LiteLLM, including links to sche... | litellm, support, contact-details |
| 720 | `720-docs-contact.md` | Contact Us - liteLLM | This document provides contact information and community support resources for LiteLLM, including li... | community-support, contact-information, customer-service |
| 721 | `721-docs-debugging-hosted-debugging.md` | hosted_debugging - liteLLM | This document fragment represents a common accessibility skip-to-content link used in Docusaurus-bas... | accessibility, docusaurus, navigation |
| 722 | `722-docs-projects-Codium-PR-Agent.md` | Codium PR Agent - liteLLM | This document introduces an AI-powered tool designed to automate pull request analysis, providing de... | ai-powered, pull-request-analysis, code-review-automation |
| 723 | `723-docs-projects-dbally.md` | dbally - liteLLM | db-ally is a library that enables natural language querying of structured databases by integrating w... | natural-language-querying, structured-data, llm-integration |
| 724 | `724-docs-projects-Docq.AI.md` | Docq.AI - liteLLM | This document introduces an open-source, secure GenAI platform that allows businesses to interact wi... | open-source, genai, document-querying |
| 725 | `725-docs-projects-Elroy.md` | üêï Elroy - liteLLM | Elroy is a scriptable AI assistant that features persistent memory and goal-tracking capabilities, a... | ai-assistant, scriptable-ai, command-line |
| 726 | `726-docs-projects-FastREPL.md` | FastREPL - liteLLM | FastRepl provides a streamlined workflow for the continuous evaluation and improvement of applicatio... | llm-ops, evaluation, fastrepl |
| 727 | `727-docs-projects-GPTLocalhost.md` | GPTLocalhost - liteLLM | This document introduces GPTLocalhost, a private Microsoft Word add-in that enables the integration ... | gptlocalhost, litellm, microsoft-word |
| 728 | `728-docs-projects-Langstream.md` | Langstream - liteLLM | Langstream is a framework designed for building robust and composable applications using Large Langu... | llm, composability, orchestration |
| 729 | `729-docs-projects-llm-cord.md` | llmcord.py - liteLLM | llmcord.py is a tool that enables users to interact with various local or remote large language mode... | discord-bot, llm-integration, ai-chatbot |
| 730 | `730-docs-projects-Prompt2Model.md` | Prompt2Model - liteLLM | Prompt2Model is a system designed to transform natural language task descriptions into small, specia... | machine-learning, model-training, natural-language-processing |
| 731 | `731-docs-projects-Quivr.md` | Quivr - liteLLM | This document introduces an AI-powered personal knowledge management platform that allows users to s... | generative-ai, second-brain, knowledge-management |
| 732 | `732-docs-projects-Softgen.md` | Softgen - liteLLM | This document introduces Softgen, an AI platform that leverages LiteLLM to enable users to create fu... | softgen, litellm, ai-app-builder |

---

## Quick Reference

### By Topic
| Topic | File Range |
|-------|------------|
| **litellm** | (Search in Index) |
| **python-sdk** | (Search in Index) |
| **api-integration** | (Search in Index) |
| **llm-proxy** | (Search in Index) |
| **litellm-proxy** | (Search in Index) |

---

## Learning Path

### Level 1: Foundation
- Start with **Introduction & Overview**
- Proceed to **Quick Start & Installation**

### Level 2: Core
- Read **Concepts & Fundamentals**
- Check **Tutorials & How-To**

### Level 3: Advanced
- Explore **Features & Capabilities**
- Master **Advanced Topics**

---
*This index is auto-generated and optimized for AI agent search.*
