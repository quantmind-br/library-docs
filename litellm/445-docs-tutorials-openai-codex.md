---
title: OpenAI Codex | liteLLM
url: https://docs.litellm.ai/docs/tutorials/openai_codex
source: sitemap
fetched_at: 2026-01-21T19:55:43.34062639-03:00
rendered_js: false
word_count: 258
summary: This guide provides step-by-step instructions for integrating OpenAI Codex with LiteLLM, allowing users to route Codex requests to various LLM providers and track usage through a central proxy.
tags:
    - litellm
    - openai-codex
    - llm-proxy
    - model-routing
    - devops
    - api-configuration
    - multi-llm
category: guide
---

This guide walks you through connecting OpenAI Codex to LiteLLM. Using LiteLLM with Codex allows teams to:

- Access 100+ LLMs through the Codex interface
- Use powerful models like Gemini through a familiar interface
- Track spend and usage with LiteLLM's built-in analytics
- Control model access with virtual keys

![](https://docs.litellm.ai/assets/images/litellm_codex-fbb846dc378cc25a11d1e4d4d1cc8d5a.gif)

## Quickstart[​](#quickstart "Direct link to Quickstart")

info

Requires LiteLLM v1.66.3.dev5 and higher

Make sure to set up LiteLLM with the [LiteLLM Getting Started Guide](https://docs.litellm.ai/docs/proxy/docker_quick_start).

## 1. Install OpenAI Codex[​](#1-install-openai-codex "Direct link to 1. Install OpenAI Codex")

Install the OpenAI Codex CLI tool globally using npm:

- npm
- yarn

## 2. Start LiteLLM Proxy[​](#2-start-litellm-proxy "Direct link to 2. Start LiteLLM Proxy")

- Docker
- LiteLLM CLI

```
docker run \
    -v $(pwd)/litellm_config.yaml:/app/config.yaml \
    -p 4000:4000 \
    docker.litellm.ai/berriai/litellm:main-latest \
    --config /app/config.yaml
```

LiteLLM should now be running on [http://localhost:4000](http://localhost:4000)

## 3. Configure LiteLLM for Model Routing[​](#3-configure-litellm-for-model-routing "Direct link to 3. Configure LiteLLM for Model Routing")

Ensure your LiteLLM Proxy is properly configured to route to your desired models. Create a `litellm_config.yaml` file with the following content:

```
model_list:
-model_name: o3-mini
litellm_params:
model: openai/o3-mini
api_key: os.environ/OPENAI_API_KEY
-model_name: claude-3-7-sonnet-latest
litellm_params:
model: anthropic/claude-3-7-sonnet-latest
api_key: os.environ/ANTHROPIC_API_KEY
-model_name: gemini-2.0-flash
litellm_params:
model: gemini/gemini-2.0-flash
api_key: os.environ/GEMINI_API_KEY

litellm_settings:
drop_params:true
```

This configuration enables routing to specific OpenAI, Anthropic, and Gemini models with explicit names.

## 4. Configure Codex to Use LiteLLM Proxy[​](#4-configure-codex-to-use-litellm-proxy "Direct link to 4. Configure Codex to Use LiteLLM Proxy")

Set the required environment variables to point Codex to your LiteLLM Proxy:

```
# Point to your LiteLLM Proxy server
export OPENAI_BASE_URL=http://0.0.0.0:4000 

# Use your LiteLLM API key (if you've set up authentication)
export OPENAI_API_KEY="sk-1234"
```

## 5. Run Codex with Gemini[​](#5-run-codex-with-gemini "Direct link to 5. Run Codex with Gemini")

With everything configured, you can now run Codex with Gemini:

```
codex --model gemini-2.0-flash --full-auto
```

![](https://docs.litellm.ai/assets/images/litellm_codex-fbb846dc378cc25a11d1e4d4d1cc8d5a.gif)

The `--full-auto` flag allows Codex to automatically generate code without additional prompting.

## 6. Advanced Options[​](#6-advanced-options "Direct link to 6. Advanced Options")

### Using Different Models[​](#using-different-models "Direct link to Using Different Models")

You can use any model configured in your LiteLLM proxy:

```
# Use Claude models
codex --model claude-3-7-sonnet-latest

# Use Google AI Studio Gemini models
codex --model gemini/gemini-2.0-flash
```

## Troubleshooting[​](#troubleshooting "Direct link to Troubleshooting")

- If you encounter connection issues, ensure your LiteLLM Proxy is running and accessible at the specified URL
- Verify your LiteLLM API key is valid if you're using authentication
- Check that your model routing configuration is correct
- For model-specific errors, ensure the model is properly configured in your LiteLLM setup

## Additional Resources[​](#additional-resources "Direct link to Additional Resources")

- [LiteLLM Docker Quick Start Guide](https://docs.litellm.ai/docs/proxy/docker_quick_start)
- [OpenAI Codex GitHub Repository](https://github.com/openai/codex)
- [LiteLLM Virtual Keys and Authentication](https://docs.litellm.ai/docs/proxy/virtual_keys)