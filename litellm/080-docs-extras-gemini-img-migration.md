---
title: Gemini Image Generation Migration Guide | liteLLM
url: https://docs.litellm.ai/docs/extras/gemini_img_migration
source: sitemap
fetched_at: 2026-01-21T19:45:16.683715462-03:00
rendered_js: false
word_count: 213
summary: This document details a breaking change in LiteLLM v1.77.0 that updates the response format for Gemini image generation models from a single field to a list of images. It provides migration examples and configuration steps for both the Python SDK and the LiteLLM Proxy Server.
tags:
    - litellm
    - gemini
    - image-generation
    - api-change
    - migration-guide
    - python-sdk
    - proxy-server
category: guide
---

## Who is impacted by this change?[​](#who-is-impacted-by-this-change "Direct link to Who is impacted by this change?")

Anyone using the following models with /chat/completions:

- `gemini/gemini-2.0-flash-exp-image-generation`
- `vertex_ai/gemini-2.0-flash-exp-image-generation`

## Key Change[​](#key-change "Direct link to Key Change")

From v1.77.0, LiteLLM will return the List of images in `response.choices[0].message.images` instead of a single image in `response.choices[0].message.image`.

Gemini models now support image generation through chat completions. Images are returned in `response.choices[0].message.images` with base64 data URLs.

## Before and After[​](#before-and-after "Direct link to Before and After")

### Before[​](#before "Direct link to Before")

```
from litellm import completion

response = completion(
    model="gemini/gemini-2.0-flash-exp-image-generation",
    messages=[{"role":"user","content":"Generate an image of a cat"}],
    modalities=["image","text"],
)


base_64_image_data = response.choices[0].message.content
```

### After[​](#after "Direct link to After")

```
from litellm import completion

response = completion(
    model="gemini/gemini-2.0-flash-exp-image-generation",
    messages=[{"role":"user","content":"Generate an image of a cat"}],
    modalities=["image","text"],
)

# Image is now available in the response
image_url = response.choices[0].message.images[0]["image_url"]["url"]# "data:image/png;base64,..."
```

### Why the change?[​](#why-the-change "Direct link to Why the change?")

Because the newer `gemini-2.5-flash-image-preview` model sends both text and image responses in the same response. This interface allows a developer to explicitly access the image or text components of the response. Before a developer would have needed to search through the message content to find the image generated by the model.

**Why the change from `image` to `images`?** This is to be consistent with the OpenRouter API, making sure we are using simple, well-known interfaces where possible.

## Usage[​](#usage "Direct link to Usage")

### Using the Python SDK[​](#using-the-python-sdk "Direct link to Using the Python SDK")

**Key Change:**

```
# Before
-- base_64_image_data = response.choices[0].message.content

# After
++ image_url = response.choices[0].message.images[0]["image_url"]["url"]
```

#### Basic Image Generation[​](#basic-image-generation "Direct link to Basic Image Generation")

```
from litellm import completion
import os

# Set your API key
os.environ["GEMINI_API_KEY"]="your-api-key"

# Generate an image
response = completion(
    model="gemini/gemini-2.0-flash-exp-image-generation",
    messages=[{"role":"user","content":"Generate an image of a cat"}],
    modalities=["image","text"],
)

# Access the generated image
print(response.choices[0].message.content)# Text response (if any)
print(response.choices[0].message.images[0])# Image data
```

#### Response Format[​](#response-format "Direct link to Response Format")

The image is returned in the `message.images` field:

```
{
"image_url":{
"url":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...",
"detail":"auto"
},
"index":0,
"type":"image_url"
}
```

### Using the LiteLLM Proxy Server[​](#using-the-litellm-proxy-server "Direct link to Using the LiteLLM Proxy Server")

**Key Change:**

```
# Before
-- "content": "base64-image-data..."

# After  
++ "images": [{
++   "image_url": {
++     "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...",
++     "detail": "auto"
++   },
++   "index": 0,
++   "type": "image_url"
++ }]
```

#### Configuration Setup[​](#configuration-setup "Direct link to Configuration Setup")

1. **Configure your models in `config.yaml`:**

```
model_list:
-model_name: gemini-image-gen
litellm_params:
model: gemini/gemini-2.0-flash-exp-image-generation
api_key: os.environ/GEMINI_API_KEY
-model_name: vertex-image-gen  
litellm_params:
model: vertex_ai/gemini-2.5-flash-image-preview
vertex_project: your-project-id
vertex_location: us-central1

general_settings:
master_key: sk-1234# Your proxy API key
```

2. **Start the proxy server:**

```
litellm --config /path/to/config.yaml

# RUNNING on http://0.0.0.0:4000
```

#### Making Requests[​](#making-requests "Direct link to Making Requests")

**Using OpenAI SDK:**

```
from openai import OpenAI

# Point to your proxy server
client = OpenAI(
    api_key="sk-1234",# Your proxy API key
    base_url="http://0.0.0.0:4000"
)

response = client.chat.completions.create(
    model="gemini-image-gen",
    messages=[{"role":"user","content":"Generate an image of a cat"}],
    extra_body={"modalities":["image","text"]}
)

# Access the generated image
print(response.choices[0].message.content)# Text response (if any)
print(response.choices[0].message.image)# Image data
```

**Using curl:**

```
curl -X POST 'http://0.0.0.0:4000/v1/chat/completions' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
  "model": "gemini-image-gen",
  "messages": [
    {
      "role": "user",
      "content": "Generate an image of a cat"
    }
  ],
  "modalities": ["image", "text"]
}'
```

**Response format from proxy:**

```
{
"id":"chatcmpl-123",
"object":"chat.completion",
"created":1704089632,
"model":"gemini-image-gen",
"choices":[
{
"index":0,
"message":{
"role":"assistant",
"content":"Here's an image of a cat for you!",
"images":[{
"url":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA...",
"detail":"auto"
}
},
"finish_reason":"stop"
}
],
"usage":{
"prompt_tokens":10,
"completion_tokens":8,
"total_tokens":18
}
}
```

- [Who is impacted by this change?](#who-is-impacted-by-this-change)
- [Key Change](#key-change)
- [Before and After](#before-and-after)
  
  - [Before](#before)
  - [After](#after)
  - [Why the change?](#why-the-change)
- [Usage](#usage)
  
  - [Using the Python SDK](#using-the-python-sdk)
  - [Using the LiteLLM Proxy Server](#using-the-litellm-proxy-server)