---
title: Custom Callbacks | liteLLM
url: https://docs.litellm.ai/docs/observability/custom_callback
source: sitemap
fetched_at: 2026-01-21T19:46:00.641107793-03:00
rendered_js: false
word_count: 341
summary: This document explains how to implement custom callback classes and functions in LiteLLM to log events, track costs, and modify responses during LLM API interactions.
tags:
    - litellm
    - callbacks
    - logging
    - async-hooks
    - event-handling
    - monitoring
    - api-integration
category: guide
---

## Callback Class[â€‹](#callback-class "Direct link to Callback Class")

You can create a custom callback class to precisely log events as they occur in litellm.

```
import litellm
from litellm.integrations.custom_logger import CustomLogger
from litellm import completion, acompletion

classMyCustomHandler(CustomLogger):
deflog_pre_api_call(self, model, messages, kwargs):
print(f"Pre-API Call")

deflog_post_api_call(self, kwargs, response_obj, start_time, end_time):
print(f"Post-API Call")


deflog_success_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Success")

deflog_failure_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Failure")

#### ASYNC #### - for acompletion/aembeddings

asyncdefasync_log_success_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Async Success")

asyncdefasync_log_failure_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Async Failure")

customHandler = MyCustomHandler()

litellm.callbacks =[customHandler]

## sync 
response = completion(model="gpt-3.5-turbo", messages=[{"role":"user","content":"Hi ðŸ‘‹ - i'm openai"}],
                              stream=True)
for chunk in response:
continue


## async
import asyncio 

defasync completion():
    response =await acompletion(model="gpt-3.5-turbo", messages=[{"role":"user","content":"Hi ðŸ‘‹ - i'm openai"}],
                              stream=True)
asyncfor chunk in response:
continue
asyncio.run(completion())
```

## Common Hooks[â€‹](#common-hooks "Direct link to Common Hooks")

- `async_log_success_event` - Log successful API calls
- `async_log_failure_event` - Log failed API calls
- `log_pre_api_call` - Log before API call
- `log_post_api_call` - Log after API call

**Proxy-only hooks** (only work with LiteLLM Proxy):

- `async_post_call_success_hook` - Access user data + modify responses
- `async_pre_call_hook` - Modify requests before sending

### Example: Modifying the Response in async\_post\_call\_success\_hook[â€‹](#example-modifying-the-response-in-async_post_call_success_hook "Direct link to Example: Modifying the Response in async_post_call_success_hook")

You can use `async_post_call_success_hook` to add custom headers or metadata to the response before it is returned to the client. For example:

```
asyncdefasync_post_call_success_hook(data, user_api_key_dict, response):
# Add a custom header to the response
    additional_headers =getattr(response,"_hidden_params",{}).get("additional_headers",{})or{}
    additional_headers["x-litellm-custom-header"]="my-value"
ifnothasattr(response,"_hidden_params"):
        response._hidden_params ={}
    response._hidden_params["additional_headers"]= additional_headers
return response
```

This allows you to inject custom metadata or headers into the response for downstream consumers. You can use this pattern to pass information to clients, proxies, or observability tools.

## Callback Functions[â€‹](#callback-functions "Direct link to Callback Functions")

If you just want to log on a specific event (e.g. on input) - you can use callback functions.

You can set custom callbacks to trigger for:

- `litellm.input_callback` - Track inputs/transformed inputs before making the LLM API call
- `litellm.success_callback` - Track inputs/outputs after making LLM API call
- `litellm.failure_callback` - Track inputs/outputs + exceptions for litellm calls

## Defining a Custom Callback Function[â€‹](#defining-a-custom-callback-function "Direct link to Defining a Custom Callback Function")

Create a custom callback function that takes specific arguments:

```
defcustom_callback(
    kwargs,# kwargs to completion
    completion_response,# response from completion
    start_time, end_time    # start/end time
):
# Your custom code here
print("LITELLM: in custom callback function")
print("kwargs", kwargs)
print("completion_response", completion_response)
print("start_time", start_time)
print("end_time", end_time)
```

### Setting the custom callback function[â€‹](#setting-the-custom-callback-function "Direct link to Setting the custom callback function")

```
import litellm
litellm.success_callback =[custom_callback]
```

## Using Your Custom Callback Function[â€‹](#using-your-custom-callback-function "Direct link to Using Your Custom Callback Function")

```
import litellm
from litellm import completion

# Assign the custom callback function
litellm.success_callback =[custom_callback]

response = completion(
    model="gpt-3.5-turbo",
    messages=[
{
"role":"user",
"content":"Hi ðŸ‘‹ - i'm openai"
}
]
)

print(response)

```

## Async Callback Functions[â€‹](#async-callback-functions "Direct link to Async Callback Functions")

We recommend using the Custom Logger class for async.

```
from litellm.integrations.custom_logger import CustomLogger
from litellm import acompletion 

classMyCustomHandler(CustomLogger):
#### ASYNC #### 


asyncdefasync_log_success_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Async Success")

asyncdefasync_log_failure_event(self, kwargs, response_obj, start_time, end_time):
print(f"On Async Failure")

import asyncio 
customHandler = MyCustomHandler()

litellm.callbacks =[customHandler]

defasync completion():
    response =await acompletion(model="gpt-3.5-turbo", messages=[{"role":"user","content":"Hi ðŸ‘‹ - i'm openai"}],
                              stream=True)
asyncfor chunk in response:
continue
asyncio.run(completion())
```

**Functions**

If you just want to pass in an async function for logging.

LiteLLM currently supports just async success callback functions for async completion/embedding calls.

```
import asyncio, litellm 

asyncdefasync_test_logging_fn(kwargs, completion_obj, start_time, end_time):
print(f"On Async Success!")

asyncdeftest_chat_openai():
try:
# litellm.set_verbose = True
        litellm.success_callback =[async_test_logging_fn]
        response =await litellm.acompletion(model="gpt-3.5-turbo",
                              messages=[{
"role":"user",
"content":"Hi ðŸ‘‹ - i'm openai"
}],
                              stream=True)
asyncfor chunk in response:
continue
except Exception as e:
print(e)
        pytest.fail(f"An error occurred - {str(e)}")

asyncio.run(test_chat_openai())
```

## What's Available in kwargs?[â€‹](#whats-available-in-kwargs "Direct link to What's Available in kwargs?")

The kwargs dictionary contains all the details about your API call.

```
defcustom_callback(kwargs, completion_response, start_time, end_time):
# Access common data
    model = kwargs.get("model")
    messages = kwargs.get("messages",[])
    cost = kwargs.get("response_cost",0)
    cache_hit = kwargs.get("cache_hit",False)

# Access metadata you passed in
    metadata = kwargs.get("litellm_params",{}).get("metadata",{})
```

**Key fields in kwargs:**

- `model` - The model name
- `messages` - Input messages
- `response_cost` - Calculated cost
- `cache_hit` - Whether response was cached
- `litellm_params.metadata` - Your custom metadata

## Practical Examples[â€‹](#practical-examples "Direct link to Practical Examples")

### Track API Costs[â€‹](#track-api-costs "Direct link to Track API Costs")

```
deftrack_cost_callback(kwargs, completion_response, start_time, end_time):
    cost = kwargs["response_cost"]# litellm calculates this for you
print(f"Request cost: ${cost}")

litellm.success_callback =[track_cost_callback]

response = completion(model="gpt-3.5-turbo", messages=[{"role":"user","content":"Hello"}])
```

### Log Inputs to LLMs[â€‹](#log-inputs-to-llms "Direct link to Log Inputs to LLMs")

```
defget_transformed_inputs(kwargs):
    params_to_model = kwargs["additional_args"]["complete_input_dict"]
print("params to model", params_to_model)

litellm.input_callback =[get_transformed_inputs]

response = completion(model="claude-2", messages=[{"role":"user","content":"Hello"}])
```

### Send to External Service[â€‹](#send-to-external-service "Direct link to Send to External Service")

```
import requests

defsend_to_analytics(kwargs, completion_response, start_time, end_time):
    data ={
"model": kwargs.get("model"),
"cost": kwargs.get("response_cost",0),
"duration":(end_time - start_time).total_seconds()
}
    requests.post("https://your-analytics.com/api", json=data)

litellm.success_callback =[send_to_analytics]
```

## Common Issues[â€‹](#common-issues "Direct link to Common Issues")

### Callback Not Called[â€‹](#callback-not-called "Direct link to Callback Not Called")

Make sure you:

1. Register callbacks correctly: `litellm.callbacks = [MyHandler()]`
2. Use the right hook names (check spelling)
3. Don't use proxy-only hooks in library mode

### Performance Issues[â€‹](#performance-issues "Direct link to Performance Issues")

- Use async hooks for I/O operations
- Don't block in callback functions
- Handle exceptions properly:

```
classSafeHandler(CustomLogger):
asyncdefasync_log_success_event(self, kwargs, response_obj, start_time, end_time):
try:
await external_service(response_obj)
except Exception as e:
print(f"Callback error: {e}")# Log but don't break the flow
```