---
title: Caching | liteLLM
url: https://docs.litellm.ai/docs/proxy/caching
source: sitemap
fetched_at: 2026-01-21T19:51:18.229019026-03:00
rendered_js: false
word_count: 830
summary: This document explains how to configure and use LiteLLM's caching systems to store and reuse LLM responses, reducing latency and operational costs. It details setup procedures for various backends like Redis and S3, alongside dynamic per-request cache controls and environment variable configurations.
tags:
    - litellm
    - caching
    - redis-config
    - semantic-cache
    - latency-optimization
    - response-caching
    - llm-performance
category: configuration
---

note

For OpenAI/Anthropic Prompt Caching, go [here](https://docs.litellm.ai/docs/completion/prompt_caching)

Cache LLM Responses. LiteLLM's caching system stores and reuses LLM responses to save costs and reduce latency. When you make the same request twice, the cached response is returned instead of calling the LLM API again.

### Supported Caches[â€‹](#supported-caches "Direct link to Supported Caches")

- In Memory Cache
- Disk Cache
- Redis Cache
- Qdrant Semantic Cache
- Redis Semantic Cache
- S3 Bucket Cache
- GCS Bucket Cache

## Quick Start[â€‹](#quick-start "Direct link to Quick Start")

- redis cache
- Qdrant Semantic cache
- s3 cache
- gcs cache
- redis semantic cache
- In Memory Cache
- Disk Cache

Caching can be enabled by adding the `cache` key in the `config.yaml`

#### Step 1: Add `cache` to the config.yaml[â€‹](#step-1-add-cache-to-the-configyaml "Direct link to step-1-add-cache-to-the-configyaml")

```
model_list:
-model_name: gpt-3.5-turbo
litellm_params:
model: gpt-3.5-turbo
-model_name: text-embedding-ada-002
litellm_params:
model: text-embedding-ada-002

litellm_settings:
set_verbose:True
cache:True# set cache responses to True, litellm defaults to using a redis cache
```

#### \[OPTIONAL] Step 1.5: Add redis namespaces, default ttl[â€‹](#optional-step-15-add-redis-namespaces-default-ttl "Direct link to [OPTIONAL] Step 1.5: Add redis namespaces, default ttl")

#### Namespace[â€‹](#namespace "Direct link to Namespace")

If you want to create some folder for your keys, you can set a namespace, like this:

```
litellm_settings:
cache:true
cache_params:# set cache params for redis
type: redis
namespace:"litellm.caching.caching"
```

and keys will be stored like:

```
litellm.caching.caching:<hash>
```

#### Redis Cluster[â€‹](#redis-cluster "Direct link to Redis Cluster")

- Set on config.yaml
- Set on .env

```
model_list:
-model_name:"*"
litellm_params:
model:"*"

litellm_settings:
cache:True
cache_params:
type: redis
redis_startup_nodes:[{"host":"127.0.0.1","port":"7001"}]
```

#### Redis Sentinel[â€‹](#redis-sentinel "Direct link to Redis Sentinel")

- Set on config.yaml
- Set on .env

```
model_list:
-model_name:"*"
litellm_params:
model:"*"

litellm_settings:
cache:true
cache_params:
type:"redis"
service_name:"mymaster"
sentinel_nodes:[["localhost",26379]]
sentinel_password:"password"# [OPTIONAL]
```

#### TTL[â€‹](#ttl "Direct link to TTL")

```
litellm_settings:
cache:true
cache_params:# set cache params for redis
type: redis
ttl:600# will be cached on redis for 600s
# default_in_memory_ttl: Optional[float], default is None. time in seconds.
# default_in_redis_ttl: Optional[float], default is None. time in seconds.
```

#### SSL[â€‹](#ssl "Direct link to SSL")

just set `REDIS_SSL="True"` in your .env, and LiteLLM will pick this up.

For quick testing, you can also use REDIS\_URL, eg.:

but we **don't** recommend using REDIS\_URL in prod. We've noticed a performance difference between using it vs. redis\_host, port, etc.

#### GCP IAM Authentication[â€‹](#gcp-iam-authentication "Direct link to GCP IAM Authentication")

For GCP Memorystore Redis with IAM authentication, install the required dependency:

IAM authentication for redis is only supported via GCP and only on Redis Clusters for now.

```
pip install google-cloud-iam
```

- Set on config.yaml
- Set on .env

For Redis Cluster with GCP IAM:

```
litellm_settings:
cache:True
cache_params:
type: redis
redis_startup_nodes:
[{"host":"10.128.0.2","port":6379},{"host":"10.128.0.2","port":11008}]
gcp_service_account:"projects/-/serviceAccounts/your-sa@project.iam.gserviceaccount.com"
ssl:true
ssl_cert_reqs:null
ssl_check_hostname:false
```

#### Step 2: Add Redis Credentials to .env[â€‹](#step-2-add-redis-credentials-to-env "Direct link to Step 2: Add Redis Credentials to .env")

Set either `REDIS_URL` or the `REDIS_HOST` in your os environment, to enable caching.

```
REDIS_URL = ""        # REDIS_URL='redis://username:password@hostname:port/database'
## OR ## 
REDIS_HOST = ""       # REDIS_HOST='redis-18841.c274.us-east-1-3.ec2.cloud.redislabs.com'
REDIS_PORT = ""       # REDIS_PORT='18841'
REDIS_PASSWORD = ""   # REDIS_PASSWORD='liteLlmIsAmazing'
REDIS_USERNAME = ""   # REDIS_USERNAME='my-redis-username' [OPTIONAL] if your redis server requires a username
REDIS_SSL = "True"    # REDIS_SSL='True' to enable SSL by default is False
```

**Additional kwargs**

info

Use `REDIS_*` environment variables to configure all Redis client library parameters. This is the suggested mechanism for toggling Redis settings as it automatically maps environment variables to Redis client kwargs.

You can pass in any additional redis.Redis arg, by storing the variable + value in your os environment, like this:

```
REDIS_<redis-kwarg-name> = ""
```

For example:

```
REDIS_SSL = "True"
REDIS_SSL_CERT_REQS = "None" 
REDIS_CONNECTION_POOL_KWARGS = '{"max_connections": 20}'
```

warning

**Note**: For non-string Redis parameters (like integers, booleans, or complex objects), avoid using `REDIS_*` environment variables as they may fail during Redis client initialization. Instead, use `cache_kwargs` in your router configuration for such parameters.

[**See how it's read from the environment**](https://github.com/BerriAI/litellm/blob/4d7ff1b33b9991dcf38d821266290631d9bcd2dd/litellm/_redis.py#L40)

#### Step 3: Run proxy with config[â€‹](#step-3-run-proxy-with-config "Direct link to Step 3: Run proxy with config")

```
$ litellm --config /path/to/config.yaml
```

## Usage[â€‹](#usage "Direct link to Usage")

### Basic[â€‹](#basic "Direct link to Basic")

- /chat/completions
- /embeddings

Send the same request twice:

```
curl http://0.0.0.0:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "gpt-3.5-turbo",
     "messages": [{"role": "user", "content": "write a poem about litellm!"}],
     "temperature": 0.7
   }'

curl http://0.0.0.0:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
     "model": "gpt-3.5-turbo",
     "messages": [{"role": "user", "content": "write a poem about litellm!"}],
     "temperature": 0.7
   }'
```

### Dynamic Cache Controls[â€‹](#dynamic-cache-controls "Direct link to Dynamic Cache Controls")

ParameterTypeDescription`ttl`*Optional(int)*Will cache the response for the user-defined amount of time (in seconds)`s-maxage`*Optional(int)*Will only accept cached responses that are within user-defined range (in seconds)`no-cache`*Optional(bool)*Will not store the response in cache.`no-store`*Optional(bool)*Will not cache the response`namespace`*Optional(str)*Will cache the response under a user-defined namespace

Each cache parameter can be controlled on a per-request basis. Here are examples for each parameter:

### `ttl`[â€‹](#ttl-1 "Direct link to ttl-1")

Set how long (in seconds) to cache a response.

- OpenAI Python SDK
- curl

```
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://0.0.0.0:4000"
)

chat_completion = client.chat.completions.create(
    messages=[{"role":"user","content":"Hello"}],
    model="gpt-3.5-turbo",
    extra_body={
"cache":{
"ttl":300# Cache response for 5 minutes
}
}
)
```

### `s-maxage`[â€‹](#s-maxage "Direct link to s-maxage")

Only accept cached responses that are within the specified age (in seconds).

- OpenAI Python SDK
- curl

```
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://0.0.0.0:4000"
)

chat_completion = client.chat.completions.create(
    messages=[{"role":"user","content":"Hello"}],
    model="gpt-3.5-turbo",
    extra_body={
"cache":{
"s-maxage":600# Only use cache if less than 10 minutes old
}
}
)
```

### `no-cache`[â€‹](#no-cache "Direct link to no-cache")

Force a fresh response, bypassing the cache.

- OpenAI Python SDK
- curl

```
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://0.0.0.0:4000"
)

chat_completion = client.chat.completions.create(
    messages=[{"role":"user","content":"Hello"}],
    model="gpt-3.5-turbo",
    extra_body={
"cache":{
"no-cache":True# Skip cache check, get fresh response
}
}
)
```

### `no-store`[â€‹](#no-store "Direct link to no-store")

Will not store the response in cache.

- OpenAI Python SDK
- curl

```
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://0.0.0.0:4000"
)

chat_completion = client.chat.completions.create(
    messages=[{"role":"user","content":"Hello"}],
    model="gpt-3.5-turbo",
    extra_body={
"cache":{
"no-store":True# Don't cache this response
}
}
)
```

### `namespace`[â€‹](#namespace-1 "Direct link to namespace-1")

Store the response under a specific cache namespace.

- OpenAI Python SDK
- curl

```
from openai import OpenAI

client = OpenAI(
    api_key="your-api-key",
    base_url="http://0.0.0.0:4000"
)

chat_completion = client.chat.completions.create(
    messages=[{"role":"user","content":"Hello"}],
    model="gpt-3.5-turbo",
    extra_body={
"cache":{
"namespace":"my-custom-namespace"# Store in custom namespace
}
}
)
```

## Set cache for proxy, but not on the actual llm api call[â€‹](#set-cache-for-proxy-but-not-on-the-actual-llm-api-call "Direct link to Set cache for proxy, but not on the actual llm api call")

Use this if you just want to enable features like rate limiting, and loadbalancing across multiple instances.

Set `supported_call_types: []` to disable caching on the actual api call.

```
litellm_settings:
cache:True
cache_params:
type: redis
supported_call_types:[]
```

## Debugging Caching - `/cache/ping`[â€‹](#debugging-caching---cacheping "Direct link to debugging-caching---cacheping")

LiteLLM Proxy exposes a `/cache/ping` endpoint to test if the cache is working as expected

**Usage**

```
curl --location 'http://0.0.0.0:4000/cache/ping'  -H "Authorization: Bearer sk-1234"
```

**Expected Response - when cache healthy**

```
{
    "status": "healthy",
    "cache_type": "redis",
    "ping_response": true,
    "set_cache_response": "success",
    "litellm_cache_params": {
        "supported_call_types": "['completion', 'acompletion', 'embedding', 'aembedding', 'atranscription', 'transcription']",
        "type": "redis",
        "namespace": "None"
    },
    "redis_cache_params": {
        "redis_client": "Redis<ConnectionPool<Connection<host=redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com,port=16337,db=0>>>",
        "redis_kwargs": "{'url': 'redis://:******@redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com:16337'}",
        "async_redis_conn_pool": "BlockingConnectionPool<Connection<host=redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com,port=16337,db=0>>",
        "redis_version": "7.2.0"
    }
}
```

## Advanced[â€‹](#advanced "Direct link to Advanced")

### Control Call Types Caching is on for - (`/chat/completion`, `/embeddings`, etc.)[â€‹](#control-call-types-caching-is-on-for---chatcompletion-embeddings-etc "Direct link to control-call-types-caching-is-on-for---chatcompletion-embeddings-etc")

By default, caching is on for all call types. You can control which call types caching is on for by setting `supported_call_types` in `cache_params`

**Cache will only be on for the call types specified in `supported_call_types`**

```
litellm_settings:
cache:True
cache_params:
type: redis
supported_call_types:
["acompletion","atext_completion","aembedding","atranscription"]
# /chat/completions, /completions, /embeddings, /audio/transcriptions
```

### Set Cache Params on config.yaml[â€‹](#set-cache-params-on-configyaml "Direct link to Set Cache Params on config.yaml")

```
model_list:
-model_name: gpt-3.5-turbo
litellm_params:
model: gpt-3.5-turbo
-model_name: text-embedding-ada-002
litellm_params:
model: text-embedding-ada-002

litellm_settings:
set_verbose:True
cache:True# set cache responses to True, litellm defaults to using a redis cache
cache_params:# cache_params are optional
type:"redis"# The type of cache to initialize. Can be "local", "redis", "s3", or "gcs". Defaults to "local".
host:"localhost"# The host address for the Redis cache. Required if type is "redis".
port:6379# The port number for the Redis cache. Required if type is "redis".
password:"your_password"# The password for the Redis cache. Required if type is "redis".

# Optional configurations
supported_call_types:
["acompletion","atext_completion","aembedding","atranscription"]
# /chat/completions, /completions, /embeddings, /audio/transcriptions
```

### Deleting Cache Keys - `/cache/delete`[â€‹](#deleting-cache-keys---cachedelete "Direct link to deleting-cache-keys---cachedelete")

In order to delete a cache key, send a request to `/cache/delete` with the `keys` you want to delete

Example

```
curl -X POST "http://0.0.0.0:4000/cache/delete" \
  -H "Authorization: Bearer sk-1234" \
  -d '{"keys": ["586bf3f3c1bf5aecb55bd9996494d3bbc69eb58397163add6d49537762a7548d", "key2"]}'
```

#### Viewing Cache Keys from responses[â€‹](#viewing-cache-keys-from-responses "Direct link to Viewing Cache Keys from responses")

You can view the cache\_key in the response headers, on cache hits the cache key is sent as the `x-litellm-cache-key` response headers

```
curl -i --location 'http://0.0.0.0:4000/chat/completions' \
    --header 'Authorization: Bearer sk-1234' \
    --header 'Content-Type: application/json' \
    --data '{
    "model": "gpt-3.5-turbo",
    "user": "ishan",
    "messages": [
        {
        "role": "user",
        "content": "what is litellm"
        }
    ],
}'
```

Response from litellm proxy

```
date: Thu,04 Apr 202417:37:21 GMT
content-type: application/json
x-litellm-cache-key: 586bf3f3c1bf5aecb55bd9996494d3bbc69eb58397163add6d49537762a7548d

{
"id":"chatcmpl-9ALJTzsBlXR9zTxPvzfFFtFbFtG6T",
"choices":[
{
"finish_reason":"stop",
"index":0,
"message":{
"content":"I'm sorr.."
"role":"assistant"
}
}
],
"created":1712252235,
}

```

### \*\*Set Caching Default Off - Opt in only \*\*[â€‹](#set-caching-default-off---opt-in-only- "Direct link to **Set Caching Default Off - Opt in only **")

1. **Set `mode: default_off` for caching**

```
model_list:
-model_name: fake-openai-endpoint
litellm_params:
model: openai/fake
api_key: fake-key
api_base: https://exampleopenaiendpoint-production.up.railway.app/

# default off mode
litellm_settings:
set_verbose:True
cache:True
cache_params:
mode: default_off # ðŸ‘ˆ Key change cache is default_off
```

2. **Opting in to cache when cache is default off**

<!--THE END-->

- OpenAI Python SDK
- curl

```
import os
from openai import OpenAI

client = OpenAI(api_key=<litellm-api-key>, base_url="http://0.0.0.0:4000")

chat_completion = client.chat.completions.create(
    messages=[
{
"role":"user",
"content":"Say this is a test",
}
],
    model="gpt-3.5-turbo",
    extra_body ={# OpenAI python accepts extra args in extra_body
"cache":{"use-cache":True}
}
)
```

## Redis max\_connections[â€‹](#redis-max_connections "Direct link to Redis max_connections")

You can set the `max_connections` parameter in your `cache_params` for Redis. This is passed directly to the Redis client and controls the maximum number of simultaneous connections in the pool. If you see errors like `No connection available`, try increasing this value:

```
litellm_settings:
cache:true
cache_params:
type: redis
max_connections:100
```

## Supported `cache_params` on proxy config.yaml[â€‹](#supported-cache_params-on-proxy-configyaml "Direct link to supported-cache_params-on-proxy-configyaml")

```
cache_params:
# ttl
ttl: Optional[float]
default_in_memory_ttl: Optional[float]
default_in_redis_ttl: Optional[float]
max_connections: Optional[Int]

# Type of cache (options: "local", "redis", "s3", "gcs")
type: s3

# List of litellm call types to cache for
# Options: "completion", "acompletion", "embedding", "aembedding"
supported_call_types:
["acompletion","atext_completion","aembedding","atranscription"]
# /chat/completions, /completions, /embeddings, /audio/transcriptions

# Redis cache parameters
host: localhost # Redis server hostname or IP address
port:"6379"# Redis server port (as a string)
password: secret_password # Redis server password
namespace: Optional[str] = None,

# GCP IAM Authentication for Redis
gcp_service_account:"projects/-/serviceAccounts/your-sa@project.iam.gserviceaccount.com"# GCP service account for IAM authentication
gcp_ssl_ca_certs:"./server-ca.pem"# Path to SSL CA certificate file for GCP Memorystore Redis
ssl:true# Enable SSL for secure connections
ssl_cert_reqs:null# Set to null for self-signed certificates
ssl_check_hostname:false# Set to false for self-signed certificates

# S3 cache parameters
s3_bucket_name: your_s3_bucket_name # Name of the S3 bucket
s3_region_name: us-west-2# AWS region of the S3 bucket
s3_api_version:2006-03-01# AWS S3 API version
s3_use_ssl:true# Use SSL for S3 connections (options: true, false)
s3_verify:true# SSL certificate verification for S3 connections (options: true, false)
s3_endpoint_url: https://s3.amazonaws.com # S3 endpoint URL
s3_aws_access_key_id: your_access_key # AWS Access Key ID for S3
s3_aws_secret_access_key: your_secret_key # AWS Secret Access Key for S3
s3_aws_session_token: your_session_token # AWS Session Token for temporary credentials

# GCS cache parameters
gcs_bucket_name: your_gcs_bucket_name # Name of the GCS bucket
gcs_path_service_account: /path/to/service-account.json # Path to GCS service account JSON file
gcs_path: cache/ # [OPTIONAL] GCS path prefix for cache objects
```

## Provider-Specific Optional Parameters Caching[â€‹](#provider-specific-optional-parameters-caching "Direct link to Provider-Specific Optional Parameters Caching")

By default, LiteLLM only includes standard OpenAI parameters in cache keys. However, some providers (like Vertex AI) use additional parameters that affect the output but aren't included in the standard cache key generation.

### Enable Provider-Specific Parameter Caching[â€‹](#enable-provider-specific-parameter-caching "Direct link to Enable Provider-Specific Parameter Caching")

Add this setting to your `config.yaml` to include provider-specific optional parameters in cache keys:

```
litellm_settings:
cache:True
cache_params:
type:"redis"
enable_caching_on_provider_specific_optional_params:True# Include provider-specific params in cache keys
```

## Advanced - user api key cache ttl[â€‹](#advanced---user-api-key-cache-ttl "Direct link to Advanced - user api key cache ttl")

Configure how long the in-memory cache stores the key object (prevents db requests)

```
general_settings:
user_api_key_cache_ttl: <your-number>#time in seconds
```

By default this value is set to 60s.