---
title: New Released
url: https://docs.z.ai/release-notes/new-released.md
source: llms
fetched_at: 2026-01-24T11:23:36.278774471-03:00
rendered_js: false
word_count: 639
summary: This document provides a chronological log of updates and new releases for Z.AI's suite of models, covering advancements in large language models, vision, audio, and multimodal agents.
tags:
    - z-ai
    - model-updates
    - release-notes
    - glm-series
    - multimodal-ai
    - changelog
category: reference
---

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.z.ai/llms.txt
> Use this file to discover all available pages before exploring further.

# New Released

> Follow along with updates across Z.AI’s models

## Models

<Update label="2025-01-19" description="  GLM-4.7-Flash">
  * We’ve launched GLM-4.7-Flash, a lightweight and efficient model designed as the free-tier version of GLM-4.7, delivering strong performance across coding, reasoning, and generative tasks with low latency and high throughput.
  * The update brings competitive coding capabilities at its scale, offering best-in-class general abilities in writing, translation, long-form content, role play, and aesthetic outputs for high-frequency and real-time use cases. Learn more in our [documentation](/guides/llm/glm-4.7).\*
</Update>

<Update label="2025-01-14" description="  GLM-Image">
  * We’ve launched GLM-Image, a state-of-the-art image generation model built on a multimodal architecture and fully trained on domestic chips, combining autoregressive semantic understanding with diffusion-based decoding to deliver high-quality, controllable visual generation.
  * The update significantly enhances performance in knowledge-intensive scenarios, with more stable and accurate text rendering inside images, making GLM-Image especially well suited for commercial design, educational illustrations, and content-rich visual applications.Learn more in our [documentation](/guides/image/glm-image).\*
</Update>

<Update label="2025-12-22" description="  GLM-4.7">
  * We’ve released GLM-4.7, our latest flagship foundation model with significant improvements in coding, reasoning, and agentic capabilities. It delivers more reliable code generation, stronger long-context understanding, and improved end-to-end task execution across real-world development workflows.
  * The update brings open-source SOTA performance on major coding and reasoning benchmarks, enhanced agentic coding for goal-driven, multi-step tasks, and improved front-end and document generation quality. Learn more in our [documentation](/guides/llm/glm-4.7).\*
</Update>

<Update label="2025-12-11" description="  AutoGLM-Phone-Multilingual">
  * We’ve launched AutoGLM-Phone-Multilingual, our latest multimodal mobile automation framework that understands screen content and executes real actions through ADB. It enables natural-language task execution across 50+ mainstream apps, delivering true end-to-end mobile control.
  * The update introduces multilingual support (English & Chinese), enhanced workflow planning capabilities, and improved task execution reliability. Learn more in our [documentation](/guides/vlm/autoglm-phone-multilingual).\*
</Update>

<Update label="2025-12-10" description="  GLM-ASR-2512">
  * We’ve launched GLM-ASR-2512, our ASR model, delivering industry-leading accuracy with a Character Error Rate of just 0.0717, and significantly improved performance across real-world multilingual and accent-rich scenarios.
  * The update introduces enhanced custom dictionary support and expanded specialized terminology recognition. Learn more in our [documentation](/guides/audio/glm-asr-2512).\*
</Update>

<Update label="2025-12-08" description="  GLM-4.6V">
  * We’re excited to introduce GLM-4.6V, Z.ai’s latest iteration in multimodal large language models. This version enhances vision understanding, achieving state-of-the-art performance in tasks involving images and text.
  * The update also expands the context window to 128K, enabling more efficient processing of long inputs and complex multimodal tasks. Learn more in our [documentation](/guides/vlm/glm-4.6v).\*
</Update>

<Update label="2025-09-30" description="  GLM-4.6">
  * We’ve launched GLM-4.6, the flagship coding model, showcasing enhanced performance in both public benchmarks and real-world programming tasks, making it the leading coding model in China.
  * The update also expands the context window to 200K, improving its ability to handle longer code and complex agent tasks. Learn more in our [documentation](/guides/llm/glm-4.6).\*
</Update>

<Update label="2025-08-11" description="  GLM-4.5V">
  * We’ve launched GLM-4.5V, a 100B-scale open-source vision reasoning model, supporting a broad range of visual tasks including video understanding, visual grounding, GUI agents and etc.
  * The update also adds a new thinking mode. Learn more in our [documentation](/guides/vlm/glm-4.5v).\*
</Update>

<Update label="2025-08-08" description="  GLM Slide/Poster Agent(beta)">
  * We’ve launched GLM Slide/Poster Agent, an AI-powered creation agent that combines information retrieval, content structuring, and visual layout design to generate professional-grade slides and posters from natural language instructions.
  * The update also brings a seamless integration of content generation with design conventions. Learn more in our [documentation](/guides/agents/slide).\*
</Update>

<Update label="2025-07-28" description="  GLM-4.5 Series">
  * We’ve launched GLM-4.5, our latest native agentic LLM, delivering doubled parameter efficiency and strong reasoning, coding, and agentic capabilities.
  * It also offers seamless one-click compatibility with the Claude Code framework. Learn more in our [documentation](/guides/llm/glm-4.5).\*
</Update>

<Update label="2025-07-15" description="  CogVideoX-3">
  * We’ve launched CogVideoX-3, an incremental upgrade to our video generation model with improved quality and new features.
  * It adds support for start and end frame synthesis. Learn more in our [documentation](/guides/video/cogvideox-3).\*
</Update>